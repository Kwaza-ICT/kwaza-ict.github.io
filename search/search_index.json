{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Developer Portal of Kwaza ICT","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation home page of Kwaza ICT.</p>"},{"location":"ubuntu/","title":"Check version list from apt","text":"<pre><code>apt-cache policy curl\n</code></pre>"},{"location":"API/","title":"Public API's","text":""},{"location":"API/#introduction","title":"Introduction","text":"<p>This page describe some public API's that are available for use.</p>"},{"location":"API/#public-apis-for-open-data","title":"Public API's for Open Data","text":""},{"location":"API/#openstreetmap","title":"OpenStreetMap","text":"<p>OpenStreetMap is a collaborative project to create a free editable map of the world. The maps are created using data from portable GPS devices, aerial photography, other free sources or simply from local knowledge. The data is then made available under the Open Database License.  Link: https://www.openstreetmap.org/</p>"},{"location":"API/#nasa-api","title":"Nasa API","text":"<p>NASA has a large collection of APIs that provide access to a wide range of data. The APIs are organized into categories, such as Earth Science, Astronomy, and Space Data. The APIs provide access to data such as images, videos, and information about planets, stars, and other celestial bodies.  Link: https://api.nasa.gov/</p>"},{"location":"API/#world-bank","title":"World Bank","text":"<p>The World Bank provides access to a wide range of data through its API. The data covers a wide range of topics, such as poverty, education, health, and the environment. The API provides access to data in a variety of formats, such as JSON, XML, and CSV.  Link: https://data.worldbank.org/</p>"},{"location":"API/#geo-names","title":"Geo Names","text":"<p>GeoNames is a geographical database that covers all countries and contains over 11 million place names. The database is available under a Creative Commons Attribution 3.0 License. The GeoNames API provides access to the database and allows users to search for place names by name, postal code, or geographic coordinates.  Link: http://www.geonames.org/</p>"},{"location":"API/#open-library","title":"Open Library","text":"<p>Open Library is an open, editable library catalog that provides access to millions of books. The Open Library API provides access to the catalog and allows users to search for books by title, author, or subject. The API also provides access to information about authors, subjects, and editions.</p> <p>Link: https://openlibrary.org/developers/api</p>"},{"location":"API/#weather-apis","title":"Weather API's","text":""},{"location":"API/#openweathermap","title":"OpenWeatherMap","text":"<p>OpenWeatherMap is a service that provides weather data, including current weather conditions, forecasts, and historical data. The OpenWeatherMap API provides access to the data and allows users to search for weather data by location, time, and other parameters.  Link: https://openweathermap.org/api</p>"},{"location":"API/#weather-api","title":"Weather API","text":"<p>Weather API is a service that provides weather data, including current weather conditions, forecasts, and historical data. The Weather API provides access to the data and allows users to search for weather data by location, time, and other parameters.  Link: https://www.weatherapi.com/</p>"},{"location":"API/#storm-glass","title":"Storm Glass","text":"<p>Storm Glass is a weather API that provides access to a wide range of weather data, including current weather conditions, forecasts, and historical data. The Storm Glass API provides access to the data and allows users to search for weather data by location, time, and other parameters.  Link: https://stormglass.io/</p>"},{"location":"API/#visual-crossing","title":"Visual Crossing","text":"<p>Visual Crossing is a weather API that provides access to a wide range of weather data, including current weather conditions, forecasts, and historical data. The Visual Crossing API provides access to the data and allows users to search for weather data by location, time, and other parameters.  Link: https://www.visualcrossing.com/weather-api</p>"},{"location":"API/#weatherbit","title":"WeatherBit","text":"<p>WeatherBit is a weather API that provides access to a wide range of weather data, including current weather conditions, forecasts, and historical data. The WeatherBit API provides access to the data and allows users to search for weather data by location, time, and other parameters.  Link: https://www.weatherbit.io/</p>"},{"location":"API/#news-apis","title":"News API's","text":""},{"location":"API/#news-api","title":"News API","text":"<p>News API is a service that provides access to a wide range of news articles from around the world. The News API provides access to articles from a variety of sources, including newspapers, magazines, and blogs. The API allows users to search for articles by keyword, source, or category.  Link: https://newsapi.org/</p>"},{"location":"API/#gnews-api","title":"GNews API","text":"<p>GNews API is a service that provides access to a wide range of news articles from around the world. The GNews API provides access to articles from a variety of sources, including newspapers, magazines, and blogs. The API allows users to search for articles by keyword, source, or category.  Link: https://gnews.io/</p>"},{"location":"API/#guardian-api","title":"Guardian API","text":"<p>The Guardian API is a service that provides access to a wide range of news articles from The Guardian newspaper. The API provides access to articles from a variety of categories, including news, sports, and entertainment. The API allows users to search for articles by keyword, source, or category.  Link: https://open-platform.theguardian.com/</p>"},{"location":"API/#currents-api","title":"Currents API","text":"<p>Currents API is a service that provides access to a wide range of news articles from around the world. The Currents API provides access to articles from a variety of sources, including newspapers, magazines, and blogs. The API allows users to search for articles by keyword, source, or category.  Link: https://currentsapi.services/en</p>"},{"location":"API/#new-york-times-api","title":"New York Times API","text":"<p>The New York Times API is a service that provides access to a wide range of news articles from The New York Times newspaper. The API provides access to articles from a variety of categories, including news, sports, and entertainment. The API allows users to search for articles by keyword, source, or category.  Link: https://developer.nytimes.com/</p>"},{"location":"API/#ai-nlp-apis","title":"AI &amp; NLP API's","text":""},{"location":"API/#openai","title":"OpenAI","text":"<p>OpenAI is an artificial intelligence research lab that provides access to a wide range of AI models and tools. The OpenAI API provides access to models that can generate text, images, and code, as well as tools for training and deploying AI models.  Link: https://openai.com/</p>"},{"location":"API/#gemini","title":"Gemini","text":"<p>Gemini is an artificial intelligence research lab that provides access to a wide range of AI models and tools. The Gemini API provides access to models that can generate text, images, and code, as well as tools for training and deploying AI models.  Link: https://aistudio.google.com/welcome?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=brand_gemini-eur-sem&amp;utm_id=21341690381&amp;gad_source=1&amp;gclid=Cj0KCQjwm7q-BhDRARIsACD6-fWv8C_yppQsmHMDz25God0iD0BC457_jDYI7J_bUy2s-xkOnArC0McaAvB9EALw_wcB</p>"},{"location":"API/#hugging-face","title":"Hugging Face","text":"<p>Hugging Face is an artificial intelligence research lab that provides access to a wide range of AI models and tools. The Hugging Face API provides access to models that can generate text, images, and code, as well as tools for training and deploying AI models.  Link: https://huggingface.co/</p>"},{"location":"API/#claude-api","title":"Claude API","text":"<p>Claude API is an artificial intelligence research lab that provides access to a wide range of AI models and tools. The Claude API provides access to models that can generate text, images, and code, as well as tools for training and deploying AI models.  Link: https://www.anthropic.com/api</p>"},{"location":"API/#grok-api","title":"Grok API","text":"<p>Grok API is an artificial intelligence research lab that provides access to a wide range of AI models and tools. The Grok API provides access to models that can generate text, images, and code, as well as tools for training and deploying AI models.  Link: https://x.ai/api</p>"},{"location":"API/#sports-apis","title":"Sports API's","text":""},{"location":"API/#football-api","title":"Football API","text":"<p>Football API is a service that provides access to a wide range of football data, including match results, player statistics, and team information. The Football API provides access to data from a variety of leagues and competitions, including the English Premier League, La Liga, and the UEFA Champions League.  Link: https://www.football-data.org/</p>"},{"location":"API/#nba-api","title":"NBA API","text":"<p>NBA API is a service that provides access to a wide range of basketball data, including match results, player statistics, and team information. The NBA API provides access to data from a variety of leagues and competitions, including the NBA, WNBA, and NCAA.  Link: https://rapidapi.com/theapiguy/api/free-nba</p>"},{"location":"API/#all-sports-api","title":"All Sports API","text":"<p>All Sports API is a service that provides access to a wide range of sports data, including match results, player statistics, and team information. The All Sports API provides access to data from a variety of sports, including football, basketball, and baseball.  Link: https://www.allsportsapi.com/</p>"},{"location":"API/#miscellaneous","title":"Miscellaneous","text":""},{"location":"API/#dictionary-api","title":"Dictionary API","text":"<p>Dictionary API is a service that provides access to a wide range of dictionary data, including word definitions, synonyms, and translations. The Dictionary API provides access to data from a variety of languages and dictionaries, including English, Spanish, and French.  Link: https://dictionaryapi.dev/</p>"},{"location":"API/#marvel-api","title":"Marvel API","text":"<p>Marvel API is a service that provides access to a wide range of Marvel comics data, including character information, comic book details, and creator information. The Marvel API provides access to data from a variety of Marvel comics, including  Link: https://developer.marvel.com/</p>"},{"location":"API/#unsplash-api","title":"Unsplash API","text":"<p>Unsplash API is a service that provides access to a wide range of high-quality images, including photographs, illustrations, and vector graphics. The Unsplash API provides access to images from a variety of categories, including nature, architecture, and technology.  Link: https://unsplash.com/developers</p>"},{"location":"API/#qr-code-api","title":"QR Code API","text":"<p>QR Code API is a service that provides access to a wide range of QR code data, including QR code generation, scanning, and decoding. The QR Code API provides access to data from a variety of QR code formats, including URL, text, and contact information.  Link: https://www.qr-code-generator.com/</p>"},{"location":"ai/aws/","title":"AWS","text":""},{"location":"ai/aws/bedrock/","title":"What is Amazon Bedrock?","text":"<p>It's a service that's allows developers to build and scale GenAI. </p>"},{"location":"ai/aws/bedrock/#key-features-of-amazon-bedrock","title":"Key Features of Amazon Bedrock","text":"<ul> <li>Access to multiple foundations models</li> </ul>"},{"location":"ai/aws/bedrock/#amazon-bedrock-foundation-model","title":"Amazon Bedrock foundation model","text":"<p>Allows users to build and scale applications using foundation models.</p>"},{"location":"ai/aws/bedrock/#key-features-of-bedrock-foundation-models","title":"Key features of Bedrock foundation models","text":"<ul> <li>Pre-trained on a diverse range of models available</li> </ul>"},{"location":"ai/aws/bedrock/#example-of-bedrock-foundation-models-available","title":"Example of Bedrock foundation models available","text":"<ul> <li>anthropic</li> <li>ai21labs</li> <li>cohere</li> <li>stabilityAI</li> </ul>"},{"location":"ai/aws/bedrock/#customizable","title":"Customizable","text":"<p>Users can fine-tune the foundation models to perform a wide range of tasks.</p>"},{"location":"ai/aws/bedrock/#serverless","title":"Serverless","text":"<p>Amazon Bedrock is a serverless service.</p>"},{"location":"ai/aws/bedrock/#api-driven","title":"API driven","text":"<p>It enables seamless integration.</p>"},{"location":"ai/aws/bedrock/#use-cases","title":"Use cases","text":"<ul> <li>Text generation</li> <li>Image generation</li> <li>Audio generation</li> <li>Video generation</li> <li>Code generation</li> <li>Data generation</li> </ul>"},{"location":"ai/aws/bedrock/#what-is-foundation-module-evaluation","title":"What is foundation module evaluation?","text":"<p>Process of testing the foundation models to ensure they are performing as expected.</p>"},{"location":"ai/aws/bedrock/#why-we-should-evaluate-foundation-models","title":"Why we should evaluate foundation models?","text":"<p>To ensure the foundation models are performing as expected.</p>"},{"location":"ai/aws/bedrock/#how-to-evaluate-foundation-models","title":"How to evaluate foundation models?","text":"<ul> <li>Run test cases</li> <li>Monitor performance</li> <li>Collect feedback</li> </ul>"},{"location":"ai/aws/bedrock/#classification-of-foundation-models","title":"Classification of foundation models","text":"<ul> <li>Accuracy: For classification or decision making task.</li> <li>Bleu or Rouge: For text generation task.</li> <li>F1: For precision and recall task.</li> <li>Human evaluation: For subjective tasks like image, audio, video generation.</li> <li>Latency: For real-time applications.</li> <li>Cost: For cost-sensitive applications.</li> </ul>"},{"location":"ai/aws/bedrock/#guardrails","title":"GuardRails","text":"<ul> <li>Control the interaction between the model and the user.</li> <li>Filter out inappropriate content.</li> <li>Remove personaly identifiable information.</li> <li>Enhanced privacy and security.</li> <li>Reduce hallucinations and biases.</li> </ul>"},{"location":"ai/aws/bedrock/#agents","title":"Agents","text":"<ul> <li>Enable developers to create and manage autonomous agents of executing complex multi-step tasks.</li> </ul>"},{"location":"ai/aws/bedrock/#cloudwatch","title":"CloudWatch","text":"<ul> <li>Monitor and troubleshoot the performance of the foundation models.</li> </ul>"},{"location":"ai/aws/bedrock/#studio","title":"Studio","text":"<ul> <li>Visual interface to manage and monitor the foundation models.</li> </ul>"},{"location":"ai/aws/bedrock/#whatermark-detection","title":"Whatermark detection","text":"<ul> <li>Check if an image was generated by Amazon Titan</li> </ul>"},{"location":"ai/aws/bedrock/#pricing","title":"Pricing","text":""},{"location":"ai/aws/bedrock/#on-demand","title":"On-demand","text":"<ul> <li>Pay as you go</li> <li>Text model: Charge for every input/output</li> <li>Embedding model: Charge for every generated input</li> <li>Image model: Charge for every generated image</li> </ul>"},{"location":"ai/aws/bedrock/#batch","title":"Batch","text":"<ul> <li>Can provide 50% discount</li> </ul>"},{"location":"ai/aws/bedrock/#provisioned-throughput","title":"Provisioned Throughput","text":"<ul> <li>Purchase model: 1 month, 6 months</li> <li>Throughput: max number of input/output</li> <li>Works with base, fine-tuned, and custom models</li> </ul>"},{"location":"ai/aws/sagemaker/","title":"AWS sagemaker","text":""},{"location":"ai/aws/sagemaker/#about","title":"About","text":"<p>End-to-end machine learning service that enables data scientists, developers, and machine learning experts to build, train, and deploy machine learning models quickly.</p> <ul> <li>Collect and prepare data</li> <li>Build and train machine learning models</li> <li>Deploy and monitor the performance of the predictions</li> </ul>"},{"location":"ai/aws/sagemaker/#built-in-algorithms-extract","title":"Built-in algorithms (extract)","text":"<ul> <li>Supervised algorithms</li> <li>Linear regression</li> <li>Classification</li> <li>KNN Algorithm</li> <li>Unsupervised algorithms</li> <li>Principal component analysis</li> <li>K-means</li> <li>Anomaly detection</li> <li>textual algorithms</li> <li>Natural language processing</li> <li>Summarization</li> <li>image processing</li> <li>Classification</li> <li>Detection</li> </ul>"},{"location":"ai/aws/sagemaker/#model-deployment-e-inference","title":"Model deployment e inference","text":"<ul> <li>Deploy models in real-time or batch mode</li> <li>Monitor the performance of the deployed models</li> <li>Scale the deployed models</li> <li>Managed solution: reduce the operational overhead</li> <li>Real-time</li> <li>One prediction at a time</li> <li>Serverless</li> <li>No need to manage the infrastructure</li> </ul>"},{"location":"ai/aws/sagemaker/#deployment-types","title":"Deployment types","text":"<ul> <li>Real-time</li> <li>Fast</li> <li>Latency: low</li> <li>near instant predictions</li> <li>Batch<ul> <li>Large datasets</li> <li>Latency: high</li> <li>High throughput</li> <li>Asynchronous</li> </ul> </li> <li>Serverless<ul> <li>No need to manage the infrastructure</li> <li>Latency: low</li> <li>Pay for what you use</li> <li>Auto-scaling</li> </ul> </li> <li>Asynchronous<ul> <li>Large datasets</li> <li>Latency: medium/high</li> <li>High throughput</li> <li>Asynchronous</li> </ul> </li> </ul>"},{"location":"ai/aws/sagemaker/#sagemaker-studio","title":"SageMaker Studio","text":"<ul> <li>Deploy ML models</li> <li>Team collaboration</li> <li>Tune and debug ML models</li> <li>Automate ML workflows</li> <li>End-to-end ML development</li> </ul>"},{"location":"ai/aws/sagemaker/canvas/","title":"Sage Maker - Canvas","text":""},{"location":"ai/aws/sagemaker/canvas/#what-is-amazon-sage-maker-canvas","title":"What is Amazon Sage Maker Canvas?","text":"<p>It's a service that allows developers to build, train, and deploy machine learning models.</p> <ul> <li>For DEV to build a visual interface for ML models</li> <li>Access to ready to use</li> </ul>"},{"location":"ai/aws/sagemaker/clarify/","title":"Sage Maker - Clarify","text":""},{"location":"ai/aws/sagemaker/clarify/#what-is-amazon-sage-maker-clarify","title":"What is Amazon Sage Maker Clarify?","text":"<p>It's a service that allows developers to detect bias in machine learning models.</p> <ul> <li>Evaluate foundation models</li> <li>Evaluate human-factors such as friendliness, fairness, and explainability</li> <li>Leverages an AWS manage team or bring your own team</li> </ul> <p>Identify potentials biases in datasets and machine learning (ML) to ensure fairness and transparency.</p>"},{"location":"ai/aws/sagemaker/data-wrangler/","title":"Sage Maker - Data Wrangler","text":""},{"location":"ai/aws/sagemaker/data-wrangler/#what-is-amazon-sage-maker-data-wrangler","title":"What is Amazon Sage Maker Data Wrangler?","text":"<p>It's a service that allows developers to prepare data for machine learning models.</p>"},{"location":"ai/aws/sagemaker/data-wrangler/#key-features-of-amazon-sage-maker-data-wrangler","title":"Key Features of Amazon Sage Maker Data Wrangler","text":"<ul> <li>Access to multiple data sources</li> <li>Data transformation</li> <li>Data cleaning</li> <li>Data visualization</li> <li>Data profiling</li> <li>Data validation</li> <li>Data export</li> <li>Data import</li> </ul>"},{"location":"ai/aws/sagemaker/ground-truth/","title":"Sage Maker - Ground Truth","text":""},{"location":"ai/aws/sagemaker/ground-truth/#what-is-amazon-sage-maker-ground-truth","title":"What is Amazon Sage Maker Ground Truth?","text":"<p>It's a service that allows developers to build highly accurate training datasets for machine learning models.</p> <ul> <li>Labeling data using human force</li> <li>Reinforcement learning from human feedback</li> <li>Human feedback for ML</li> </ul>"},{"location":"ai/aws/sagemaker/jumpstart/","title":"Sage Maker - Jumpstart","text":""},{"location":"ai/aws/sagemaker/jumpstart/#what-is-amazon-sage-maker-jumpstart","title":"What is Amazon Sage Maker Jumpstart?","text":"<p>It's a service that allows developers to quickly build, train, and deploy machine learning models.</p> <ul> <li>ML Hub to find pre-trained</li> <li>ML Solutions</li> <li>Access and browse</li> <li>Select and deploy</li> </ul>"},{"location":"ai/aws/sagemaker/ml-governance/","title":"Sage Maker - ML Governance","text":""},{"location":"ai/aws/sagemaker/ml-governance/#what-is-amazon-sage-maker-model-governance","title":"What is Amazon Sage Maker Model Governance?","text":"<p>It's a service that allows developers to manage, monitor, and govern machine learning models.</p> <ul> <li>Model card</li> <li>Information</li> <li>Model dashboard</li> <li>centralize repository</li> <li>info insights</li> <li>Role manner</li> <li>Defile roles for persona</li> </ul>"},{"location":"ai/aws/sagemaker/model-monitor/","title":"Sage Maker - Model Monitor","text":""},{"location":"ai/aws/sagemaker/model-monitor/#what-is-amazon-sage-maker-model-monitor","title":"What is Amazon Sage Maker Model Monitor?","text":"<p>It's a service that allows developers to monitor machine learning models.</p> <ul> <li>Monitor model quality</li> <li>Alert for deviations</li> </ul>"},{"location":"ai/aws/sagemaker/pipeline/","title":"Sage Maker - Pipeline","text":""},{"location":"ai/aws/sagemaker/pipeline/#what-is-amazon-sage-maker-pipeline","title":"What is Amazon Sage Maker Pipeline?","text":"<p>It's a service that allows developers to automate and manage machine learning workflows.</p> <ul> <li>Pipeline composed of steps</li> <li>Supported steps</li> <li>Processing: Data processing</li> <li>Training: Training model</li> <li>Evaluation: Evaluate model</li> <li>Register</li> <li>Condition</li> <li>Transform</li> <li>Schedule</li> </ul>"},{"location":"ai/basics/","title":"Basics","text":""},{"location":"ai/basics/#ai","title":"AI","text":"<p>AI is a broad field of computer science that focuses on creating machines that can perform tasks that require human intelligence. AI can be classified into two categories: narrow AI and general AI.</p>"},{"location":"ai/basics/#ai-scopes","title":"AI Scopes","text":"<p>AI have a wide range of technologies and techniques, including machine learning, natural language processing (NLP), robotics, computer vision.</p>"},{"location":"ai/basics/#ai-includes","title":"AI includes","text":"<ul> <li>Machine learning</li> <li>Deep learning</li> <li>Natural language processing</li> <li>Computer vision</li> <li>Robotics</li> <li>Expert systems</li> <li>Speech recognition</li> <li>Planning and scheduling</li> <li>Knowledge representation</li> </ul>"},{"location":"ai/basics/#what-is-a-foundation-model","title":"What is a foundation model?","text":"<p>They are large models that are trained on a diverse range of tasks and data. They can be fine-tuned to perform a wide range of tasks.</p>"},{"location":"ai/basics/#example-of-foundation-models","title":"Example of foundation models","text":"<ul> <li>anthropic</li> <li>ai21labs</li> <li>cohere</li> <li>stabilityAI</li> </ul>"},{"location":"ai/basics/prompt-engineering/","title":"Prompt Engineering","text":"<p>Practice of crafting effective prompts for AI models to generate desired outputs. The goals is to structure input in a way that elicits accurate, creative and desired responses from the model.</p>"},{"location":"ai/basics/prompt-engineering/#enhanced-prompt-engineering","title":"Enhanced Prompt Engineering","text":"<p>Enhanced prompt engineering is the process of optimizing the input to the model to generate the desired output. It involves crafting prompts that are clear, concise, and relevant to the task at hand. Enhanced prompt engineering can help improve the performance of the model and ensure that it generates accurate and relevant responses.</p> <ul> <li>Instructions: Provide clear and concise instructions to the model to guide its response.</li> <li>Context: Provide relevant context to the model to help it understand the task at hand.</li> <li>Input: Structure the input in a way that is easy for the model to understand and process.</li> <li>Output: Define the desired output to guide the model in generating the response.</li> </ul>"},{"location":"ai/basics/prompt-engineering/#negative-prompt-engineering","title":"Negative Prompt Engineering","text":"<p>Technique where you explicitly instruct the model on what not to include or do in its response.</p>"},{"location":"ai/basics/prompt-engineering/#helps-to","title":"Helps to","text":"<ul> <li>Avoid unwanted content</li> <li>maintain focus</li> <li>Enhance clarity</li> </ul>"},{"location":"ai/basics/prompt-engineering/#prompt-performance-optimization","title":"Prompt performance optimization","text":"<ul> <li>Top P: consider likely words or broad range</li> <li>Temperature: control creativity</li> <li>Top K: Limit the number of probable words</li> <li>Legit: Maximum length of the answer</li> <li>Stop sequences: Tokens that signal the model to stop generated output</li> </ul>"},{"location":"ai/basics/prompt-engineering/#prompt-latency","title":"Prompt latency","text":"<ul> <li>Latency is how fast the model responds</li> <li>Its impacted by a few parameters</li> <li>model size</li> <li>model type itself</li> <li>number of tokens (input)</li> <li>number of tokens (output)</li> <li>Latency is not impacted by top P, top K or temperature</li> </ul>"},{"location":"ai/basics/prompt-engineering/#prompt-engineering-techniques","title":"Prompt engineering techniques","text":"<ul> <li>zero-shot prompt: Task to the model without providing examples</li> <li>Few-shot prompt: Task to the model with a few examples</li> <li>Chain of thought: Provide a series of prompts to the model to generate a coherent response</li> <li>Divide the task into a sequence</li> <li>Helpful when solving a problem that requires multiple steps</li> </ul>"},{"location":"ai/basics/prompt-engineering/#example-of-chain-of-thought","title":"Example of Chain of thought","text":"<ul> <li>Task: Write a story about a detective solving</li> <li>FIRST</li> <li>THEN</li> <li>NEXT</li> <li>FINALLY</li> </ul> <p>Think step by step</p>"},{"location":"ai/basics/prompt-engineering/#rag-and-prompt","title":"RAG and prompt","text":"<ul> <li>Combine the models capability with external data sources to generate more accurate and relevant responses</li> <li>The initial prompt is then augmented with the external info</li> </ul>"},{"location":"ai/basics/prompt-engineering/#prompt-templates","title":"Prompt templates","text":"<ul> <li>Simplify and standardize the prompt creation process</li> <li>Reusable structure</li> <li>dynamic inputs</li> <li>consistency</li> <li>scalability</li> </ul>"},{"location":"ai/basics/prompt-engineering/#where-to-use-prompt-templates","title":"Where to use Prompt Templates","text":"<ul> <li>Customer support chatbots</li> <li>content generation</li> <li>data extraction</li> <li>analysis</li> </ul>"},{"location":"ai/basics/prompt-engineering/#prompt-templates-injection","title":"Prompt templates injection","text":"<ul> <li>Ignoring the prompt template attack</li> </ul>"},{"location":"ai/basics/prompt-engineering/#how-to-protect","title":"How to protect","text":"<ul> <li>Add explicit instructions instroction to ignore any unrelated malicious content</li> </ul>"},{"location":"ai/basics/rag/","title":"RAG: Retrieval Augmented Generation","text":"<p>It's the process of optimizing the output of a large language model. </p>"},{"location":"ai/gen-ai/","title":"What is GenAI?","text":"<p>AI that's generate new content like text, image, audio and video. </p>"},{"location":"ai/gen-ai/#examples","title":"Examples","text":"<p>Example of GenAI are GPT-3, DALL-E, OpenAI Codex, etc.</p>"},{"location":"ai/gen-ai/#genai-concepts","title":"GenAI Concepts","text":"<ul> <li>Tokenization: Converting raw text into tokens.</li> </ul>"},{"location":"ai/gen-ai/#why-we-should-tokenize","title":"Why we should tokenize","text":"<ul> <li>Tokenization convert it into a format that the model can understand.</li> <li>It creates a mapping between the raw input text and the model vocabulary.</li> </ul>"},{"location":"ai/gen-ai/#context-window","title":"Context window","text":"<ul> <li>It's the number of tokens that the model considers when generating the next token.</li> <li>It helps the model to understand the context of the input text.</li> <li>Accuracy and relevance: The context window helps the model to generate more accurate and relevant output.</li> <li>Cost and performance: The context window affects the cost and performance of the model.</li> </ul>"},{"location":"ai/gen-ai/#embedding","title":"Embedding","text":"<ul> <li>It's the process of converting tokens into vectors.</li> <li>Vectors have a high dimensionality to capture many features for one input token.</li> <li>Embedding models can power search applications.</li> </ul>"},{"location":"ai/ml/","title":"Machine Learning (ML)","text":"<p>Machine learning is a subset of artificial intelligence (AI) that focuses on developing algorithms that allow computers to learn from and make predictions or decisions based on data. Machine learning algorithms can be classified into three categories: supervised learning, unsupervised learning, and reinforcement learning.</p>"},{"location":"ai/ml/#deep-learning","title":"Deep Learning","text":"<p>Deep learning is a subset of machine learning that focuses on developing neural networks with multiple layers to model complex patterns in large datasets. Deep learning algorithms have been used to achieve state-of-the-art performance in various tasks, such as image recognition, natural language processing, and speech recognition.</p> <ul> <li>Uses large, layered neural networks to learn from a massive amount of data</li> <li>neural network uses multiple layers</li> <li>Input layer: Where raw data is fed into the model</li> <li>Hidden layers: Intermediate layers that analyze and transform the data</li> <li>Output layer: The final layer that produces the model's predictions</li> </ul>"},{"location":"ai/ml/#deep-learning-example","title":"Deep learning example","text":"<ul> <li>Image recognition</li> <li>Natural language processing</li> <li>Speech recognition</li> <li>Autonomous vehicles</li> <li>Healthcare</li> <li>Finance</li> </ul>"},{"location":"ai/ml/#ml-terms","title":"ML Terms","text":""},{"location":"ai/ml/#gpt","title":"GPT","text":"<p>Generative Pre-trained Transformer (GPT) is a type of deep learning model that uses transformers to generate human-like text. GPT models have been used for various natural language processing tasks, such as text generation, translation, and summarization.</p>"},{"location":"ai/ml/#bert","title":"bert","text":"<p>Bidirectional Encoder Representations from Transformers (BERT) is a type of deep learning model that uses transformers to understand the context of words in a sentence. BERT models have been used for various natural language processing tasks, such as question answering, sentiment analysis, and named entity recognition.</p>"},{"location":"ai/ml/#rnn","title":"RNN","text":"<p>Recurrent Neural Networks (RNNs) are a type of neural network that is designed to handle sequential data. RNNs have been used for various tasks, such as speech recognition, language modeling, and machine translation.</p>"},{"location":"ai/ml/#resnet","title":"ResNet","text":"<p>Residual Networks (ResNets) are a type of deep learning model that uses residual connections to enable the training of very deep neural networks. ResNets have been used for various computer vision tasks, such as image classification, object detection, and image segmentation.</p>"},{"location":"ai/ml/#svm","title":"SVM","text":"<p>Support Vector Machines (SVMs) are a type of supervised learning algorithm that is used for classification and regression tasks. SVMs work by finding the optimal hyperplane that separates the data points into different classes.</p>"},{"location":"ai/ml/#wavenet","title":"Wavenet","text":"<p>WaveNet is a type of deep learning model that uses dilated convolutions to generate high-quality audio waveforms. WaveNet models have been used for various audio processing tasks, such as speech synthesis, music generation, and noise reduction.</p>"},{"location":"ai/ml/#gan","title":"Gan","text":"<p>Generative Adversarial Networks (GANs) are a type of deep learning model that consists of two neural networks: a generator and a discriminator. GANs are used to generate realistic data samples, such as images, audio, and text.</p>"},{"location":"ai/ml/#xgboost","title":"XGBoost","text":"<p>Extreme Gradient Boosting (XGBoost) is a type of ensemble learning algorithm that is used for classification and regression tasks. XGBoost works by combining multiple weak learners to create a strong learner that can make accurate predictions.</p>"},{"location":"ai/ml/training-data/","title":"Training Data","text":"<ul> <li>We must have a good data</li> <li>Garbage in =&gt; Garbage out</li> <li>Critical part to build a good model</li> <li>Several options to model our data</li> <li>Labeled vs unlabeled data</li> <li>Structure vs unstructured data</li> </ul>"},{"location":"ai/ml/training-data/#labeled-data","title":"Labeled data","text":"<p>Data that has been tagged with one or more labels. Used to train supervised machine learning models</p>"},{"location":"ai/ml/training-data/#unlabeled-data","title":"Unlabeled data","text":"<p>Data that has not been tagged with any labels. Used to train unsupervised machine learning models</p>"},{"location":"ai/ml/training-data/#structure-data","title":"Structure data","text":"<p>Data that is organized in a specific format, such as a table or a database. Used to train structured machine learning models</p> <ul> <li>Tabular data</li> <li>Time series data</li> </ul>"},{"location":"ai/ml/training-data/#unstructured-data","title":"Unstructured data","text":"<p>Data that does not have a specific format, such as text, images, or audio. Used to train unstructured machine learning models</p>"},{"location":"ai/ml/training-data/#types","title":"Types","text":"<ul> <li>Regression: Predict a continuous value</li> <li>Predict the price of a house</li> <li>Classification: Predict a category</li> <li>Predict whether an email is spam or not</li> </ul>"},{"location":"ai/ml/training-data/#training-vs-validation-vs-test-set","title":"Training vs Validation vs Test set","text":"<ul> <li>Training set: Data used to train the model</li> <li>Validation set: Data used to tune the model hyperparameters</li> <li>Test set: Data used to evaluate the model performance</li> </ul>"},{"location":"ai/ml/training-data/#feature-engineering","title":"Feature Engineering","text":"<p>The process of using Domain knowledge to select and transform raw data into meaning. </p> <ul> <li>We can do feature Engineering on structured data</li> </ul>"},{"location":"ai/ml/training-data/#unsupervised-learning-k-mean","title":"Unsupervised Learning (K-mean)","text":"<ul> <li>No labeled data</li> <li>Data set contains only features</li> <li>Usage (isolation Forest)</li> <li>Market basket analysis</li> <li>Fraud detection</li> </ul>"},{"location":"ai/ml/training-data/#semi-supervised-learning","title":"Semi supervised Learning","text":"<ul> <li>Small amount of labeled data</li> <li>Huge amount of unlabeled data</li> <li>Generate humans label without humans</li> </ul>"},{"location":"ai/ml/training-data/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>Learn from the environment</li> <li>Learn from the feedback</li> <li>Learn from the reward</li> <li>Usage</li> <li>Game playing</li> <li>Robotics</li> <li>Self driving cars</li> </ul>"},{"location":"aws/ami/","title":"How to import an OVA image into AWS EC2","text":""},{"location":"aws/ami/#start-machine-on-virtualization-platform","title":"Start machine on virtualization platform","text":""},{"location":"aws/ami/#do-the-installation","title":"Do the installation","text":""},{"location":"aws/ami/#export-vdi-file-into-ova","title":"Export vdi file into OVA","text":""},{"location":"aws/ami/#import-ova-into-aws-ami","title":"Import OVA into AWS AMI","text":""},{"location":"aws/ami/#untar-ova-file","title":"Untar ova file","text":"<pre><code>tar -xvf rocky10-arm-kde.ova\n</code></pre>"},{"location":"aws/ami/#send-vmdk-file-to-aws-s3-bucket","title":"Send vmdk file to AWS S3 bucket","text":"<pre><code>aws s3 cp rocky10-arm-kde-disk001.vmdk s3://my-bucked-name/\n</code></pre>"},{"location":"aws/ami/#create-role","title":"create role","text":"<pre><code>aws iam create-role --role-name vmimport \\\n  --assume-role-policy-document '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n          \"Service\": \"vmie.amazonaws.com\"\n        },\n        \"Action\": \"sts:AssumeRole\",\n        \"Condition\": {\n          \"StringEquals\": {\n            \"sts:ExternalId\": \"vmimport\"\n          }\n        }\n      }\n    ]\n  }'\n</code></pre>"},{"location":"aws/ami/#create-a-containersjson","title":"create a containers.json","text":"<pre><code>[\n  {\n    \"Description\": \"Imported Rocky 10 OVA Image\",\n    \"Format\": \"vmdk\",\n    \"UserBucket\": {\n      \"S3Bucket\": \"kwaza-desktop-images\",\n      \"S3Key\": \"rocky10-arm-kde-disk001.vmdk\"\n    }\n  }\n]\n</code></pre>"},{"location":"aws/ami/#import-vmdk-file-into-aws-ami","title":"Import vmdk file into AWS AMI","text":"<p>```bash aws ec2 import-image --description \"Import rocky-10\" --disk-containers  file://containers.json</p>"},{"location":"aws/ami/win11/","title":"Create win11 AMI","text":""},{"location":"aws/ami/win11/#prerequisites","title":"Prerequisites","text":"<ul> <li>I\u00b4m using a Linux machine running Ubuntu 22.04 LTS.</li> <li>VirtualBox installed on your local machine.</li> <li>An AWS account with necessary permissions to create roles, S3 buckets, and import images.</li> <li>AWS CLI installed and configured on your local machine.</li> </ul>"},{"location":"aws/ami/win11/#first-challenge","title":"First challenge","text":"<p>You can download a Windows 11 ISO from Microsoft's official website. However, to create an AMI, you need to convert the ISO into a format that AWS supports, such as VMDK or VHD.</p>"},{"location":"aws/ami/win11/#step-1-download-windows-11-iso","title":"Step 1: Download Windows 11 ISO","text":"<p>You can download the Windows 11 ISO from the Microsoft website.</p>"},{"location":"aws/ami/win11/#step-2-convert-iso-to-vmdk-or-vhd","title":"Step 2: Convert ISO to VMDK or VHD","text":"<p>You can use tools like VirtualBox or qemu-img to convert the ISO to VMDK or VHD format.</p>"},{"location":"aws/ami/win11/#using-virtualbox","title":"Using VirtualBox","text":"<ol> <li>Create a new virtual machine in VirtualBox and attach the Windows 11 ISO as a bootable disk.</li> <li>Install Windows 11 on the virtual machine.</li> <li>It will ask for a product key, you can skip this step and continue the installation.</li> <li>Once the installation is complete, shut down the VM.</li> </ol>"},{"location":"aws/ami/win11/#preparation-before-exporting","title":"Preparation before exporting","text":"<ol> <li>Enable RDP on the Windows 11 VM.</li> </ol>"},{"location":"aws/ami/win11/#second-challenge","title":"Second challenge","text":""},{"location":"aws/ami/win11/#step-3-export-the-vm-to-ova","title":"Step 3: Export the VM to OVA","text":"<ol> <li>In VirtualBox, select the VM and go to <code>File</code> &gt; <code>Export Appliance</code>.</li> <li>Choose the VM you created and follow the prompts to export it as an OVA file.</li> <li>Choose OVF 1.0 as the format and complete the export process. (AWS supports OVF 1.0)</li> <li>From MAC Address Policy, choose Strip all network adapter MAC addresses. (When you import the VM to AWS, it will assign new MAC addresses to the network interfaces.)</li> </ol>"},{"location":"aws/ami/win11/#step-4-upload-the-ova-to-s3","title":"Step 4: Upload the OVA to S3","text":"<ol> <li>Create an S3 bucket in your AWS account to store the OVA file.</li> <li>Upload the OVA file to the S3 bucket using the AWS Management Console or AWS CLI.</li> </ol>"},{"location":"aws/ami/win11/#third-challenge","title":"Third challenge","text":""},{"location":"aws/ami/win11/#step-5-create-an-iam-role-for-vm-importexport","title":"Step 5: Create an IAM Role for VM Import/Export","text":"<ol> <li>Create a role named <code>vmimport</code> with the following trust policy:</li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"vmie.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"sts:ExternalId\": \"vmimport\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <ol> <li>Attach the following policy to the <code>vmimport</code> role:</li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetBucketLocation\",\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::your-bucket-name\",\n        \"arn:aws:s3:::your-bucket-name/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:ModifySnapshotAttribute\",\n        \"ec2:CopySnapshot\",\n        \"ec2:RegisterImage\",\n        \"ec2:Describe*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"aws/ami/win11/#step-6-create-a-json-file-for-the-import-task","title":"Step 6: Create a JSON file for the import task","text":"<p>Create a file named <code>containers.json</code> with the following content:</p> <pre><code>[\n  {\n    \"Description\": \"Imported Windows 11 Image\",\n    \"Format\": \"vmdk\",\n    \"UserBucket\": {\n      \"S3Bucket\": \"your-bucket-name\",\n      \"S3Key\": \"your-ova-file-name.ova\"\n    }\n  }\n]\n</code></pre>"},{"location":"aws/ami/win11/#step-7-import-the-ova-to-create-an-ami","title":"Step 7: Import the OVA to create an AMI","text":"<p>Use the following AWS CLI command to import the OVA file and create an AMI:</p> <pre><code>aws ec2 import-image \\\n    --description \"Windows 11 Image\" \\\n    --disk-containers file://containers.json \\\n    --boot-mode uefi\n</code></pre>"},{"location":"aws/ami/win11/#step-8-monitor-the-import-task","title":"Step 8: Monitor the import task","text":"<p>You can monitor the progress of the import task using the following command:</p> <pre><code>aws ec2 describe-import-image-tasks --import-task-ids import-ami-xxxxxxxxxxxx\n</code></pre>"},{"location":"aws/architecting-to-scale/","title":"Architecting to Scale","text":""},{"location":"aws/architecting-to-scale/#loosely-coupled-architectures","title":"Loosely Coupled Architectures","text":"<p>Components in a loosely coupled architecture are designed to be independent and interact with each other through well-defined interfaces. This approach allows for greater flexibility, scalability, and resilience, as components can be developed, deployed, and scaled independently. Loosely coupled architectures are often implemented using microservices, serverless computing, or event-driven architectures.</p>"},{"location":"aws/architecting-to-scale/#scaling-up","title":"Scaling Up","text":"<ul> <li>Add more CPU or RAM to an existing instance.</li> <li>Requires restart to scale up or down</li> <li>Would require scripting to automate</li> <li>Limited by instance sizes</li> </ul>"},{"location":"aws/architecting-to-scale/#scaling-out","title":"Scaling Out","text":"<ul> <li>Add more instances as demand increases</li> <li>No downtime required</li> <li>Automatic scaling available for compute services</li> <li>Theoretically unlimited scaling</li> </ul>"},{"location":"aws/architecting-to-scale/#questions-about-scaling","title":"Questions about scaling","text":"<ul> <li>Are you using the appropriate service?</li> <li>Are you scaling with the appropriate metric?</li> </ul>"},{"location":"aws/architecting-to-scale/#event-driven-architecture","title":"Event-driven architecture","text":"<ul> <li>Components communicate through events</li> <li>Decoupled and asynchronous</li> <li>Scalable and resilient</li> <li>Can be implemented using services like Amazon SNS, SQS, and Lambda</li> </ul>"},{"location":"aws/architecting-to-scale/#outputs","title":"Outputs","text":"<ul> <li>Auto scaling groups:</li> <li>Know the different scaling policies and options</li> <li>Understand the difference and limitations between horizontal and vertical scaling</li> <li>Know what a cool down period is and how it might impact your responsiveness to demand</li> <li>Kinesis:</li> <li>Exam is likely to be restricted to the data stream use cases for kinesis such as data stream and firehose</li> <li>understand shard concept and how partition keys and sequences enabled shards to manage data</li> <li>DynamoDB Auto Scaling:<ul> <li>know the new and old terminology and concept of a partition, partition key, and sort key</li> <li>Understand how DynamoDB calculates capacity units</li> </ul> </li> <li>CloudFront Part 2:</li> <li>Know that both static and dynamic content is supported</li> <li>Understand possible origins and how multiple origins can be used together with behaviors</li> <li>Know invalidation methods, zone apex and geo-restriction</li> <li>SNS:</li> <li>Understand a loosely coupled architecture</li> <li>Know the different types of topics and how they can be used</li> <li>SQS:</li> <li>Understand the different types of queues and how they can be used</li> <li>Know the different types of message delivery and how they can be used</li> <li>Lambda:</li> <li>Understand the different types of triggers and how they can be used</li> <li>Know the different types of functions and how they can be used</li> <li>SWF:</li> <li>Understand the different types of workflows and how they can be used</li> <li>Best suited for human-enabled workflows like order fulfillment or procedural requests</li> <li>Elastic MapReduce:</li> <li>Understand the different types of clusters and how they can be used</li> <li>Know the different types of nodes and how they can be used</li> <li>Step functions:</li> <li>Managed workflow and orchestration platform considered preferred for modern development orver AWS Simple workflow service</li> <li>Supports tasks, sequential and parallel execution</li> <li>AWS Batch<ul> <li>Ideal for use cases where a routine activity must be performed at a specific interval or time of day</li> <li>Behind the scenes, it uses EC2 instances to run the jobs</li> </ul> </li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/","title":"Auto-Scaling","text":"<p>Auto-scaling is a method used to automatically adjust the number of compute resources in a system based on the current load. This allows for the system to handle varying levels of traffic without manual intervention. Auto-scaling can be used to scale up or down based on the current demand.</p>"},{"location":"aws/architecting-to-scale/auto-scaling/#amazon-ec2-auto-scaling","title":"Amazon EC2 Auto Scaling","text":"<p>Amazon EC2 Auto Scaling is a service that automatically adjusts the number of EC2 instances in a group based on the current demand. It can be used to maintain a desired number of instances, automatically replace unhealthy instances, and scale out or in based on the current load.</p>"},{"location":"aws/architecting-to-scale/auto-scaling/#application-auto-scaling","title":"Application Auto Scaling","text":"<p>Application Auto Scaling is a service that allows you to automatically adjust the capacity of various AWS services based on the current demand. It supports services such as DynamoDB, ECS, and Lambda.</p>"},{"location":"aws/architecting-to-scale/auto-scaling/#aws-auto-scaling","title":"AWS Auto Scaling","text":"<p>AWS Auto Scaling is a unified service that allows you to automatically scale multiple AWS resources based on demand. It can be used to scale EC2 instances, ECS tasks, DynamoDB tables, and more.</p>"},{"location":"aws/architecting-to-scale/auto-scaling/#ec2-auto-scaling-groups","title":"EC2 Auto-Scaling Groups","text":"<ul> <li>Automatically provides horizontal scaliing (scale-out) for your landscape.</li> <li>Triggered by an event or scaling action to either launch or terminate instances.</li> <li>Availability Cost and System metrics can all factor into scaling</li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/#scaling-options","title":"Scaling Options","text":"<ul> <li>Manual Scaling: You manually adjust the number of instances in the group.</li> <li>Scheduled Scaling: You schedule the scaling actions based on a time or date.</li> <li>Dynamic Scaling: You define the scaling policies based on the demand.</li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/#scaling-policies","title":"Scaling Policies","text":"<ul> <li>Target Tracking Scaling: Scales the number of instances in the group to maintain a target metric.</li> <li>Step Scaling: Scales the number of instances based on a set of scaling adjustments.</li> <li>Simple Scaling: Scales the number of instances based on a single scaling adjustment.</li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/#scaling-cooldown-concept-for-ec2","title":"Scaling Cooldown Concept for EC2","text":"<ul> <li>Configurable time period after a scaling activity where further scaling activities are not allowed.</li> <li>Default is 300 seconds.</li> <li>Automatically applied to dynamic scaling policies.</li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/#application-auto-scaling_1","title":"Application Auto Scaling","text":"<ul> <li>Supports services like DynamoDB, ECS, and Lambda.</li> <li>Allows you to automatically adjust the capacity of various AWS services based on the current demand.</li> <li>Can be used to scale resources based on predefined schedules or custom metrics.</li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/#application-auto-scaling-policies","title":"Application Auto Scaling policies","text":"<ul> <li>Target Tracking Scaling: Scales the resource to maintain a target value.</li> <li>Step Scaling: Scales the resource based on a set of scaling adjustments.</li> <li>Scheduled Scaling: Scales the resource based on a schedule.</li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/#aws-auto-scaling_1","title":"AWS Auto Scaling","text":"<ul> <li>Unified service that allows you to automatically scale multiple AWS resources based on demand.</li> <li>Can be used to scale EC2 instances, ECS tasks, DynamoDB tables, and more.</li> <li>Provides a simple and consistent experience for scaling across different services.</li> </ul>"},{"location":"aws/architecting-to-scale/auto-scaling/#aws-predictive-scaling","title":"AWS Predictive Scaling","text":"<p>Predictive scaling is a feature of AWS Auto Scaling that uses machine learning to predict the demand for your resources. It can be used to automatically adjust the capacity of your resources based on predicted demand, helping to improve performance and reduce costs.</p>"},{"location":"aws/architecting-to-scale/cloudfront/","title":"CloudFront","text":"<ul> <li>Can deliver content to your users faster by caching at edge locations</li> <li>Dynamic content delivery is archieved using HTTP cookies forwarded to the origin</li> <li>Supports adobe Flash Media Server RTMP protocol but you have to choose RTMP delivery method</li> <li>Web distribution also support media streaming using and live streaming using but use HTTP or HTTPS</li> <li>Origins can be S3, EC2, ELB or another web server</li> <li>Multiple origins can be specified for a distribution</li> <li>Use behaviors to specify how CloudFront should handle requests for different paths</li> </ul>"},{"location":"aws/architecting-to-scale/cloudfront/#invalidating-objects","title":"Invalidating Objects","text":"<ul> <li>You can invalidate objects in the cache using the AWS Management Console, the CloudFront API, or the AWS SDKs</li> <li>Invalidation is the process of removing objects from the cache before they expire</li> <li>You can invalidate objects one at a time or all at once</li> <li>Invalidation is not instant and can take time to complete</li> </ul>"},{"location":"aws/architecting-to-scale/compute-optimizer/","title":"Compute optimizer","text":""},{"location":"aws/architecting-to-scale/compute-optimizer/#overview","title":"Overview","text":"<p>Compute Optimizer is a service that recommends optimal AWS resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Compute Optimizer recommends optimal configurations for Amazon EC2 instances and Auto Scaling groups, and it provides performance improvement recommendations for EC2 instances.</p>"},{"location":"aws/architecting-to-scale/compute-optimizer/#benefits","title":"Benefits","text":"<ul> <li>Cost Optimization: Compute Optimizer helps you reduce costs by identifying optimal resource configurations that match your workloads.</li> <li>Performance Improvement: Compute Optimizer provides recommendations to improve the performance of your workloads.</li> <li>Resource Utilization: Compute Optimizer analyzes historical utilization metrics to recommend optimal resource configurations.</li> </ul>"},{"location":"aws/architecting-to-scale/compute-optimizer/#outputs","title":"Outputs","text":"<ul> <li>Compte Optimizer Uses Machine Learning to analyze historical utilization metrics.</li> <li>More than just an EC2 instance, it can also provide recommendations for rdS, DynamoDB, and Lambda.</li> </ul>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/","title":"DynamoDB Scaling","text":"<p>Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to deliver single-digit millisecond performance at any scale, making it ideal for applications that require low-latency data access.</p>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#types-of-scaling","title":"Types of Scaling","text":"<p>DynamoDB provides two types of scaling:</p> <ul> <li>Throughput Scaling: Adjusts the read and write capacity of your tables to handle traffic spikes and reduce costs.</li> <li>Size Scaling: Automatically manages the size of your tables to ensure optimal performance and cost efficiency.</li> </ul>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#terminology","title":"Terminology","text":"<ul> <li>Partition: A physical storage unit that holds a portion of your table's data.</li> <li>Partition Key: A unique identifier for each item in a table that is used to distribute data across partitions.</li> <li>Sort Key: An optional attribute that is used to sort items with the same partition key.</li> </ul>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#partition-calculations","title":"Partition calculations","text":"<ul> <li>By capacity: The number of partitions is determined by the provisioned read and write capacity of your table.</li> <li>By size: The size of a partition is determined by the amount of data stored in the table.</li> <li>Total partitions: The total number of partitions in a table is the sum of the partitions for each partition key.</li> </ul>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#on-demand-scaling","title":"On-Demand Scaling","text":"<ul> <li>Automatically adjusts read and write capacity based on the workload.</li> <li>Pay only for the resources you consume.</li> <li>Ideal for unpredictable workloads or when you want to avoid managing capacity.</li> </ul>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#provisioned-scaling","title":"Provisioned Scaling","text":"<ul> <li>Allows you to specify the read and write capacity for your tables.</li> <li>Provides predictable performance and cost.</li> <li>Ideal for workloads with consistent traffic patterns.</li> </ul>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#dynamodb-accelerator-dax","title":"DynamoDB Accelerator (DAX)","text":"<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement.</p>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#dax-use-cases","title":"DAX Use Cases","text":""},{"location":"aws/architecting-to-scale/dynamodb-scaling/#good-case","title":"Good case","text":"<ul> <li>Require fastest possible reads</li> <li>Read intense scenarios</li> <li>Repeated reads agains a large set of data</li> </ul>"},{"location":"aws/architecting-to-scale/dynamodb-scaling/#bad-case","title":"Bad case","text":"<ul> <li>Write heavy workloads</li> <li>Data that is not frequently accessed</li> <li>Data that is not frequently read</li> </ul>"},{"location":"aws/architecting-to-scale/elastic-map-reduce/","title":"Elastic Map Reduce (EMR)","text":"<p>Amazon Elastic MapReduce (EMR) is a cloud-native big data platform that simplifies running big data frameworks, such as Apache Hadoop, Apache Spark, and Presto, on AWS. EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances.</p> <ul> <li>Managed Hadoop framework</li> <li>Also supports Apache Spark, Presto, and other big data frameworks</li> <li>Most commonly used for log analysis, data transformation, and machine learning</li> <li>A step is a programatic task for performing a specific action, such as installing software or running a script</li> <li>A cluster is a collection of Amazon EC2 instances that run the Hadoop framework and other big data tools</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/","title":"Event-driven Architecture","text":"<p>Event-driven architecture is a design pattern that enables the production, detection, consumption, and reaction to events. Events are generated by producers and consumed by consumers. This architecture is based on the principle of loose coupling, where components interact with each other through events rather than direct method calls. This approach allows for greater flexibility, scalability, and resilience, as components can be developed, deployed, and scaled independently.</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#integrating-serverless-services","title":"Integrating Serverless Services","text":"<ul> <li>Trigger asynchronous events between services</li> <li>An event is any change in state that is meaningful to the business</li> <li>Event routers filter and push events and their payloads to event</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#serveless-services","title":"Serveless Services","text":"<ul> <li>No server management: patches and OS updates handled by the provider</li> <li>Flexible scaling: your application scales automatically whithin bounds your define</li> <li>High availability: automatically deployed across multiple availability zones</li> <li>Scale to zero: no charges when your code is not running</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#types-of-aws-service-serverless","title":"Types of AWS service Serverless","text":"<ul> <li>Aurora Serverless: automatically scales database capacity based on application needs</li> <li>Neptune Serverless: automatically scales graph database capacity based on application needs</li> <li>EMR Serverless: automatically scales big data processing capacity based on application needs</li> <li>OpenSearch Service Serverless: automatically scales search capacity based on application needs</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#then-main-players-for-event-driven-architecture","title":"Then main players for event-driven architecture","text":""},{"location":"aws/architecting-to-scale/event-driven-architecture/#lambda","title":"Lambda","text":"<p>AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you. It is designed to be highly available and fault-tolerant, allowing you to focus on building applications without managing servers.</p> <ul> <li>Event-driven compute service</li> <li>Automatically scales based on the number of incoming requests</li> <li>Supports multiple programming languages</li> <li>Integrates with other AWS services</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why","title":"Why","text":"<p>Serverless compute for custom logic</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#eventbridge","title":"EventBridge","text":"<p>Amazon EventBridge is a serverless event bus service that makes it easy to connect applications together using data from your own applications, integrated software as a service (SaaS) applications, and AWS services. It allows you to build event-driven architectures by routing events from a variety of sources to a variety of targets.</p> <ul> <li>Serverless event bus service</li> <li>Connects applications using events</li> <li>Integrates with AWS services and SaaS applications</li> <li>Supports event filtering and routing</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why_1","title":"Why","text":"<p>Event bus for choreographing async events</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#step-functions","title":"Step Functions","text":"<p>AWS Step Functions is a serverless orchestration service that allows you to coordinate multiple AWS services into serverless workflows. It enables you to build complex, multi-step workflows that can be triggered by events and run with high availability and fault tolerance.</p> <ul> <li>Serverless orchestration service</li> <li>Coordinates multiple AWS services into workflows</li> <li>Supports branching, parallel execution, and error handling</li> <li>Integrates with other AWS services</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why_2","title":"Why","text":"<p>Orchestrates workflows involving many services</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#sqs","title":"SQS","text":"<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It allows you to send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p> <ul> <li>Fully managed message queuing service</li> <li>Decouples and scales microservices and distributed systems</li> <li>Supports standard and FIFO queues</li> <li>Integrates with other AWS services</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why_3","title":"Why","text":"<p>Event buffer for decoupling serverless workflows</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#sns","title":"SNS","text":"<p>Amazon Simple Notification Service (SNS) is a fully managed messaging service that enables you to send messages or notifications to distributed systems, microservices, and serverless applications. It allows you to send messages to a variety of endpoints, including email, SMS, HTTP, and Lambda functions.</p> <ul> <li>Fully managed messaging service</li> <li>Sends messages or notifications to distributed systems</li> <li>Supports multiple message protocols</li> <li>Integrates with other AWS services</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why_4","title":"Why","text":"<p>Send events to subscribers, including emails</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#api-gateway","title":"API Gateway","text":"<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It allows you to create RESTful APIs, WebSocket APIs, and HTTP APIs to connect your applications to backend services, such as Lambda functions, HTTP endpoints, or other AWS services.</p> <ul> <li>Fully managed API service</li> <li>Creates, publishes, maintains, monitors, and secures APIs</li> <li>Supports RESTful APIs, WebSocket APIs, and HTTP APIs</li> <li>Integrates with other AWS services</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why_5","title":"Why","text":"<p>Receive events triggered by external producers</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#dynamodb","title":"DynamoDB","text":"<p>Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to deliver single-digit millisecond performance at any scale, making it ideal for applications that require low-latency data access.</p> <ul> <li>Fully managed NoSQL database service</li> <li>Fast and predictable performance</li> <li>Seamless scalability</li> <li>Integrates with other AWS services</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why_6","title":"Why","text":"<p>Scalable and high-performance data store for serverless applications</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#s3","title":"S3","text":"<p>Amazon Simple Storage Service (S3) is a fully managed object storage service that allows you to store and retrieve any amount of data from anywhere on the web. It is designed for 99.999999999% durability and 99.99% availability of objects, making it ideal for storing large amounts of data, such as images, videos, and backups.</p> <ul> <li>Fully managed object storage service</li> <li>High durability and availability</li> <li>Scalable and secure</li> <li>Integrates with other AWS services</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#why_7","title":"Why","text":"<p>Object storage that can emit events on bucket actions</p>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#orchestration-vs-choreography","title":"Orchestration vs choreography","text":"<ul> <li>Orchestration: Centralized control of workflow</li> <li>Choreography: Decentralized control of workflow</li> </ul>"},{"location":"aws/architecting-to-scale/event-driven-architecture/#output","title":"Output","text":"<ul> <li>Choreograph async events with EventBridge</li> <li>Orchestrate workflows with Step Functions</li> <li>Choose Serverless for new solutions</li> </ul>"},{"location":"aws/architecting-to-scale/kinesis/","title":"Kinesis","text":"<p>Amazon Kinesis is a platform on AWS to collect, process, and analyze real-time, streaming data. It is a fully managed service that scales elastically for real-time processing of streaming big data and is integrated with other AWS services, making it easy to build and manage real-time, big data applications.</p> <ul> <li>Collection of services for processing streams of various data</li> <li>Data is processed in shards - with each shard able to ingest 1000 records per second</li> <li>A default limit of 500 shards, but can request an increase to unlimited shards</li> <li>Record consists of Partition Key, Sequence Number, and Data Blob</li> <li>Transient Data Store - data is stored for 24 hours by default</li> </ul>"},{"location":"aws/architecting-to-scale/kinesis/#kinesis-video-streams","title":"Kinesis Video Streams","text":"<p>Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), and other processing. It is a fully managed service that automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices.</p> <ul> <li>Securely stream video from connected devices to AWS for analytics, machine learning, and other processing</li> <li>Fully managed service that automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices</li> <li>Supports WebRTC, RTSP, and HLS streaming protocols</li> <li>Can be integrated with Amazon Rekognition for video analysis</li> </ul>"},{"location":"aws/architecting-to-scale/kinesis/#kinesis-data-streams","title":"Kinesis Data Streams","text":"<p>Amazon Kinesis Data Streams is a scalable and durable real-time data streaming service that can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, and real-time metrics.</p> <ul> <li>Scalable and durable real-time data streaming service</li> <li>Can continuously capture gigabytes of data per second from hundreds of thousands of sources</li> <li>Data collected is available in milliseconds to enable real-time analytics use cases</li> <li>Supports multiple consumers reading from the same stream</li> </ul>"},{"location":"aws/architecting-to-scale/kinesis/#kinesis-data-firehose","title":"Kinesis Data Firehose","text":"<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards.</p> <ul> <li>Easiest way to reliably load streaming data into data lakes, data stores, and analytics services</li> <li>Can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk</li> <li>Enables near real-time analytics with existing business intelligence tools and dashboards</li> <li>Fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration</li> </ul>"},{"location":"aws/architecting-to-scale/monitoring-and-visualizing/","title":"Visualize and monitor your AWS environment","text":""},{"location":"aws/architecting-to-scale/monitoring-and-visualizing/#amazon-quicksight","title":"Amazon QuickSight","text":"<p>Amazon QuickSight is a cloud-native business intelligence service that enables you to create and publish interactive dashboards that include ML insights. It allows you to easily connect to your data, perform ad-hoc analysis, and quickly build visualizations and dashboards.</p> <ul> <li>Serverless, pay-as-you-go BI service</li> <li>Low cost compared to other BI solutions</li> <li>uses SPICE (Super-fast, Parallel, In-memory Calculation Engine) for fast data visualization</li> <li>Accessible from web browser or mobile device</li> <li>Creates hybrid datasets from multiple sources</li> <li>Can leverage ML insights for anomaly detection and forecasting</li> </ul>"},{"location":"aws/architecting-to-scale/monitoring-and-visualizing/#supported-data-sources","title":"Supported data sources","text":"<ul> <li>Amazon S3</li> <li>Amazon RDS</li> <li>Amazon Redshift</li> <li>Amazon Aurora</li> <li>Amazon Athena</li> <li>Amazon DynamoDB</li> <li>Amazon DocumentDB</li> <li>Amazon Elasticsearch Service</li> <li>AWS IoT Analytics</li> <li>AWS CloudWatch</li> <li>AWS Data Exchange</li> <li>Third-party data sources via APIs</li> </ul>"},{"location":"aws/architecting-to-scale/monitoring-and-visualizing/#amazon-opensearch-service","title":"Amazon OpenSearch Service","text":"<p>Amazon OpenSearch Service is a fully managed service that makes it easy to deploy, secure, and operate OpenSearch at scale. It provides direct access to the OpenSearch API and is fully compatible with existing OpenSearch tools and plugins.</p> <ul> <li>Fully managed OpenSearch service</li> <li>Supports OpenSearch, Elasticsearch, and Kibana</li> <li>Secure and scalable</li> <li>Integrates with other AWS services</li> <li>Provides real-time analytics and visualization</li> </ul>"},{"location":"aws/architecting-to-scale/monitoring-and-visualizing/#outputs","title":"Outputs","text":"<ul> <li>QuickSight is the AWS-native BI service</li> <li>Opensearch provides open-source search and analytics</li> </ul>"},{"location":"aws/architecting-to-scale/scaling-containers/","title":"Scaling containers on AWS","text":""},{"location":"aws/architecting-to-scale/scaling-containers/#eks","title":"EKS","text":"<p>Amazon Elastic Kubernetes Service (EKS) is a managed Kubernetes service that makes it easy to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane. EKS is fully compatible with upstream Kubernetes and provides a highly available and secure Kubernetes control plane.</p>"},{"location":"aws/architecting-to-scale/scaling-containers/#ecs","title":"ECS","text":"<p>Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that allows you to run and scale containerized applications on AWS. ECS eliminates the need to install, operate, and scale your own container orchestration software. You can run your containers on a managed cluster of Amazon EC2 instances or AWS Fargate.</p>"},{"location":"aws/architecting-to-scale/scaling-containers/#fargate","title":"Fargate","text":"<p>AWS Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying infrastructure. Fargate provisions and scales the infrastructure needed to run your containers, and you only pay for the vCPU and memory resources consumed by your containers.</p>"},{"location":"aws/architecting-to-scale/scaling-containers/#app-runner","title":"App Runner","text":"<p>AWS App Runner is a fully managed container service that makes it easy to build, deploy, and run containerized web applications and APIs. App Runner automatically builds and deploys your container images from source code or existing container images, and provides auto-scaling, monitoring, and security features out of the box.</p>"},{"location":"aws/architecting-to-scale/scaling-containers/#why","title":"Why","text":"<ul> <li>Designed exclusively for Syncronous workloads</li> <li>Supplies compute and networking for container images, python, java or node projects</li> <li>Supports public and private endpoints</li> <li>Scales to zeto, which makes it great for proof of concepts</li> </ul>"},{"location":"aws/architecting-to-scale/scaling-containers/#eks-vs-ecs","title":"EKS vs ECS","text":"<p>EKS and ECS are both container orchestration services on AWS, but they have different use cases and features:</p> <ul> <li>EKS: Ideal for running Kubernetes workloads that require advanced features, compatibility with upstream Kubernetes, and integration with other AWS services.</li> <li>ECS: Ideal for running containerized applications that require simplicity, scalability, and integration with other AWS services.</li> <li>EKS: Adding Load Balancer is more complex</li> <li>ECS: Seamless integration with ALB and NLB</li> </ul>"},{"location":"aws/architecting-to-scale/scaling-containers/#aws-batch","title":"AWS Batch","text":"<p>AWS Batch is a fully managed batch processing service that allows you to run batch computing workloads on AWS. It dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of your batch jobs.</p> <ul> <li>Plans, schedule and executes your compute workloads</li> <li>Dynamically provisions CPU or memory-optimal resources</li> <li>Runs jobs using ECS, EKS or Fargate</li> <li>Reduce costs by optionally running your jobs using spot instances</li> </ul>"},{"location":"aws/architecting-to-scale/scaling-containers/#outputs","title":"Outputs","text":"<ul> <li>Choose your container service wisely</li> <li>ECS is great for new solution development</li> <li>EKS may be a valid option for migrating and existing Kubernetes workload</li> <li>Improve compute layer scaling with fargate</li> <li>Fargate is a great option to simplify the scaling and server management of the compute layer in your containerized applications</li> <li>App Runner fully hosts HTTP Apps for ultra-simplified HTTP applications compute and networking</li> </ul>"},{"location":"aws/architecting-to-scale/serverless/","title":"Serverless","text":"<p>Serverless computing allows you to build and run applications and services without thinking about servers. It eliminates infrastructure management tasks such as server or cluster provisioning, patching, operating system maintenance, and capacity provisioning. You can build serverless applications composed of functions that are triggered by events, scales automatically, and only charges you when they run.</p>"},{"location":"aws/architecting-to-scale/serverless/#aws-serverless-application-model","title":"AWS Serverless Application Model","text":"<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application. AWS SAM also supports defining environment variables, policies, and other resources that your application needs.</p>"},{"location":"aws/architecting-to-scale/serverless/#behind-of-scenes","title":"Behind of scenes","text":"<p>Extension of cloudFormation so you can use everything cloudformation can provide.</p>"},{"location":"aws/architecting-to-scale/serverless/#amazon-eventbridge","title":"Amazon EventBridge","text":"<p>Amazon EventBridge is a serverless event bus service that makes it easy to connect applications together using data from your own applications, integrated software as a service (SaaS) applications, and AWS services. It allows you to build event-driven architectures by routing events from a variety of sources to a variety of targets.</p>"},{"location":"aws/architecting-to-scale/serverless/#types-of-event-bus","title":"Types of event bus","text":"<ul> <li>Default event bus: The event bus that receives events from AWS services and your custom applications.</li> <li>Partner event bus: The event bus that receives events from SaaS partners.</li> <li>Custom event bus: The event bus that receives events from your custom applications.</li> </ul>"},{"location":"aws/architecting-to-scale/sns/","title":"AWS SNS","text":"<p>Amazon Simple Notification Service (SNS) is a fully managed messaging service that enables you to send messages or notifications to distributed systems, microservices, and serverless applications. It allows you to send messages to a variety of endpoints, including email, SMS, HTTP, and Lambda functions.</p> <ul> <li>Enables a Pub/Sub model for messaging</li> <li>Topics = A channel for publishing messages</li> <li>Subscriptions = Endpoints that receive messages published to a topic</li> <li>Supports multiple message protocols</li> </ul>"},{"location":"aws/architecting-to-scale/sqs/","title":"AWS SQS","text":"<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware.</p> <ul> <li>Reliable and scalable</li> <li>Available integration with KMS for encryption</li> <li>Transient storage for messages</li> <li>Optionally supports First-in First-out (FIFO) queues</li> <li>Maximum message size of 256KB (standard) or 256KB (FIFO)</li> </ul>"},{"location":"aws/architecting-to-scale/sqs/#types-of-queues","title":"Types of queues","text":"<ul> <li>Standard Queues: Best-effort ordering, at-least-once delivery</li> <li>FIFO Queues: First-in-first-out delivery, exactly-once processing</li> </ul>"},{"location":"aws/architecting-to-scale/sqs/#amazon-mq","title":"Amazon MQ","text":"<p>Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes it easy to set up and operate message brokers in the cloud. Amazon MQ provides the flexibility to choose the message broker that best fits your use case, while offloading the operational overhead to AWS.</p> <ul> <li>Fully managed message broker service</li> <li>Supports Apache ActiveMQ and RabbitMQ</li> <li>Seamless integration with AWS services</li> <li>Supports standard messaging protocols</li> <li>High availability and durability</li> </ul>"},{"location":"aws/architecting-to-scale/sqs/#when-should-i-use","title":"When should I use","text":"<ul> <li>Use Amazon MQ if you want to and easy low-hassle path to migrate from existing message brokers to aws</li> <li>Use SQS if you creating a new application from scratch</li> </ul>"},{"location":"aws/architecting-to-scale/step-functions/","title":"AWS Step functions","text":"<p>AWS Step Functions is a serverless orchestration service that allows you to coordinate multiple AWS services into serverless workflows. It enables you to build complex, multi-step workflows that can be triggered by events and run with high availability and fault tolerance.</p> <ul> <li>Managed workflow and orchestration platform</li> <li>Scalable and highly available</li> <li>Define your app as a state machine</li> <li>Create tasks, sequences, and parallelism</li> <li>Amazon state language for defining state machines</li> </ul>"},{"location":"aws/architecting-to-scale/step-functions/#aws-batch","title":"AWS Batch","text":"<p>AWS Batch is a fully managed batch processing service that allows you to run batch computing workloads on AWS. It dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of your batch jobs.</p> <ul> <li>Create a compute environment: Managed or unmanaged, spot or on-demand</li> <li>Create a job queue with priority and assigned to a compute environment</li> <li>Create job definitions with container image, vCPU, memory, and environment variables</li> </ul>"},{"location":"aws/athena/convert-access-logs-from-alb-to-db/","title":"AWS Athena","text":"<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>"},{"location":"aws/athena/convert-access-logs-from-alb-to-db/#how-to-create-a-db-from-a-access-log-bucket-of-an-alb","title":"How to create a DB from a access log bucket of an ALB","text":"<pre><code>CREATE EXTERNAL TABLE `name_of_table`(\n  `type` string COMMENT '', \n  `time` string COMMENT '', \n  `elb` string COMMENT '', \n  `client_ip` string COMMENT '', \n  `client_port` int COMMENT '', \n  `target_ip` string COMMENT '', \n  `target_port` int COMMENT '', \n  `request_processing_time` double COMMENT '', \n  `target_processing_time` double COMMENT '', \n  `response_processing_time` double COMMENT '', \n  `elb_status_code` int COMMENT '', \n  `target_status_code` string COMMENT '', \n  `received_bytes` bigint COMMENT '', \n  `sent_bytes` bigint COMMENT '', \n  `request_verb` string COMMENT '', \n  `request_url` string COMMENT '', \n  `request_proto` string COMMENT '', \n  `user_agent` string COMMENT '', \n  `ssl_cipher` string COMMENT '', \n  `ssl_protocol` string COMMENT '', \n  `target_group_arn` string COMMENT '', \n  `trace_id` string COMMENT '', \n  `domain_name` string COMMENT '', \n  `chosen_cert_arn` string COMMENT '', \n  `matched_rule_priority` string COMMENT '', \n  `request_creation_time` string COMMENT '', \n  `actions_executed` string COMMENT '', \n  `redirect_url` string COMMENT '', \n  `lambda_error_reason` string COMMENT '', \n  `target_port_list` string COMMENT '', \n  `target_status_code_list` string COMMENT '', \n  `classification` string COMMENT '', \n  `classification_reason` string COMMENT '', \n  `conn_trace_id` string COMMENT '')\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.serde2.RegexSerDe' \nWITH SERDEPROPERTIES ( \n  'input.regex'='([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \\\"([^ ]*) (.*) (- |[^ ]*)\\\" \\\"([^\\\"]*)\\\" ([A-Z0-9-_]+) ([A-Za-z0-9.-]*) ([^ ]*) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" ([-.0-9]*) ([^ ]*) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" \\\"([^ ]*)\\\" \\\"([^\\\\s]+?)\\\" \\\"([^\\\\s]+)\\\" \\\"([^ ]*)\\\" \\\"([^ ]*)\\\" ?([^ ]*)?') \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.mapred.TextInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  's3://&lt;bucket-name&gt;/aws/AWSLogs/&lt;path until the region&gt;/&lt;region&gt;/&lt;year&gt;'\nTBLPROPERTIES (\n  'transient_lastDdlTime'='1742910240')\n</code></pre>"},{"location":"aws/athena/useful-queries/","title":"Useful queries for Amazon Athena","text":""},{"location":"aws/athena/useful-queries/#using-between-dates","title":"Using between dates","text":"<pre><code>SELECT time, user_agent, request_url, client_ip\nFROM \"mydb\"\nWHERE parse_datetime(time, 'yyyy-MM-dd''T''HH:mm:ss.SSSSSS''Z') \nBETWEEN parse_datetime('2025-03-23-00:00:00', 'yyyy-MM-dd-HH:mm:ss')\nAND parse_datetime('2025-03-24-23:59:59', 'yyyy-MM-dd-HH:mm:ss')\nAND user_agent LIKE '%Hypersonic%' \nAND request_url LIKE '%c=kjhasd%'\nLIMIT 1000;\n</code></pre>"},{"location":"aws/athena/useful-queries/#using-like","title":"Using like","text":"<pre><code>SELECT time, user_agent, request_url, client_ip FROM \"mydb\" where user_agent like '%Hypersonic%' and request_url like '%a=459784%' OR request_url like '%a=316050%' limit 1000;\n</code></pre>"},{"location":"aws/business-continuity/","title":"Business Continuity","text":"<p>Business continuity is the ability of an organization to maintain essential functions during, as well as after, a disaster has occurred. Business continuity planning establishes risk management processes and procedures that aim to prevent interruptions to mission-critical services, and re-establish full function to the organization as quickly and smoothly as possible.</p>"},{"location":"aws/business-continuity/#concepts","title":"Concepts","text":""},{"location":"aws/business-continuity/#business-continuity-bc","title":"Business Continuity (BC)","text":"<p>Seeks to minimize business activity disruption when something unexpected happens.</p>"},{"location":"aws/business-continuity/#disaster-recovery-dr","title":"Disaster Recovery (DR)","text":"<p>Act of responding to an event that thratens business continuity.</p>"},{"location":"aws/business-continuity/#high-availability-ha","title":"High Availability (HA)","text":"<p>Designing in redundancy to ensure that a system is always available.</p>"},{"location":"aws/business-continuity/#fault-tolerance","title":"Fault Tolerance","text":"<p>Designing in redundancy to ensure that a system can continue to operate in the event of a failure.</p>"},{"location":"aws/business-continuity/#service-level-agreement-sla","title":"Service Level Agreement (SLA)","text":"<p>A contract between a service provider and a customer that specifies the level of service that the customer can expect.</p>"},{"location":"aws/business-continuity/#recovery-time-objective-rto","title":"Recovery Time Objective (RTO)","text":"<p>The maximum amount of time that a system can be down before it starts to impact the business.</p>"},{"location":"aws/business-continuity/#recovery-point-objective-rpo","title":"Recovery Point Objective (RPO)","text":"<p>The maximum amount of data that can be lost before it starts to impact the business.</p>"},{"location":"aws/business-continuity/#types-of-disaster","title":"Types of disaster","text":"Category Description Hardware Failure Failure of a physical component Deployment Failure Failure of a deployment Load Induced Distributed denial of Service attack on your website Data Induced Ariane 5 rocket explosion on June 1996 Credential Expiration An SSL/TLS certificate expires Dependency S3 subsystem failure cause numerous other AWS service failures Infra Lack of sufficient capacity Human Always a mistake made by someone"},{"location":"aws/business-continuity/#outputs","title":"Outputs","text":"<ul> <li>General Concepts</li> <li>Know the difference between Business Continuity, Disaster Recovery, High Availability, and Fault Tolerance.</li> <li>Know the difference between Recovery Time Objective (RTO) and Recovery Point Objective (RPO).</li> <li>Understand the 4 options for Disaster Recovery: Backup and Restore, Pilot Light, Warm Standby, and Multi-Site.</li> <li>Storage Options<ul> <li>Understand RAID and the potential benefits of using it.</li> </ul> </li> <li>Compute Options<ul> <li>Understand why horizontal scaling is preferred from an HA perspective.</li> </ul> </li> </ul>"},{"location":"aws/business-continuity/compute_ha/","title":"Compute High Availability","text":"<p>High availability (HA) is a design approach that ensures a system is always available, even in the event of a failure. HA is achieved by designing in redundancy, which means that if one component fails, another component can take over. This ensures that the system continues to operate without interruption.</p>"},{"location":"aws/business-continuity/compute_ha/#approaches","title":"Approaches","text":"<ul> <li>up to date AMIs are critical for rapid fail-over</li> <li>AMIs can be copied to other regions for safety or DR staging</li> <li>Horizontally scalable architecture are preferred because risk can be spread across multiple instances</li> <li>Reserved instances is the only way to guarantee capacity</li> <li>Auto scaling and elastic load balancing are key components of HA</li> <li>Route 53 can be used to route traffic to healthy instances</li> </ul>"},{"location":"aws/business-continuity/database_ha/","title":"DataBases High Availability","text":""},{"location":"aws/business-continuity/database_ha/#choose-the-right-database","title":"Choose the right database","text":""},{"location":"aws/business-continuity/database_ha/#high-operational-efficiency-and-less-control","title":"High Operational Efficiency and Less Control","text":"<ul> <li>DynamoDB (no sql)</li> <li>Aurora (sql)</li> </ul>"},{"location":"aws/business-continuity/database_ha/#medium-operational-efficiency-and-control","title":"Medium Operational Efficiency and Control","text":"<ul> <li>RDS (sql)</li> </ul>"},{"location":"aws/business-continuity/database_ha/#low-operational-efficiency-and-control","title":"Low Operational Efficiency and Control","text":"<ul> <li>EC2 (sql)</li> </ul>"},{"location":"aws/business-continuity/database_ha/#high-available-dynamodb","title":"High Available DynamoDB","text":"<ul> <li>Distributed data and incoming traffic across partitions by default</li> <li>Partitions are replicated across multiple AZs</li> <li>Global tables allow multi-region availability</li> </ul>"},{"location":"aws/business-continuity/database_ha/#multi-az-rds","title":"Multi-AZ RDS","text":"<ul> <li>Synchronous replication across multiple AZs</li> <li>Failover to standby in case of primary failure</li> <li>Automated backups and snapshots</li> </ul>"},{"location":"aws/business-continuity/database_ha/#regional-read-replica","title":"Regional Read Replica","text":"<ul> <li>Asynchronous replication across regions</li> <li>Read-only copy of the primary database</li> <li>Can be promoted to primary in case of primary failure</li> <li>Warm standby solution</li> </ul>"},{"location":"aws/business-continuity/database_ha/#snapshot-recovery","title":"Snapshot Recovery","text":"<ul> <li>Multi AZ RDS cluster</li> <li>Automated backups and snapshots</li> </ul>"},{"location":"aws/business-continuity/database_ha/#aurora","title":"Aurora","text":"<ul> <li>Multi-AZ deployment</li> <li>Data copies across multiple AZs</li> <li>Automated failover</li> </ul>"},{"location":"aws/business-continuity/database_ha/#aurora-global-database","title":"Aurora Global Database","text":"<ul> <li>One primary region and up to five read-only secondary regions</li> <li>Data replicates from primary to secundary regions</li> <li>Leverages storage-level replication</li> <li>Promote a secondary region to primary in case of primary region failure</li> </ul>"},{"location":"aws/business-continuity/database_ha/#redshift-high-availability","title":"Redshift High Availability","text":"<ul> <li>Only RA3 instances supports Multi-AZ</li> <li>Best option for HA otherwise is a multi-node cluster</li> <li>single-node failure must be restored from a s3 snapshot</li> </ul>"},{"location":"aws/business-continuity/database_ha/#output","title":"Output","text":"<ul> <li>DynamoDB is the best option for high availability</li> <li>Aurora is the best option for high availability with SQL</li> <li>Redshift is the best option for data warehousing high availability</li> </ul>"},{"location":"aws/business-continuity/dr/","title":"Disaster Recovery","text":"<p>Disaster recovery is the act of responding to an event that threatens business continuity. It is a subset of business continuity planning and focuses on the IT or technology systems that support business functions. Disaster recovery planning is a critical component of business continuity planning.</p>"},{"location":"aws/business-continuity/dr/#backup-and-restore","title":"Backup and restore","text":"<ul> <li>Pros</li> <li>Very common entry point</li> <li>into AWS</li> <li>Minimal effort configuration</li> <li>Cons</li> <li>Least flexibility</li> <li>Analogous to off-site</li> <li>backuop storage</li> </ul>"},{"location":"aws/business-continuity/dr/#pilot-light","title":"Pilot Light","text":"<ul> <li>Pros</li> <li>Faster recovery time</li> <li>than backup and restore</li> <li>More cost-effective</li> <li>Cons</li> <li>More complex</li> <li>than backup and restore</li> <li>Requires more resources</li> </ul>"},{"location":"aws/business-continuity/dr/#warm-standby","title":"Warm Standby","text":"<ul> <li>Pros</li> <li>All services are up and ready to accept a failover faster with less data loss</li> <li>Can be used as a shadow environment for testing</li> <li>Cons</li> <li>Resources would need to be scaled to accept production load</li> <li>Still requires some adjustments but could be scripted</li> </ul>"},{"location":"aws/business-continuity/dr/#multi-site","title":"Multi-Site","text":"<ul> <li>Pros</li> <li>Can be used for active-active or active-passive</li> <li>Can be used for disaster recovery or high availability</li> <li>Cons</li> <li>More complex than warm standby</li> <li>More expensive than warm standby</li> </ul>"},{"location":"aws/business-continuity/network_ha/","title":"Network High Availability","text":"<ul> <li>By creating subnets in the available AZs, you create multi-AZ presence for your VPC</li> <li>Best practice is to create at least two VPN tunnels into your Virtual Private Gateway</li> <li>Direct Connect is not HA by default, you need to create a second connection to another location or use VPN</li> <li>Route 53 can be used to route traffic to healthy instances</li> <li>For multi-AZ redundancy or NAT Gateways, create gateways in each AZ with routes for private subnets to use the local gateway</li> </ul>"},{"location":"aws/business-continuity/storage_ha/","title":"Storage High Availability","text":"<ul> <li>Annual failure rate (AFR) of 0.1% compared to commodity drives at 4%</li> <li>Availability target of 99.99%</li> <li>Replication across multiple Availability Zones</li> <li>Vulnerable to AZ failure</li> <li>Easy to snapshot</li> <li>You can copy snapshots to other regions</li> <li>Supports RAID configurations</li> </ul>"},{"location":"aws/business-continuity/storage_ha/#what-is-raid","title":"What is RAID?","text":"<p>RAID (Redundant Array of Independent Disks) is a data storage virtualization technology that combines multiple physical disk drive components into one or more logical units for the purposes of data redundancy, performance improvement, or both.</p>"},{"location":"aws/business-continuity/storage_ha/#tip","title":"Tip","text":"<p>Use RAID 0 and RAID 1 on AWS</p>"},{"location":"aws/business-continuity/storage_ha/#s3-storage","title":"S3 storage","text":"<ul> <li>Standard S3 storage is designed for 99.999999999% durability and 99.99% availability</li> <li>Standard S3 infrequent access is designed for 99.9% availability</li> <li>One-zone infrequent access is designed for 99.5% availability</li> <li>Eleven 9s of durability</li> <li>Standard &amp; Standard IA are designed for 99.99% availability</li> </ul>"},{"location":"aws/business-continuity/storage_ha/#amazon-efs","title":"Amazon EFS","text":"<ul> <li>Implementation of NFS File System</li> <li>True file system as opposed to EBS or S3</li> <li>File locking, strong consistency and concurrently accessible</li> <li>Each file object and metadata is stored across multiple AZs</li> <li>Can be accessed by multiple EC2 instances</li> <li>Can be accessed from all AZs in a region</li> <li>Mount targets are highly available</li> </ul>"},{"location":"aws/business-continuity/storage_ha/#other-options","title":"Other options","text":""},{"location":"aws/business-continuity/storage_ha/#amazon-storage-gateway","title":"Amazon Storage Gateway","text":"<ul> <li>Good way to migrate on-prem data to AWS for offsite backuo</li> <li>Best for continuous sync needs</li> </ul>"},{"location":"aws/business-continuity/storage_ha/#aws-snowball","title":"AWS Snowball","text":"<ul> <li>Various options for migrating data to AWS based on volume</li> <li>Only for batch transfer of data</li> </ul>"},{"location":"aws/business-continuity/storage_ha/#glacier","title":"Glacier","text":"<ul> <li>Safe offsite archive storage</li> <li>Low cost</li> </ul>"},{"location":"aws/cli/","title":"Useful scripts","text":""},{"location":"aws/cli/#create-temporary-credentials-for-aws-cli","title":"Create temporary credentials for AWS CLI","text":"<p>create-temp-credentials.sh</p>"},{"location":"aws/cli/#remove-all-file-versions-from-an-s3-bucket","title":"Remove all file versions from an S3 bucket","text":"<p>erase-all.sh</p>"},{"location":"aws/cli/#display-membership-id-and-group-name-from-aws-identity","title":"Display Membership ID and Group Name from AWS Identity","text":"<p>get-member-id-group-name.sh</p>"},{"location":"aws/cli/#display-dynamodb-table-name-if-pitr-is-enabled-and-how-many-backups-a-table-have","title":"Display DynamoDb table name, if PITR is enabled and how many backups a table have","text":"<p>check-dynamo-tables.sh</p>"},{"location":"aws/cli/#heres-a-script-that-adds-a-name-tag-to-every-dynamodb-table-using-the-table-name-as-the-tag-value","title":"Here\u2019s a script that adds a Name tag to every DynamoDB table, using the table name as the tag value","text":"<p>add-dynamo-tag-name.sh</p>"},{"location":"aws/cli/#heres-a-script-that-adds-a-name-tag-to-one-specific-dynamodb-table-using-the-table-name-as-the-tag-value","title":"Here\u2019s a script that adds a Name tag to one specific DynamoDB table, using the table name as the tag value","text":"<p>add-dynamo-tag-name-one-table.sh</p>"},{"location":"aws/cost-management/","title":"Cost management","text":""},{"location":"aws/cost-management/#concepts","title":"Concepts","text":""},{"location":"aws/cost-management/#capital-expenses-capex","title":"Capital Expenses (CapEx)","text":"<p>Capital expenses (CapEx) are funds used by a company to acquire, upgrade, and maintain physical assets such as property, buildings, or equipment. CapEx is often used to undertake new projects or investments that will help the company grow or maintain its competitive position in the market.</p>"},{"location":"aws/cost-management/#operational-expenses-opex","title":"Operational Expenses (OpEx)","text":"<p>Operational expenses (OpEx) are the ongoing costs that a company incurs to run its day-to-day operations. OpEx includes expenses such as rent, utilities, salaries, and other costs that are necessary to keep the business running. OpEx is typically considered a cost of doing business and is deducted from revenue to calculate net income.</p>"},{"location":"aws/cost-management/#total-cost-of-ownership-tco","title":"Total Cost of Ownership (TCO)","text":"<p>Total cost of ownership (TCO) is the total cost of acquiring, operating, and maintaining an asset over its entire lifecycle. TCO includes the initial purchase price of the asset, as well as ongoing costs such as maintenance, repairs, and operating expenses. TCO is used to evaluate the cost-effectiveness of different investment options and to make informed decisions about resource allocation.</p>"},{"location":"aws/cost-management/#return-on-investment-roi","title":"Return on Investment (ROI)","text":"<p>Return on investment (ROI) is a financial metric used to evaluate the profitability of an investment. ROI is calculated by dividing the net profit of an investment by the initial cost of the investment and expressing the result as a percentage. A positive ROI indicates that an investment is profitable, while a negative ROI indicates that an investment is not profitable.</p>"},{"location":"aws/cost-management/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"aws/cost-management/#appropriate-provisioning","title":"Appropriate provisioning","text":"<ul> <li>Provision the resources you need and nothing more</li> <li>Consolidate where possible for greater density and lower complexity (multi-database, containers)</li> <li>CloudWatch can help by monitoring utilization</li> </ul>"},{"location":"aws/cost-management/#right-sizing","title":"Right-sizing","text":"<ul> <li>Choose the right instance type and size for your workload</li> <li>Use tools like AWS Trusted Advisor to identify underutilized resources</li> <li>Consider using reserved instances for predictable workloads</li> <li>Loosely coupled architectures using SNS, SQS, and Lambda can smooth demand and create more predictable and consistent</li> </ul>"},{"location":"aws/cost-management/#purchase-options","title":"Purchase options","text":"<ul> <li>Reserved Instances (RIs) can provide significant cost savings for predictable workloads</li> <li>Savings Plans offer flexibility and savings for a wide range of workloads</li> <li>Spot Instances can provide significant savings for fault-tolerant and flexible workloads</li> <li>On-Demand Instances are ideal for unpredictable workloads or when you need to quickly scale up or down</li> </ul>"},{"location":"aws/cost-management/#geographic-selection","title":"Geographic Selection","text":"<ul> <li>AWS Pricing can vary from region to region</li> <li>Consider potential savings by locating resources in a remote region if local access is not required</li> <li>Route53 and CloudFront can help with global distribution</li> </ul>"},{"location":"aws/cost-management/#managed-services","title":"Managed Services","text":"<ul> <li>Leverage managed services to reduce operational overhead (Mysql over self-managed)</li> <li>Cost savings gained through lower complexity and manual intervention</li> <li>RDS, Redshift, and DynamoDB are examples of managed services</li> </ul>"},{"location":"aws/cost-management/#optimized-data-transfer","title":"Optimized Data Transfer","text":"<ul> <li>Data going out and between regions can be costly</li> <li>Direct connect can be a more cost-effective option for large data transfers</li> </ul>"},{"location":"aws/cost-management/#tagging-and-resource-groups","title":"Tagging and Resource Groups","text":"<ul> <li>Tagging resources can help you track and manage costs</li> <li>Tagging strategies can be used for Cost Allocation, security, automation and manu other resources</li> <li>Enforcing standardized tagging can be done via AWS Config Rules or custom scripts</li> <li>Most resources can have up to 50 tags</li> </ul>"},{"location":"aws/cost-management/#aws-resources-groups","title":"AWS Resources groups","text":"<ul> <li>Resources Groups are grouping of AWS assets defined by tags</li> <li>Create custom consoles to consolidate metrics, alarms and config details</li> </ul>"},{"location":"aws/cost-management/#managing-costs-accross-accounts","title":"Managing Costs accross accounts","text":""},{"location":"aws/cost-management/#cost-and-usage-reports","title":"Cost and Usage reports","text":"<ul> <li>Generate Csv files to track costs by account, services or tags</li> <li>With consolidated billing enabled, track spending across your organization from the management account</li> <li>Adjust granualarity to hourly, daily or monthly</li> <li>Further analysis can be done with Athena or QuickSight</li> </ul>"},{"location":"aws/cost-management/#outputs-about-costs-accross-accounts","title":"Outputs about costs accross accounts","text":"<ul> <li>analyze costs accross your organization</li> <li>Budgets can alert you or trigger automation</li> <li>Have plan for tagging resources</li> </ul>"},{"location":"aws/cost-management/#spot-and-reserved-instances","title":"Spot and Reserved Instances","text":""},{"location":"aws/cost-management/#reserved-instances","title":"Reserved Instances","text":"<ul> <li>Reserved Instances (RIs) provide a significant discount compared to On-Demand pricing</li> <li>RIs require a commitment to a specific instance type in a specific region for a term of 1 or 3 years</li> <li>RIs are ideal for predictable workloads with steady state usage</li> </ul>"},{"location":"aws/cost-management/#spot-instances","title":"Spot Instances","text":"<ul> <li>Excess capacity that AWS tries to sell on an market exchange basis</li> <li>Customer creates a Spot request and specifies AMI, desired instance type, and max price</li> <li>Customer defines highest price willing to pay for instance capacity</li> <li>Spot request can be fill and kill, maintain or duration-based</li> </ul>"},{"location":"aws/cost-management/#dedicated-instance-vs-dedicated-host","title":"Dedicated Instance vs Dedicated Host","text":"<ul> <li>Dedicated Instances run on hardware that's dedicated to a single customer</li> <li>Dedicated Hosts run on a physical server that's dedicated to a single customer</li> <li>Dedicated Hosts provide visibility and control over the placement of instances</li> <li>Dedicated Hosts can be purchased On-Demand or as a Reservation</li> <li>Dedicated Hosts can be used to meet compliance requirements</li> <li>Each dedicated host can only run one EC2 instance</li> <li>Dedicated Instances may share hardware with other instances</li> <li>Available as on demand, reserved adn spot instances</li> </ul>"},{"location":"aws/cost-management/#consolidated-billing","title":"Consolidated billing","text":""},{"location":"aws/cost-management/#aws-budgets","title":"AWS Budgets","text":"<ul> <li>Allow you to set pre-defined limits and notification</li> <li>Can be set for cost, usage, RI utilization, and reservation coverage</li> <li>Useful as a method to distribute costs and track spending</li> </ul>"},{"location":"aws/cost-management/#consolidated-billing_1","title":"Consolidated Billing","text":"<ul> <li>Enable a single player account thats locked down to only those who need access</li> <li>Economies of scale can be achieved by aggregating usage</li> </ul>"},{"location":"aws/cost-management/#trusted-advisor","title":"Trusted Advisor","text":"<ul> <li>Provides real-time guidance to help you provision your resources following AWS best practices</li> <li>Trusted Advisor inspects your environment and makes recommendations for saving money, improving system performance, and closing security gaps</li> </ul>"},{"location":"aws/cost-management/#outputs","title":"Outputs","text":"<ul> <li>Consting in general</li> <li>understand the difference between CapEx and OpEx</li> <li>understand the concept of TCO and ROI</li> <li>Cost optimization strategies</li> <li>understand the different cost optimization strategies</li> <li>understand the benefits of tagging and resource groups</li> <li>taggin and resource groups</li> <li>understand the benefits of tagging and resource groups</li> <li>understand how to manage costs across accounts</li> <li>Spot and Reserved Instances</li> <li>understand the benefits of Reserved Instances</li> <li>understand the benefits of Spot Instances</li> <li>understand the difference between Dedicated Instances and Dedicated Hosts</li> <li>Cost management<ul> <li>understand the benefits of AWS Budgets</li> <li>understand the benefits of Consolidated Billing</li> <li>understand the benefits of Trusted Advisor</li> </ul> </li> </ul>"},{"location":"aws/data-store/","title":"Data Store","text":""},{"location":"aws/data-store/#s3","title":"S3","text":"<p>Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance. It is designed to store and retrieve any amount of data from anywhere on the web. S3 is commonly used for backup and restore, disaster recovery, data archiving, big data analytics, and as a data lake for analytics.</p>"},{"location":"aws/data-store/#s3-tiers","title":"S3 tiers","text":"<ul> <li>S3 Standard: for frequently accessed data</li> <li>S3 Intelligent-Tiering: for data with unknown or changing access patterns</li> <li>S3 Standard-IA: for long-lived, infrequently accessed data</li> <li>S3 One Zone-IA: for long-lived, infrequently accessed, non-critical data</li> <li>S3 Glacier: for long-term archive and digital preservation</li> <li>S3 Glacier Deep Archive: for long-term archive and digital preservation with retrieval times within 12 hours</li> </ul>"},{"location":"aws/data-store/#ebs","title":"EBS","text":"<p>Amazon Elastic Block Store (EBS) provides block-level storage volumes for use with EC2 instances. EBS volumes are highly available and reliable storage volumes that can be attached to any running instance that is in the same Availability Zone. EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance.</p>"},{"location":"aws/data-store/#efs","title":"EFS","text":"<p>Amazon Elastic File System (EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p>"},{"location":"aws/data-store/#fsx","title":"FSX","text":"<p>Amazon FSx provides fully managed third-party file systems with the native compatibility and feature sets for workloads such as Windows-based storage, high-performance computing, machine learning, and electronic design automation.</p>"},{"location":"aws/data-store/#storage-gateway","title":"Storage Gateway","text":"<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. It seamlessly enables hybrid cloud storage between on-premises environments and the AWS Cloud.</p>"},{"location":"aws/data-store/#ec2-databases","title":"EC2 DataBases","text":"<p>Running databases on EC2 instances can be beneficial in several scenarios:</p> <ul> <li>Custom Configuration: When you need full control over the database configuration, including the operating system, database software, and any additional software or scripts.</li> <li>High Performance: For workloads that require high IOPS, low latency, or specific hardware configurations that are not available in managed database services.</li> <li>Legacy Applications: When dealing with legacy applications that are not compatible with managed database services or require specific database versions.</li> <li>Cost Management: In some cases, running databases on EC2 can be more cost-effective, especially if you have reserved instances or spot instances.</li> <li>Compliance and Security: When you need to meet specific compliance or security requirements that necessitate a custom setup or additional security measures.</li> <li>Hybrid Architectures: For hybrid cloud architectures where part of the database workload needs to run on-premises and part on the cloud, providing more flexibility in deployment.</li> </ul>"},{"location":"aws/data-store/#rds","title":"RDS","text":"<p>Amazon Relational Database Service (RDS) is a managed relational database service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.</p>"},{"location":"aws/data-store/#supported-engines","title":"Supported Engines","text":"<ul> <li>PostgreSQL</li> <li>MySQL</li> <li>MariaDB</li> <li>Oracle</li> <li>Microsoft SQL Server</li> </ul>"},{"location":"aws/data-store/#aurora","title":"Aurora","text":"<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open-source databases. Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases.</p>"},{"location":"aws/data-store/#dynamodb","title":"DynamoDB","text":"<p>Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to deliver single-digit millisecond performance at any scale, making it ideal for applications that require low-latency data access.</p>"},{"location":"aws/data-store/#documentdb","title":"DocumentDB","text":"<p>Amazon DocumentDB is a fully managed document database service that supports MongoDB workloads. It is designed to be highly scalable, durable, and performant, making it ideal for applications that require flexible, semi-structured, or hierarchical data storage.</p>"},{"location":"aws/data-store/#neptune","title":"Neptune","text":"<p>Amazon Neptune is a fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. It is optimized for storing billions of relationships and querying the graph with milliseconds latency.</p>"},{"location":"aws/data-store/#redshift","title":"Redshift","text":"<p>Amazon Redshift is a fully managed data warehouse service that makes it simple and cost-effective to analyze all your data using standard SQL and your existing business intelligence tools. It is designed for high-performance analysis of large datasets, scaling from a few hundred gigabytes to a petabyte or more.</p>"},{"location":"aws/data-store/#athena","title":"Athena","text":"<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>"},{"location":"aws/data-store/#glue","title":"Glue","text":"<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analytics. You can create and run ETL jobs with a few clicks in the AWS Management Console, and it automatically generates the code needed to perform the data transformation.</p>"},{"location":"aws/data-store/#elasticache","title":"Elasticache","text":"<p>Amazon ElastiCache is a fully managed in-memory data store and cache service that makes it easy to deploy, operate, and scale popular open-source compatible in-memory data stores in the cloud.</p>"},{"location":"aws/data-store/#tips","title":"Tips","text":"<ul> <li>Archive and backup is great first step when building a business case for using AWS data store services.</li> <li>Make sure you are using the most cost-effective storage class for your data access patterns.</li> <li>Practice building security S3 buckets and granting the least privilege access to users.</li> <li>Encrypt data at rest, always.</li> </ul>"},{"location":"aws/data-store/fsx/","title":"Amazon FSx file systems","text":""},{"location":"aws/data-store/fsx/#amazon-fsx-for-lustre","title":"Amazon FSx for Lustre","text":"<p>FSx for Lustre makes it easy and cost-effective to launch and run the popular, high-performance Lustre file system. You use Lustre for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling.</p>"},{"location":"aws/data-store/fsx/#what-is-lustre","title":"What is Lustre?","text":"<p>Lustre is an open-source, parallel file system that is designed for high-performance computing (HPC) and machine learning workloads. Lustre is used by many of the world's largest supercomputers and is known for its high performance, scalability, and reliability.</p>"},{"location":"aws/data-store/fsx/#amazon-fsx-for-windows-file-server","title":"Amazon FSx for Windows File Server","text":"<p>Amazon FSx for Windows File Server provides fully managed Microsoft Windows file servers, backed by a fully native Windows file system. FSx for Windows File Server has the features, performance, and compatibility to easily lift and shift enterprise applications to the AWS Cloud.</p>"},{"location":"aws/data-store/fsx/#amazon-fsx-for-netapp-ontap","title":"Amazon FSx for NetApp ONTAP","text":"<p>Amazon FSx for NetApp ONTAP is a fully managed service that provides highly reliable, scalable, high-performing, and feature-rich file storage built on NetApp's popular ONTAP file system. FSx for ONTAP combines the familiar features, performance, capabilities, and API operations of NetApp file systems with the agility, scalability, and simplicity of a fully managed AWS service.</p>"},{"location":"aws/data-store/fsx/#what-is-netapp-ontap","title":"What is NetApp ONTAP?","text":"<p>NetApp ONTAP is a powerful, feature-rich file system that is used by many enterprises to store and manage their data. ONTAP provides advanced features such as snapshots, replication, deduplication, and encryption, and is known for its high performance, scalability, and reliability.</p>"},{"location":"aws/data-store/fsx/#amazon-fsx-for-openzfs","title":"Amazon FSx for OpenZFS","text":"<p>Amazon FSx for OpenZFS is a fully managed service that provides highly reliable, scalable, high-performing, and feature-rich file storage built on the popular OpenZFS file system. FSx for OpenZFS combines the familiar features, performance, capabilities, and API operations of OpenZFS file systems with the agility, scalability, and simplicity of a fully managed AWS service.</p>"},{"location":"aws/data-store/fsx/#what-is-openzfs","title":"What is OpenZFS?","text":"<p>OpenZFS is an open-source, advanced file system and volume manager that is used by many enterprises to store and manage their data. OpenZFS provides advanced features such as snapshots, replication, deduplication, and encryption, and is known for its high performance, scalability, and reliability.</p>"},{"location":"aws/data-store/storage-gateway/","title":"Storage Gateways","text":""},{"location":"aws/data-store/storage-gateway/#amazon-s3-file-gateway","title":"Amazon S3 File Gateway","text":"<p>S3 File Gateway delivers on-premises file access to your Amazon S3 data.  Amazon S3 File Gateway is useful when you need on-premises applications to interact with Amazon S3 using standard file protocols like NFS or SMB.</p>"},{"location":"aws/data-store/storage-gateway/#tape-gateway","title":"Tape Gateway","text":"<p>Tape Gateway enables you to replace physical tapes on premises with virtual tapes in AWS without changing existing backup workflows.  Tape Gateway presents cloud-backed storage through an iSCSI-based virtual tape library to your on-premises backup application.</p>"},{"location":"aws/data-store/storage-gateway/#volume-gateway","title":"Volume Gateway","text":"<p>You can use Volume Gateway in conjunction with Windows and Linux servers on premises to provide scalable storage for on-premises applications with cloud recovery options.  Volume Gateway presents cloud-backed iSCSI block storage volumes to your on-premises applications.</p>"},{"location":"aws/deployment-and-operations-management/","title":"Deployment and Operations Management","text":""},{"location":"aws/deployment-and-operations-management/#types-of-deployments","title":"Types of Deployments","text":""},{"location":"aws/deployment-and-operations-management/#rolling-deployment","title":"Rolling Deployment","text":"<ul> <li>Description: A rolling deployment is a software release strategy that gradually replaces instances of an application with new versions.</li> </ul>"},{"location":"aws/deployment-and-operations-management/#ab-testing","title":"A/B Testing","text":"<ul> <li>Description: A/B testing is a software testing method that compares two versions of a web page or application to determine which one performs better.</li> </ul>"},{"location":"aws/deployment-and-operations-management/#canary-deployment","title":"Canary Deployment","text":"<ul> <li>Description: A canary deployment is a software release strategy that gradually introduces a new version of an application to a subset of users.</li> </ul>"},{"location":"aws/deployment-and-operations-management/#blue-green-deployment","title":"Blue-Green Deployment","text":"<ul> <li>Description: A blue-green deployment is a software release strategy that deploys a new version of an application alongside the existing version, allowing for quick rollback in case of issues.</li> </ul>"},{"location":"aws/deployment-and-operations-management/#blue-green-deployment-methods-on-aws","title":"Blue-Green Deployment methods on AWS","text":"<ul> <li>Route 53: Route 53 can be used to switch traffic between blue and green environments.</li> <li>Elastic Beanstalk: Elastic Beanstalk supports blue-green deployments out of the box.</li> <li>Auto Scaling Groups: Swap autoscaling group already primed with new version instances behind the ELB</li> <li>Launch Configuration: Change auto scaling group confifiguration to use new AMI version and terminate old instances</li> <li>AWS OpsWorks: Use OpsWorks to deploy new version and switch traffic to new instances</li> </ul>"},{"location":"aws/deployment-and-operations-management/#blue-green-contraindications","title":"Blue Green Contraindications","text":"<ul> <li>Cost: Running two environments at the same time can be expensive</li> <li>Data store: Data store schema is too tihjtly coupled to code changes</li> </ul>"},{"location":"aws/deployment-and-operations-management/#outputs","title":"Outputs","text":"<ul> <li>Types of deployment</li> <li>what blue-green is recommended or not</li> <li>Elastic Beanstalk</li> <li>Deployment options</li> <li>CloudFormation</li> <li>Infrastructure as code</li> </ul>"},{"location":"aws/deployment-and-operations-management/api-gateway/","title":"API Gateway","text":"<ul> <li>Managed, high availability, and scalable API management service</li> <li>Backed with custom code via lambda, as a proxy for another AWS service, or as a proxy for an HTTP endpoint</li> <li>Regional and edge-optimized endpoints</li> <li>Support API Keys and Usage Plans</li> <li>Using CloudFront behind the scenes and custom domains</li> <li>Can be published as products for monetization</li> </ul>"},{"location":"aws/deployment-and-operations-management/beanstalk/","title":"Elastic Beanstalk","text":"<p>Elastic Beanstalk is a Platform as a Service (PaaS) offering from AWS that simplifies the process of deploying, managing, and scaling web applications and services. It supports multiple programming languages, including Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker.</p>"},{"location":"aws/deployment-and-operations-management/beanstalk/#deployment-options","title":"Deployment Options","text":"<ul> <li>All at once: Deploys the new version to all instances simultaneously.</li> <li>Rolling: Deploys the new version in batches, with a percentage of instances updated at a time.</li> <li>Rolling with additional batch: Launch new version instances prior to taking any old version instances out of service</li> <li>Immutable: Deploys the new version to a fresh group of instances, then swaps the environment's CNAME to the new group.</li> <li>Blue/Green: Deploys the new version to a separate environment, then swaps CNAMEs to redirect traffic.</li> <li>Traffic Splitting: Deploys the new version to a separate environment, then gradually shifts traffic to the new version.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/","title":"Continuous Integration and Continuous Deployment (CI/CD)","text":""},{"location":"aws/deployment-and-operations-management/ci-cd/#continuous-integration","title":"Continuous Integration","text":"<ul> <li>Description: Continuous Integration (CI) is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#continuous-delivery","title":"Continuous Delivery","text":"<ul> <li>Description: Continuous Delivery (CD) is a software development practice where code changes are automatically built, tested, and prepared for release to production.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#continuous-deployment","title":"Continuous Deployment","text":"<ul> <li>Description: Continuous Deployment (CD) is a software development practice where every code change that passes automated testing is released to production without manual intervention.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#aws-codecommit","title":"AWS CodeCommit","text":"<ul> <li>Description: AWS CodeCommit is a fully managed source control service that makes it easy for teams to host secure and highly scalable private Git repositories.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#aws-codepipeline","title":"AWS CodePipeline","text":"<ul> <li>Description: Orchestrates CI/CD events triggered by new code commit</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#aws-codebuild","title":"AWS CodeBuild","text":"<ul> <li>Description: Fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#aws-codedeploy","title":"AWS CodeDeploy","text":"<ul> <li>Description: Automates code deployments to any instance, including EC2 instances, lambdas and on-premises servers.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#multi-account-pipelines","title":"Multi account Pipelines","text":"<ul> <li>Description: Use AWS Organizations to manage multiple accounts and use cross-account roles to deploy to multiple accounts.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#cloud9","title":"Cloud9","text":"<ul> <li>Description: Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#codeguru","title":"CodeGuru","text":"<ul> <li>Description: CodeGuru is a developer tool that provides intelligent recommendations for improving code quality and identifying performance issues in your applications.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#codestar","title":"CodeStar","text":"<ul> <li>Description: CodeStar is a cloud-based service that provides a unified user interface, enabling you to easily manage your software development activities in one place.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#x-ray","title":"X-Ray","text":"<p>Trace distributed systems, and visualiza your distributed pipelines</p>"},{"location":"aws/deployment-and-operations-management/ci-cd/#codeartifact","title":"CodeArtifact","text":"<ul> <li>Description: CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their development process.</li> </ul>"},{"location":"aws/deployment-and-operations-management/ci-cd/#outputs","title":"Outputs","text":"<ul> <li>CodePipeline is Key to CI/CD</li> <li>CodeBuild compiles and tests your code</li> <li>Isolate your production resources </li> </ul>"},{"location":"aws/deployment-and-operations-management/cloudformation/","title":"CloudFormation","text":""},{"location":"aws/deployment-and-operations-management/cloudformation/#what-is-cloudformation","title":"What is CloudFormation","text":"<p>AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you. You don't need to individually create and configure AWS resources and figure out what's dependent on what; AWS CloudFormation handles all of that.</p>"},{"location":"aws/deployment-and-operations-management/cloudformation/#concepts","title":"Concepts","text":"<ul> <li>Template: A JSON or YAML formatted text file that describes your AWS infrastructure and resources.</li> <li>Stack: A collection of AWS resources that you manage as a single unit. You create, update, and delete a collection of resources by creating, updating, and deleting stacks.</li> <li>Change Set: A summary of proposed changes to a stack that CloudFormation generates before making the changes. You can use a change set to understand the impact of the changes and decide whether to proceed.</li> </ul>"},{"location":"aws/deployment-and-operations-management/cloudformation/#stack-policies","title":"Stack Policies","text":"<p>Stack policies are JSON documents that define the update actions that can be performed on designated resources. You can use a stack policy to prevent resources from being accidentally updated or deleted during a stack update.</p> <ul> <li>Protect resources from being accidentally updated or deleted during a stack update.</li> <li>Add a stack policy via the console or CLI when creating a stack.</li> <li>Add a stack policy to an existing stack using only CLI.</li> <li>Once applied, a stack policy cannot be removed</li> </ul>"},{"location":"aws/deployment-and-operations-management/cloudformation/#best-practices","title":"Best Practices","text":"<ul> <li>AWS provides a python helper scripts which can help to install software</li> <li>Use cloudformation to make changes to your landscape</li> <li>Make use of change sets to identify potential trouble spots in your changes</li> <li>Use stack policies to prevent accidental updates to your resources</li> </ul>"},{"location":"aws/deployment-and-operations-management/deploy-infra-across-accounts/","title":"Deploy infrastructure across multiple AWS accounts","text":""},{"location":"aws/deployment-and-operations-management/deploy-infra-across-accounts/#stacksets","title":"StackSets","text":"<p>AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. StackSets are useful when you need to deploy a common set of AWS resources across multiple accounts and regions.</p>"},{"location":"aws/deployment-and-operations-management/deploy-infra-across-accounts/#service-catalog","title":"Service catalog","text":"<p>AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures.</p>"},{"location":"aws/deployment-and-operations-management/deploy-infra-across-accounts/#output","title":"Output","text":"<ul> <li>Control infrastructure and App Code together</li> <li>Provision in many accounts with StackSets</li> <li>Create portfolios of approved products</li> </ul>"},{"location":"aws/deployment-and-operations-management/enterprise-app/","title":"Enterprise App","text":""},{"location":"aws/deployment-and-operations-management/enterprise-app/#amazon-workspaces","title":"Amazon WorkSpaces","text":"<p>Amazon WorkSpaces is a managed, secure Desktop-as-a-Service (DaaS) solution that allows you to provision cloud-based virtual desktops for your users. WorkSpaces provides users with a consistent desktop experience across devices, and allows you to easily scale your desktop environment to meet the needs of your organization.</p>"},{"location":"aws/deployment-and-operations-management/enterprise-app/#amazon-appstream-20","title":"Amazon AppStream 2.0","text":"<p>Amazon AppStream 2.0 is a fully managed application streaming service that allows you to stream desktop applications securely to any device running a web browser. AppStream 2.0 provides users with instant access to applications without the need for complex installations or downloads, and allows you to centrally manage and scale your application delivery.</p>"},{"location":"aws/deployment-and-operations-management/enterprise-app/#amazon-connect","title":"Amazon Connect","text":"<p>Amazon Connect is a cloud-based contact center service that allows you to set up and manage a virtual contact center in minutes. Connect provides a full-featured contact center solution that includes voice, chat, and task routing, as well as real-time and historical analytics to help you optimize your customer interactions.</p>"},{"location":"aws/deployment-and-operations-management/enterprise-app/#amazon-chime","title":"Amazon Chime","text":"<p>Amazon Chime is a secure, real-time communication service that allows you to chat, call, and meet with your team from anywhere. Chime provides high-quality audio and video conferencing, screen sharing, and file sharing capabilities, and integrates with your existing applications and workflows.</p>"},{"location":"aws/deployment-and-operations-management/enterprise-app/#amazon-workdocs","title":"Amazon WorkDocs","text":"<p>Amazon WorkDocs is a secure, fully managed document storage and collaboration service that allows you to create, share, and collaborate on documents in the cloud. WorkDocs provides users with a central location to store and access documents, and includes features such as version control, file sharing, and real-time commenting.</p>"},{"location":"aws/deployment-and-operations-management/enterprise-app/#amazon-workmail","title":"Amazon WorkMail","text":"<p>Amazon WorkMail is a secure, fully managed email and calendaring service that allows you to send, receive, and manage email from your desktop or mobile device. WorkMail provides users with a familiar email experience, and includes features such as encryption, spam and virus protection, and integration with existing email clients.</p>"},{"location":"aws/deployment-and-operations-management/enterprise-app/#amazon-worklink","title":"Amazon WorkLink","text":"<p>Amazon WorkLink is a secure, fully managed service that allows you to provide your employees with secure access to internal websites and web applications from their mobile devices. WorkLink provides a secure browsing experience by rendering web content in the cloud and sending only the interactive content to the user's device.</p>"},{"location":"aws/deployment-and-operations-management/enterprise-app/#alexa-for-business","title":"Alexa for Business","text":"<p>Alexa for Business is a service that allows you to use Alexa to simplify your workday and increase productivity in the workplace. Alexa for Business provides users with voice-activated access to business applications, calendars, and information, and allows you to create custom skills to automate tasks and workflows.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/","title":"IoT Landscape","text":"<p>The IoT landscape is a complex and rapidly evolving ecosystem. It is made up of a wide range of devices, sensors, gateways, and cloud services that work together to collect, process, and analyze data from the physical world. This data can be used to monitor and control devices, optimize operations, and create new business opportunities.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#aws-iot-core","title":"AWS IoT Core","text":"<p>AWS IoT Core is a managed cloud service that allows you to connect devices to the cloud, securely interact with them, and manage their lifecycle. IoT Core provides features such as device provisioning, message routing, and device shadowing, and integrates with other AWS services to enable real-time data processing and analytics.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#aws-iot-events","title":"AWS IoT Events","text":"<p>Trigger alerts when events occur in your IoT environment.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#aws-iot-one-click","title":"AWS IoT One Click","text":"<p>One-click device provisioning and setup.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#aws-iot-greengrass","title":"AWS IoT Greengrass","text":"<p>Deploy AWS Lambda functions, docker imagesm or ML models to edge devices.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#aws-iot-device-management","title":"AWS IoT Device Management","text":"<p>Manage your IoT devices at scale.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#aws-iot-device-defender","title":"AWS IoT Device Defender","text":"<p>Monitor and secure your IoT devices.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#aws-iot-analytics","title":"AWS IoT Analytics","text":"<p>Analyze and visualize IoT data.</p>"},{"location":"aws/deployment-and-operations-management/iot-landscape/#outputs","title":"Outputs","text":"<ul> <li>IoT Core enables connection to devices</li> <li>Manage Groups of IoT devices at scale</li> <li>Utilize your IoT Device Data</li> </ul>"},{"location":"aws/deployment-and-operations-management/management-tools/","title":"Management Tools","text":""},{"location":"aws/deployment-and-operations-management/management-tools/#aws-config","title":"AWS Config","text":"<ul> <li>Allows you to assess, audit, and evaluate the configurations of your AWS resources</li> <li>Very useful for compliance and security</li> <li>Creates a basiline of various configuration settings and files that cna track variations</li> <li>AWS Config Rules can be used to evaluate the configuration settings of AWS resources</li> </ul>"},{"location":"aws/deployment-and-operations-management/management-tools/#aws-opsworks","title":"AWS OpsWorks","text":"<ul> <li>Configuration management service that helps you configure and operate applications of all shapes and sizes</li> <li>Supports Chef and Puppet</li> <li>Automates how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments</li> </ul>"},{"location":"aws/deployment-and-operations-management/system-manager/","title":"AWS System Manager","text":"<ul> <li>Centralized console and toolset for a wide variety of system management tasks</li> <li>Designed for both EC2 and on-premises servers</li> <li>SSM Agent is installed by default on recent AWS-provided AMIs</li> </ul>"},{"location":"aws/deployment-and-operations-management/system-manager/#inventory","title":"inventory","text":"<p>Collects metadata from your managed instances like OS type, IP address, and hostname</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#state-manager","title":"State Manager","text":"<p>Create states that represent a centaimn configuration is applied to instances</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#logging","title":"logging","text":"<p>Stream logs of our web servers directly to CloudWatch Logs</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#parameter-store","title":"Parameter Store","text":"<p>Shared secure storage for config data</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#insights-dashboard","title":"Insights dashboard","text":"<p>Account level dashboard of cloudtrail, config, trust advisor</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#resource-groups","title":"Resource Groups","text":"<p>Group resource through tags</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#maintenance-windows","title":"Maintenance Windows","text":"<p>Define schedules for instances to patch, update apps, or run scripts</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#automation","title":"Automation","text":"<p>Automate common maintenance and deployment tasks</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#run-command","title":"Run Command","text":"<p>Run commands on instances</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#patch-manager","title":"Patch Manager","text":"<p>Automate patching of instances</p>"},{"location":"aws/deployment-and-operations-management/system-manager/#ssm-documents","title":"SSM Documents","text":"<p>Define the actions that you want to perform on your instances</p>"},{"location":"aws/migrations/","title":"Migrations","text":"<p>This section contains information about the migrations that are available for the AWS provider.</p>"},{"location":"aws/migrations/#migratoin-strategies","title":"Migratoin Strategies","text":"<p>There are several migration strategies that you can use when moving workloads to AWS:</p> <ul> <li>Rehosting (Lift and Shift): Rehosting involves moving applications from on-premises to the cloud with minimal changes. This approach is typically used when you need to quickly migrate workloads to the cloud without making significant modifications to the application architecture.</li> <li>Replatforming (Lift, Tinker, and Shift): Replatforming involves making some optimizations to the application architecture during the migration process. This approach is used when you want to take advantage of cloud-native features and services to improve performance, scalability, or cost efficiency.</li> <li>Repurchasing (Drop and Shop): Repurchasing involves replacing existing applications with commercial off-the-shelf (COTS) software or software as a service (SaaS) solutions. This approach is used when you want to reduce maintenance costs, simplify operations, or take advantage of new features provided by third-party vendors.</li> <li>Rearchitecting (Rebuild): Rearchitecting involves redesigning and rebuilding applications to take full advantage of cloud-native features and services. This approach is used when you want to modernize legacy applications, improve scalability and performance, or enable new capabilities that were not possible in the on-premises environment.</li> <li>Retiring: Retiring involves decommissioning applications or workloads that are no longer needed. This approach is used when you want to reduce costs, simplify operations, or consolidate redundant systems.</li> <li>Retaining: Retaining involves keeping existing applications or workloads in their current state without making any changes. This approach is used when you have legacy applications that cannot be easily migrated to the cloud or when the cost of migration outweighs the benefits.</li> </ul>"},{"location":"aws/migrations/#cloud-adoption-framework","title":"Cloud Adoption Framework","text":""},{"location":"aws/migrations/#business","title":"Business","text":"<ul> <li>Creation of a strong business case for cloud adoption</li> <li>Business goals are congruent with cloud objectives</li> <li>Ability to measure benefits (TCO, ROI, etc.)</li> </ul>"},{"location":"aws/migrations/#people","title":"People","text":"<ul> <li>Evaluate organizational roles and structures, new skills and process needs and identify gaps</li> <li>Incentives and Career management aligned with evolving roles</li> <li>Training options appropriate for learning styles and needs</li> </ul>"},{"location":"aws/migrations/#governance","title":"Governance","text":"<ul> <li>Portfolio management geared for determining cloud eligibility and priority</li> <li>Program and project management more agile projects</li> <li>Align KPIs with newly enabled business capabilities</li> </ul>"},{"location":"aws/migrations/#platform","title":"Platform","text":"<ul> <li>Resource provisioning can happen with standardization</li> <li>Architecture patterns adjusted to leverage cloud native</li> <li>New application development skills and processes enable more agility</li> </ul>"},{"location":"aws/migrations/#security","title":"Security","text":"<ul> <li>Identity and access management models change</li> <li>Logging and audit capabilities are enhanced</li> <li>Shared responsibility model is understood and implemented</li> </ul>"},{"location":"aws/migrations/#operations","title":"Operations","text":"<ul> <li>Monitoring and management tools are cloud aware</li> <li>Automation is used to reduce manual intervention</li> <li>Incident response and change management processes are updated</li> </ul>"},{"location":"aws/migrations/hybrid-architectures/","title":"Hybrid Architectures","text":"<p>Hybrid cloud architectures combine on-premises infrastructure with cloud services to provide a flexible and scalable environment that can meet the needs of different workloads. This approach allows organizations to leverage the benefits of both on-premises and cloud resources, enabling them to optimize performance, cost, and security based on specific requirements.</p> <ul> <li>Hybrid Architectures make use of cloud resources along with on-premises resources</li> <li>Very common first step as a pilot for cloud migrations</li> <li>Infrastructure can augment or simply be extensions of on-prem platforms - VMWare, for example</li> <li>Ideally integrations are loosely coupled - meaning each end can exist without extensive knowledge of the other</li> </ul>"},{"location":"aws/migrations/migrating-data/","title":"Migrating data","text":""},{"location":"aws/migrations/migrating-data/#data-migration-service-dms","title":"Data Migration Service (DMS)","text":"<p>AWS Database Migration Service (DMS) is a service that helps you migrate databases to AWS quickly and securely. The service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to Amazon Aurora or Microsoft SQL Server to MySQL. DMS can also be used for continuous data replication with high availability.</p>"},{"location":"aws/migrations/migrating-data/#datasync","title":"DataSync","text":"<p>AWS DataSync is a data transfer service that makes it easy to automate moving data between on-premises storage and Amazon S3 or Amazon EFS. DataSync can transfer data at speeds up to 10 times faster than open-source tools, and it automatically handles many of the tasks related to data transfer, such as encryption, data integrity verification, and network optimization.</p> <ul> <li>Secure online service that automates moving data from on-premises storage to Amazon S3 or Amazon EFS.</li> <li>DataSync agents run on-premises and communicate with the DataSync service in the AWS Cloud.</li> </ul>"},{"location":"aws/migrations/migrating-data/#transfer-family","title":"Transfer Family","text":"<p>AWS Transfer Family is a fully managed service that enables you to transfer files over the internet using a range of protocols, including FTP, FTPS, SFTP, and Amazon S3. The service provides a highly available and scalable file transfer solution that is designed to meet the needs of a wide range of use cases, from simple file transfers to complex workflows that require automation and integration with other AWS services.</p> <ul> <li>AWS Transfer Family provides a fully managed service for transferring files over the internet using a range of protocols.</li> <li>The service is designed to meet the needs of a wide range of use cases, from simple file transfers to complex workflows that require automation and integration with other AWS services.</li> </ul>"},{"location":"aws/migrations/migrating-data/#output","title":"Output","text":"<ul> <li>DMS helps you migrate databases to AWS quickly and securely and also convert schema and migrate data.</li> <li>Datasysnc automates moving data between on-premises storage and Amazon S3 or Amazon EFS.</li> <li>Transfer Family provides a fully managed service for transferring files over the internet using a range of protocols and in a security way</li> </ul>"},{"location":"aws/migrations/migration-applications/","title":"Application Migration","text":""},{"location":"aws/migrations/migration-applications/#aws-migration-hub","title":"AWS Migration Hub","text":"<p>AWS Migration Hub is a service that provides a single location to track the progress of application migrations across multiple AWS and partner solutions. Using Migration Hub, you can view the status of your migrations, identify and troubleshoot issues, and access key metrics and logs.</p> <ul> <li>Application Discovery Service: Collects information about your on-premises data centers, servers, and applications to help you plan your migration to AWS.</li> <li>Instance recommendations: Provides recommendations for migrating your on-premises instances to AWS.</li> </ul>"},{"location":"aws/migrations/migration-applications/#application-discovery-service","title":"Application Discovery Service","text":"<p>AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers. The collected data is stored in an AWS Application Discovery Service repository, which you can access through the AWS Management Console.</p> <ul> <li>Agentless discovery: Collects information about your on-premises environment without installing agents on your servers.</li> <li>Continuous discovery: Automatically updates the information about your environment to help you plan your migration projects.</li> <li>Integration with AWS Migration Hub: Provides a single location to track the progress of your migration projects.</li> </ul>"},{"location":"aws/migrations/migration-applications/#application-migration-service-mgn","title":"Application Migration Service (MGN)","text":"<ul> <li>Migrate windows and Linux servers to AWS.</li> <li>Can be on-premises or in the cloud.</li> <li>Allow test environments to be created in AWS.</li> <li>MGN agent is installed on the source server.</li> <li>Free to use</li> </ul>"},{"location":"aws/migrations/migration-applications/#eks-anywhere","title":"EKS Anywhere","text":"<p>Amazon EKS Anywhere is a new deployment option for Amazon EKS that enables you to create and operate Kubernetes clusters on-premises using your own infrastructure. With Amazon EKS Anywhere, you can use the same APIs, tools, and cluster configurations that you use with Amazon EKS to run Kubernetes clusters on-premises.</p>"},{"location":"aws/migrations/migration-applications/#ecs-anywhere","title":"ECS Anywhere","text":"<p>Amazon ECS Anywhere is a new deployment option for Amazon ECS that enables you to run Amazon ECS tasks on-premises using your own infrastructure. With Amazon ECS Anywhere, you can use the same APIs, tools, and task definitions that you use with Amazon ECS to run tasks on-premises.</p>"},{"location":"aws/migrations/migration-applications/#aws-outposts","title":"AWS Outposts","text":"<p>AWS Outposts is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to virtually any datacenter, co-location space, or on-premises facility for a truly consistent hybrid experience. AWS Outposts is ideal for workloads that require low latency access to on-premises systems, local data processing, data residency, and migration of applications with local system interdependencies.</p>"},{"location":"aws/migrations/migration-applications/#aws-snow-edge","title":"AWS Snow Edge","text":"<p>AWS Snow Family is a collection of physical devices that help migrate data to and from AWS. AWS Snow Edge is a part of the AWS Snow Family and is a 100 TB data transfer device with on-board storage and compute capabilities. It can be used to collect, process, and move large amounts of data to and from AWS.</p>"},{"location":"aws/migrations/migration-applications/#outuput","title":"Outuput","text":"<ul> <li>Have a workload modernization Mindset</li> <li>Track AppMigrations with Migration Hub</li> <li>Modernization can also happen On-prem</li> </ul>"},{"location":"aws/migrations/migration-tips/","title":"Migration Tips","text":""},{"location":"aws/migrations/migration-tips/#migration-strategies","title":"Migration Strategies","text":"<ul> <li>understand the different strategies that companies might undertake when deciding if the cloud is right for them.</li> <li>understand the typical trade-offs and relative benefits for each strategy.</li> </ul>"},{"location":"aws/migrations/migration-tips/#cloud-adoption-framework","title":"Cloud Adoption Framework","text":"<ul> <li>Know what \"framework\" is and the realistic expectations that should accompany it.</li> <li>Understand the high-level components of the Cloud Adoption Framework.</li> <li>Most importantly, kno that cloud adoption is only partially a technology effort.</li> </ul>"},{"location":"aws/migrations/migration-tips/#hybrid-architecture","title":"Hybrid architecture","text":"<ul> <li>Be able to speak to some typical hybrid architectures that leverage both on-prem and cloud assets.</li> <li>Know that VMware has some nifty tools for abstracting the differences between on-prem and cloud.</li> </ul>"},{"location":"aws/migrations/migration-tips/#migrations","title":"Migrations","text":"<ul> <li>Understand the different types of migrations that can be undertaken.</li> <li>Know the tools that AWS provides to help with migrations.</li> <li>Understand the different types of data migrations that can be undertaken.</li> <li>Know the tools that AWS provides to help with data migrations.</li> </ul>"},{"location":"aws/migrations/migration-tips/#network-migration-and-cutover","title":"Network migration and cutover","text":"<ul> <li>Know various hybrid network architectures.</li> <li>Understand that smooth transitions from and to VPN and direct connect can be done using BGP routing; abrupt routes changes risk downtime.</li> </ul>"},{"location":"aws/migrations/network-migration/","title":"Network Migration","text":"<ul> <li>Ensure your IP addresses will not overlap between on-premises and AWS.</li> <li>VPC support IPv4 netmasks range from /16 to /28.</li> <li>/16: 65,536 IP addresses</li> <li>/28: 16 IP addresses</li> <li>Remember: 5 IP addresses are reserved in each subnet.</li> <li>/28: 11 IP addresses</li> </ul>"},{"location":"aws/migrations/togaf/","title":"TOGAF","text":"<p>The Open Group Architecture Framework (TOGAF) is a widely used framework for enterprise architecture that provides a comprehensive approach to designing, planning, implementing, and governing enterprise information technology architecture. TOGAF is based on the concept of an Enterprise Continuum, which represents the evolution of an organization's architecture from the current state to the target state.</p>"},{"location":"aws/migrations/togaf/#a-architecture-vision","title":"A: Architecture Vision","text":"<p>The Architecture Vision phase is the initial phase of the TOGAF ADM cycle and involves defining the scope, objectives, and stakeholders of the architecture effort. The Architecture Vision document outlines the high-level goals and objectives of the architecture initiative and provides a roadmap for achieving them.</p>"},{"location":"aws/migrations/togaf/#b-business-architecture","title":"B: Business Architecture","text":"<p>The Business Architecture phase focuses on defining the business strategy, governance, organization, and key business processes of the organization. The Business Architecture document describes the current and target state of the business architecture and identifies the key business drivers and requirements that will guide the development of the architecture.</p>"},{"location":"aws/migrations/togaf/#c-information-systems-architecture","title":"C: Information Systems Architecture","text":"<p>The Information Systems Architecture phase focuses on defining the information systems architecture that will support the business architecture. The Information Systems Architecture document describes the current and target state of the information systems architecture and identifies the key information systems and technologies that will be used to implement the architecture.</p>"},{"location":"aws/migrations/togaf/#d-technology-architecture","title":"D: Technology Architecture","text":"<p>The Technology Architecture phase focuses on defining the technology architecture that will support the information systems architecture. The Technology Architecture document describes the current and target state of the technology architecture and identifies the key technology components and standards that will be used to implement the architecture.</p>"},{"location":"aws/migrations/togaf/#e-opportunities-and-solutions","title":"E: Opportunities and Solutions","text":"<p>The Opportunities and Solutions phase focuses on identifying opportunities for improving the architecture and developing solutions to address them. The Opportunities and Solutions document describes the key opportunities and challenges facing the architecture initiative and outlines the solutions that will be implemented to address them.</p>"},{"location":"aws/migrations/togaf/#f-migration-planning","title":"F: Migration Planning","text":"<p>The Migration Planning phase focuses on developing a detailed plan for migrating from the current state to the target state. The Migration Planning document describes the migration strategy, roadmap, and timeline for implementing the architecture and identifies the key activities, resources, and dependencies that will be required to complete the migration.</p>"},{"location":"aws/migrations/togaf/#g-implementation-governance","title":"G: Implementation Governance","text":"<p>The Implementation Governance phase focuses on establishing governance mechanisms to ensure that the architecture is implemented successfully. The Implementation Governance document describes the governance structure, processes, and controls that will be used to monitor and manage the implementation of the architecture and ensure that it meets the desired outcomes.</p>"},{"location":"aws/migrations/togaf/#h-architecture-change-management","title":"H: Architecture Change Management","text":"<p>The Architecture Change Management phase focuses on managing changes to the architecture and ensuring that it remains aligned with the business goals and objectives. The Architecture Change Management document describes the change management process, roles, and responsibilities that will be used to assess, prioritize, and implement changes to the architecture.</p>"},{"location":"aws/networking/","title":"Networking","text":"<p>This section contains information about networking in AWS.</p>"},{"location":"aws/networking/#tips","title":"Tips","text":"<ul> <li>VPCs in General</li> <li>know the pros and cons of each On-prem to AWS connection mode</li> <li>know the functions of each VPC component</li> <li>Know what is meant by:<ul> <li>stateful: the firewall remembers the state of the connection</li> <li>stateless: the firewall does not remember the state of the connection</li> <li>connectionless: the firewall does not remember the state of the connection</li> <li>connection-based: the firewall remembers the state of the connection</li> </ul> </li> <li>Routing<ul> <li>Understand BGP and how to use weight to influence routing</li> <li>know how route table are prioritized</li> <li>what other routing protocols are supported by AWS</li> </ul> </li> <li>VPC peering<ul> <li>CIDR block overlap</li> <li>After VPC owner accepts a peering request, routes must be added to route tables</li> <li>Transitive peering is not supported</li> <li>A transit VPC is supported</li> </ul> </li> <li>Internet Gateways<ul> <li>Difference between NAT instance and NAT gateway</li> <li>Internet gateway is horizontally scaled and redundant</li> <li>NATs do have bandwidth limitations</li> <li>VPCs can use multiple NAT gateways</li> <li>Use Egress-Only Internet Gateway for IPv6</li> </ul> </li> <li>Route53<ul> <li>Understand the difference between a hosted zone and a domain</li> <li>Know the Weighted Routing Policy</li> <li>Route53 is a global service</li> </ul> </li> <li>CloudFront<ul> <li>Understand the difference between an origin and a distribution</li> <li>Know the difference between a web distribution and a RTMP distribution</li> <li>Understand the difference between a cache hit and a cache miss</li> </ul> </li> <li>Elastic Load Balancer<ul> <li>Know the difference between an Application Load Balancer and a Classic Load Balancer</li> <li>Know the difference between a Network Load Balancer and a Classic Load Balancer</li> <li>Know the difference between a Layer 4 and a Layer 7 Load Balancer</li> </ul> </li> </ul>"},{"location":"aws/networking/cloudfront/","title":"CloudFront","text":"<p>CloudFront is a content delivery network (CDN) service provided by AWS. It speeds up the distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so content is delivered with the best possible performance.</p>"},{"location":"aws/networking/cloudfront/#key-features","title":"Key Features","text":"<ul> <li>Global Reach: CloudFront has a large number of edge locations around the world, which helps ensure that your users receive your content with the lowest latency.</li> <li>Security: CloudFront integrates with AWS Shield for DDoS protection and AWS Web Application Firewall (WAF) for application security.</li> <li>High Performance: CloudFront uses a global network of edge locations to cache and deliver your content with low latency.</li> <li>Cost-Effective: CloudFront offers a pay-as-you-go pricing model with no upfront fees or long-term contracts.</li> </ul>"},{"location":"aws/networking/cloudfront/#outputs","title":"Outputs","text":""},{"location":"aws/networking/elastic-load-balancer/","title":"Elastic Load Balancer","text":"<p>An Elastic Load Balancer (ELB) is a managed load balancer service provided by AWS. It automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, to ensure optimal load distribution and fault tolerance.</p>"},{"location":"aws/networking/elastic-load-balancer/#types-of-elastic-load-balancers","title":"Types of Elastic Load Balancers","text":"<p>AWS provides the following types of Elastic Load Balancers:</p> <ul> <li>Application Load Balancer (ALB): Best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and container-based applications.</li> <li>Network Load Balancer (NLB): Best suited for load balancing of Transmission Control Protocol (TCP), Transport Layer Security (TLS), and User Datagram Protocol (UDP) traffic where extreme performance is required.</li> <li>Classic Load Balancer (CLB): Provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level.</li> <li>Gateway Load Balancer (GWLB): A new type of load balancer that is designed to work with third-party security appliances and software-based solutions.</li> </ul>"},{"location":"aws/networking/enhanced-networking/","title":"AWS Enhanced Networking","text":""},{"location":"aws/networking/enhanced-networking/#what-is-aws-enhanced-networking","title":"What is AWS Enhanced Networking?","text":"<p>AWS Enhanced Networking is a feature that allows you to get significantly higher network performance on your EC2 instances. It is available for instances that are launched in a Virtual Private Cloud (VPC). Enhanced Networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types.</p> <ul> <li>Generally used for High Performance Computing (HPC) workloads</li> <li>Uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types</li> <li>Might have to install driver if other than Amazon Linux HVM AMI</li> <li>Intel 82599 VF Interface: 10 Gbps</li> <li>Elastic Network Adapter (ENA): 25 Gbps</li> </ul>"},{"location":"aws/networking/enhanced-networking/#aws-placement-groups","title":"AWS Placement Groups","text":"<p>AWS Placement Groups are a feature that enables you to launch instances in a way that allows them to work together. This can be useful for applications that require low network latency, high network throughput, or both. There are three types of placement groups:</p> <ul> <li>Cluster Placement Group: Packs instances close together inside a single Availability Zone. This is recommended for applications that can benefit from low network latency, high network throughput, or both.</li> <li>Spread Placement Group: Spreads instances across underlying hardware (dedicated hosts). This is recommended for applications that have a small number of critical instances that should be kept separate from each other.</li> <li>Partition Placement Group: Spreads instances across many different partitions, which rely on different sets of racks, and can be used to reduce the likelihood of correlated failures.</li> </ul>"},{"location":"aws/networking/global-accelerator/","title":"Global Accelerator","text":"<p>Global Accelerator is a networking service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances.</p>"},{"location":"aws/networking/global-accelerator/#common-use-cases","title":"Common use cases","text":"<ul> <li>Public applications that require global availability</li> <li>Hybrid Network Architectures</li> <li>Origin Masking</li> <li>Multi-Region Failover</li> </ul>"},{"location":"aws/networking/global-accelerator/#outputs","title":"Outputs","text":"<ul> <li>Can improve latency and availability for your applications</li> <li>Multi-Region Failover</li> </ul>"},{"location":"aws/networking/hybrid-and-cross-account-networking/","title":"Hybrid and Cross-Account Networking","text":""},{"location":"aws/networking/hybrid-and-cross-account-networking/#direct-connect","title":"Direct Connect","text":"<p>A process for establishing a dedicated network connection from your on-premises network to AWS. Direct Connect is a good option when you require a \"big pipe\" into AWS, or when you have a lot of resources and services being provided on AWS to your corporate network. Direct Connect provides more predictable network performance and can reduce bandwidth costs. It supports BGP peering and routing.</p> <ul> <li>AWS connects its global network to your data center</li> <li>This reduces dependency on fluctuating internet bandwidth</li> <li>Work with partners to establish a dedicated network connection</li> <li>Great for frequent transfers of large data sets</li> <li>Can be quite pricey and take time to provision</li> </ul>"},{"location":"aws/networking/hybrid-and-cross-account-networking/#site-to-site-vpn","title":"Site-to-Site VPN","text":"<p>A Site-to-Site VPN connection is a secure connection between your on-premises network and your VPC. It is used when you need to establish a secure tunneled connection to a VPC, or when you need a redundant link for Direct Connect or other VPN. Site-to-Site VPN supports static routes or BGP peering and routing.</p> <ul> <li>Secure connection between your on-premises network and your VPC</li> <li>Can be used to establish a secure tunneled connection to a VPC</li> <li>Can be used as a redundant link for Direct Connect or other VPN</li> <li>Supports static routes or BGP peering and routing</li> <li>Dependent on your internet connection</li> </ul>"},{"location":"aws/networking/hybrid-and-cross-account-networking/#establishing-high-availability","title":"Establishing High Availability","text":"<p>When establishing high availability for your network, consider the following:</p> <ul> <li>Multiple Direct Connect Connections: Establish multiple Direct Connect connections to different AWS Direct Connect locations to ensure high availability.</li> <li>Multiple VPN Connections: Establish multiple VPN connections to different AWS VPN endpoints to ensure high availability.</li> <li>Multiple Availability Zones: Deploy your resources across multiple Availability Zones to ensure high availability.</li> <li>Multiple Regions: Deploy your resources across multiple regions to ensure high availability.</li> </ul>"},{"location":"aws/networking/hybrid-and-cross-account-networking/#outputs","title":"Outputs","text":"<ul> <li>Direct Connect Bypasses Internet: Direct Connect bypasses the internet and provides a dedicated connection to AWS.</li> <li>Site-to-Site VPN: Site-to-Site VPN can be the fastest way to establish a secure tunneled connection to a VPC.</li> <li>Transit Gateways Are Key in Hybrid Networks: Transit gateways are key components in hybrid networks that allow you to connect multiple VPCs and on-premises networks.</li> </ul>"},{"location":"aws/networking/internet-gateway/","title":"Internet Gateway","text":"<p>An Internet Gateway (IGW) is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It is a logical connection between your VPC and the internet.</p> <p>An IGW serves two main purposes:</p> <ol> <li>Inbound Traffic: Allows incoming traffic from the internet to reach your VPC resources.</li> <li>Outbound Traffic: Allows resources within your VPC to access the internet.</li> </ol>"},{"location":"aws/networking/internet-gateway/#egress-only-internet-gateway","title":"Egress-Only Internet Gateway","text":"<p>An Egress-Only Internet Gateway (EIGW) is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, but prevents the internet from initiating an IPv6 connection with your instances.</p>"},{"location":"aws/networking/internet-gateway/#nat-instance","title":"NAT Instance","text":"<p>A Network Address Translation (NAT) instance is an EC2 instance that forwards traffic from instances in a private subnet to the internet or other AWS services, while hiding the internal IP addresses of your instances. It allows instances in a private subnet to initiate outbound traffic to the internet, but prevents the internet from initiating a connection with those instances.</p>"},{"location":"aws/networking/internet-gateway/#nat-gateway","title":"NAT Gateway","text":"<p>A NAT Gateway is a managed service that allows instances in a private subnet to connect to the internet or other AWS services, while preventing the internet from initiating a connection with those instances. It provides better availability and bandwidth than a NAT instance.</p>"},{"location":"aws/networking/internet-gateway/#nat-instance-vs-nat-gateway","title":"NAT Instance vs. NAT Gateway","text":"NAT Instance NAT Gateway Manually deployed EC2 instance Managed service Requires regular maintenance Fully managed by AWS Single point of failure Highly available and redundant Limited bandwidth Scales automatically up to 45 Gbps Cost-effective for small workloads More expensive for larger workloads Elastic IP cannot be detached Elastic IP can be detached Cannot be assiciated with a security group Associated with a security group Not support Bastion Host Support Bastion Host"},{"location":"aws/networking/network-to-vpc/","title":"Network to VPC Connectivity","text":""},{"location":"aws/networking/network-to-vpc/#aws-managed-vpn","title":"AWS Managed VPN","text":"<ul> <li>what: AWS Managed IPsec VPN connection ove your existing internet connection.</li> <li>when: Quick and usually simple way to establish a secure tunneled connection to a VPC; Redundant link for Direct Connect or other VPN.</li> <li>Pros: Supports static routes or BGP peering and routing</li> <li>Cons: Dependent on your internet connection</li> </ul>"},{"location":"aws/networking/network-to-vpc/#aws-direct-connect","title":"AWS Direct Connect","text":"<ul> <li>What: Dedicated network connection from your on-premises network to AWS.</li> <li>When: Require a \"big pipe\" into AWS; lots of resources and services being provided on AWS to your corporate network.</li> <li>Pros: More predictable network performance: potential bandwidth cost reduction. up to 10 Gbps provisioned connections; Supports BGP peering and routing</li> <li>Cons: May require additional telecom and hosting provider relationships and/or new network circuits.</li> <li>How: Work with your existing data networking provider; create virtual interfaces to connecto to VPCs</li> </ul>"},{"location":"aws/networking/network-to-vpc/#aws-direct-connect-plus-vpn","title":"AWS Direct Connect Plus VPN","text":"<ul> <li>What: IPsec VPN connection over private lines</li> <li>When: Want to added security of encrypted traffic over a dedicated line</li> <li>Pros: More secure than Direct Connect alone</li> <li>Cons: More complex introduced by VPN layer</li> <li>How: Work with your existing Data Networking provider;</li> </ul>"},{"location":"aws/networking/network-to-vpc/#vpn-cloud-hub","title":"VPN Cloud Hub","text":"<ul> <li>What: Connect locations in a Hub and Spoke manner using AWS virtual private gateway</li> <li>When: Link remote offices for backup or primary WAN access to AWS resource and each other</li> <li>Pros: Reuses existing internet connection; Supports BGP routes to direct traffic</li> <li>Cons: Dependent on internet connection; No inherent redundancy</li> <li>How: Assign multiple Customer gateways to a virtual private gateway, each with their own BGP ASN and unique IP ranges</li> </ul>"},{"location":"aws/networking/network-to-vpc/#software-vpn","title":"Software VPN","text":"<ul> <li>What: You provide the VPN software and hardware</li> <li>When: You must manage both ends of the VPN connection for compliance reasons or you want to use VPN option not supported by AWS</li> <li>Pros: Ultimate flexibility and manageability</li> <li>Cons: You must design for any needed redundancy and manage the VPN software and hardware</li> <li>How: Install VPN software via marketplace appliance or on an EC2 instance</li> </ul>"},{"location":"aws/networking/network-to-vpc/#aws-transit-vpc","title":"AWS Transit VPC","text":"<ul> <li>What: Common strategy for connecting geographically distributed VPCs and locations in order to create a global network transit center</li> <li>When: Locations and VPC-deployed assets across multiple regions that need to communicate with one and other</li> <li>Pros: Ultimate flexibility and manageability but also AWS-managed VPN hub-and-spoken between VPCs</li> <li>Cons: You must design for any needed redundancy across the whole chain</li> <li>How: Providers like Cisco, Juniper Networks and Riverbed have offerings which work with their equipment and AWS VPC</li> </ul>"},{"location":"aws/networking/private-link/","title":"Private Link","text":"<p>Secure way to connect to VPC services without exposing them to the public internet.</p> <ul> <li>Establish private connectivity between VPCs, AWS services, and on-premises applications.</li> <li>Highly available and scalable.</li> <li>Traffic stays within the AWS network.</li> <li>Control API endpoints using VPC endpoint policies.</li> </ul>"},{"location":"aws/networking/private-link/#services-available","title":"Services Available","text":"<ul> <li>AWS Services: S3, DynamoDB, Kinesis, and more.</li> <li>SaaS Providers: CloudWatch, CloudTrail, and more.</li> <li>VPC Endpoints: Interface and Gateway.</li> </ul>"},{"location":"aws/networking/private-link/#outputs","title":"Outputs","text":"<ul> <li>Connect with services outside your VPC without exposing them to the public internet.</li> <li>Infinite of native service integrations.</li> <li>Marketplaces for third-party services.</li> </ul>"},{"location":"aws/networking/route53/","title":"Route 53","text":"<p>Route 53 is a scalable Domain Name System (DNS) web service designed to route end users to Internet applications by translating names into IP addresses. It connects user requests to infrastructure running in AWS.</p>"},{"location":"aws/networking/route53/#route-policies","title":"Route policies","text":"<p>Route 53 supports the following routing policies:</p> <ul> <li>Simple routing policy: Use when you have a single resource that performs a given function for your domain, such as a web server that serves content for the example.com website.</li> <li>Weighted routing policy: Use when you want to route traffic to multiple resources in proportions that you specify. For example, you might want to route 10% of your traffic to one group of servers and 90% to another group.</li> <li>Latency routing policy: Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.</li> <li>Failover routing policy: Use when you want to configure active-passive failover.</li> <li>Geolocation routing policy: Use when you want to route traffic based on the location of your users.</li> <li>Geoproximity routing policy: Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.</li> <li>Multivalue answer routing policy: Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.</li> </ul>"},{"location":"aws/networking/routing/","title":"AWS Routing table","text":"<p>A routing table is a set of rules, often viewed in a table format, that is used to determine where network traffic from a subnet or gateway should be directed. In AWS, a routing table is associated with a subnet and controls the routing of traffic from instances in the subnet. Each subnet in a VPC must be associated with a routing table, which controls the routing for the subnet.</p>"},{"location":"aws/networking/routing/#border-gateway-protocol-bgp","title":"Border Gateway Protocol (BGP)","text":"<p>Border Gateway Protocol (BGP) is a standardized exterior gateway protocol that is used to exchange routing information between different networks on the internet. BGP is the protocol used to make core routing decisions on the internet. It is a path vector protocol that routes traffic based on the shortest path and routing policies.</p> <ul> <li>Popular routing protocol for the internet</li> <li>Propagates routing information between networks to allow dynamic routing</li> <li>Required for Direct Connect AND optimal for VPN connections</li> <li>Alternative of not using BGP with AWS VPC is to use static routes</li> <li>AWS supports BGP community tagging as a way to control traffic scope and route preference</li> <li>Required TCP port 179 to be open for BGP to work</li> <li>Autonomous System Numbers (ASNs) are used to identify networks on the internet</li> <li>Weighting is local to the router and higher weight is preferred</li> </ul>"},{"location":"aws/networking/vpc-to-vpc/","title":"VPC Connection Options","text":""},{"location":"aws/networking/vpc-to-vpc/#vpc-peering","title":"VPC Peering","text":"<p>VPC Peering is a way to connect two VPCs together. It is a one-to-one connection. It is a private connection between two VPCs that allows you to route traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network.</p>"},{"location":"aws/networking/vpc-to-vpc/#aws-transit-gateway","title":"AWS Transit Gateway","text":"<p>AWS Transit Gateway is a service that enables you to connect multiple VPCs, on-premises networks, and remote networks to a single gateway. It acts as a hub that allows you to route traffic between connected networks. It simplifies network connectivity and management by providing a centralized way to connect multiple networks.</p>"},{"location":"aws/networking/vpc-to-vpc/#output","title":"Output","text":"<ul> <li>VPC Peering is not transitive, meaning that if VPC A is peered with VPC B and VPC B is peered with VPC C, VPC A and VPC C are not peered.</li> <li>Transit Gateways enable scalibity and simplification of network connectivity and management.</li> <li>Plan your CIDR Ranges carefully to avoid overlapping IP addresses.</li> </ul>"},{"location":"aws/networking/vpc-to-vpc/#example","title":"Example","text":"<p>To ensure that your 12 VPCs have non-overlapping CIDR blocks, follow these steps:</p> <ol> <li>Plan Your IP Addressing Scheme    Choose a large enough IP range (e.g., a /12 or /16 subnet) and divide it into smaller non-overlapping blocks.</li> </ol> <p>For example, if you start with 10.0.0.0/12 (which covers 10.0.0.0 - 10.15.255.255), you can divide it into multiple /16 subnets:</p> VPC CIDR Block VPC1 10.0.0.0/16 VPC2 10.1.0.0/16 VPC3 10.2.0.0/16 VPC4 10.3.0.0/16 VPC5 10.4.0.0/16 VPC6 10.5.0.0/16 VPC7 10.6.0.0/16 VPC8 10.7.0.0/16 VPC9 10.8.0.0/16 VPC10 10.9.0.0/16 VPC11 10.10.0.0/16 VPC12 10.11.0.0/16 <p>If you need smaller subnets, you can divide each /16 into /17 or /18 subnets while maintaining uniqueness.</p>"},{"location":"aws/security/","title":"Security","text":"<p>Security is a critical aspect of any cloud deployment, and AWS provides a wide range of services and features to help you secure your applications and data. In this section, we will cover some of the key security services and best practices that you should consider when designing and implementing your AWS architecture.</p>"},{"location":"aws/security/#concepts","title":"Concepts","text":""},{"location":"aws/security/#shared-responsibility-model","title":"Shared Responsibility Model","text":"<p>AWS operates on a shared responsibility model, where AWS is responsible for the security of the cloud infrastructure, and customers are responsible for securing their applications and data. This means that while AWS provides a secure infrastructure, customers are responsible for implementing security best practices and controls to protect their data and applications.</p>"},{"location":"aws/security/#principle-of-least-privilege","title":"Principle of Least Privilege","text":"<p>The principle of least privilege is a security best practice that states that users should only have the minimum level of access required to perform their job functions. By following this principle, you can reduce the risk of unauthorized access and limit the potential damage that can be caused by a compromised account.</p>"},{"location":"aws/security/#facets-of-identity-security","title":"Facets of Identity Security","text":"<ul> <li>Identity: uers, roles, root account, security credentials</li> <li>Authorization: policies</li> <li>Authentication: MFA, password policies</li> <li>Trust: cross-account access. Do other entities trust you?</li> </ul>"},{"location":"aws/security/#planning-for-things-to-go-wrong","title":"Planning for Things to Go wrong","text":"<ul> <li>Passwords will be stolen</li> <li>Exposed resources will be acessed</li> <li>Static access keys will be leaked</li> <li>How to limit security events</li> </ul>"},{"location":"aws/security/control-access/","title":"Controlling Access to Your Organization","text":""},{"location":"aws/security/control-access/#service-control-policy","title":"Service Control Policy","text":"<ul> <li>Applied to organizations, organizational units, or accounts</li> <li>Uses IAM policy syntax, but never grants permissions</li> <li>Effects are inherited by all accounts bellow the SCP's target</li> </ul>"},{"location":"aws/security/control-access/#aws-config","title":"AWS Config","text":"<p>Monitoring Best Practices across Your Organization</p> <ul> <li>Continuously monitor and record your AWS resource configurations</li> <li>Evaluate configurations against best practices</li> <li>Receive notifications when resources are out of compliance</li> </ul>"},{"location":"aws/security/control-access/#iam-identity-center","title":"IAM Identity Center","text":"<ul> <li>Maps users and groups from an Indentity provider</li> <li>Integrates with identity providers that support SAML 2.0</li> <li>AWS SSO can also act as an identity provider</li> </ul>"},{"location":"aws/security/control-access/#output","title":"Output","text":"<ul> <li>Deny Actions with Service Control Policies</li> <li>Dettect Compliance with AWS Config</li> <li>Control User Access with IAM Identity Center</li> </ul>"},{"location":"aws/security/direct-services/","title":"AWS Directory Services","text":""},{"location":"aws/security/direct-services/#types","title":"Types","text":""},{"location":"aws/security/direct-services/#simple-ad","title":"Simple AD","text":"<ul> <li>Low scale, low cost AD implementation based on Samba</li> <li>Simple user directory, or you need LDAP compatibility</li> </ul>"},{"location":"aws/security/direct-services/#ad-connector","title":"AD Connector","text":"<ul> <li>Allows on-premises users to log into AWS servifces with their existing credentials</li> <li>Single sign-on for on-prem employees and for adding EC2 intances</li> </ul>"},{"location":"aws/security/direct-services/#aws-directory-service-for-microsoft-ad","title":"AWS Directory Service for Microsoft AD","text":"<ul> <li>AWS-managed full microsoft AD running on Windows Server 2012 R2</li> <li>Enterprises that want hosted Microsoft AD or you need LDAP for Linux Apps</li> </ul>"},{"location":"aws/security/direct-services/#amazon-cognito","title":"Amazon Cognito","text":"<ul> <li>User directory that manages user sign-up, sign-in, and access control</li> <li>Mobile and web applications that need user authentication</li> </ul>"},{"location":"aws/security/direct-services/#aws-cloud-directory","title":"aws Cloud Directory","text":"<ul> <li>Hierarchical data store for use with other AWS services</li> <li>Applications that need a flexible directory structure</li> </ul>"},{"location":"aws/security/direct-services/#ad-connector-vs-simple-ad","title":"AD Connector VS Simple AD","text":"<ul> <li>AD Connector: Use existing AD credentials to log into AWS services</li> <li>Simple AD: standalone directory for small organizations based on Samba</li> <li>AD Connector: Existing AD users can log into AWS services</li> <li>AD Connector: Supports MFA</li> <li>Simple AD: No support for MFA</li> <li>Simple AD: No support for trust relationships</li> <li>Simple AD: Kerberos-based authentication</li> </ul>"},{"location":"aws/security/move-accounts/","title":"Account migrations","text":"<p>You can migrate an AWS account from another organization to another at any time.</p>"},{"location":"aws/security/move-accounts/#notes","title":"Notes","text":""},{"location":"aws/security/move-accounts/#closed-or-suspended-accounts-cannot-be-migrated","title":"Closed or suspended accounts cannot be migrated","text":"<p>You cannot migrate a closed or suspended account.</p>"},{"location":"aws/security/move-accounts/#seven-day-age-requirement","title":"Seven day age requirement","text":"<p>To migrate an account that you created in an organization, you must wait until at least seven days after the account was created.</p>"},{"location":"aws/security/move-accounts/#old-organization","title":"Old organization","text":"<p>Remove the migrating account from the old organization before migrating it to the new organization.</p>"},{"location":"aws/security/move-accounts/#migrate-a-management-account","title":"Migrate a management account","text":"<p>If you want to migrate the management account, you must remove all member accounts from the organization and delete the organization before migrating the management account to the new organization. </p>"},{"location":"aws/security/multiple-accounts/","title":"Multiple accounts","text":""},{"location":"aws/security/multiple-accounts/#aws-organizations","title":"AWS Organizations","text":"<p>AWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business.</p>"},{"location":"aws/security/multiple-accounts/#anatomy-of-an-organization","title":"Anatomy of an organization","text":"<ul> <li>Organization: The root entity that contains all of your AWS accounts.</li> <li>Organizational unit (OU): A container for accounts within an organization. OUs can contain other OUs, enabling you to create a hierarchy.</li> <li>Account: A standalone AWS account that is part of an organization.</li> <li>Service control policies (SCPs): Policies that you can use to control access to AWS services and resources across multiple accounts.</li> <li>Consolidated billing: A feature that enables you to consolidate payment for multiple AWS accounts within an organization.</li> <li>Management account: The account that you use to create an organization and manage its settings.</li> <li>Member account: An account that is part of an organization but is not the management account.</li> </ul>"},{"location":"aws/security/multiple-accounts/#why-use-multiple-accounts","title":"Why use Multiple Accounts","text":"<ul> <li>Security: Isolate workloads and resources to reduce the impact of security incidents.</li> <li>Compliance: Enforce compliance requirements by segregating workloads and resources.</li> </ul>"},{"location":"aws/security/multiple-accounts/#aws-control-tower","title":"AWS Control Tower","text":"<p>AWS Control Tower is a service that provides the easiest way to set up and govern a new, secure, multi-account AWS environment based on best practices established through AWS' experience working with thousands of enterprises as they move to the cloud.</p> <ul> <li>An automated approach to managing best practices</li> <li>Centralizes and automates guardrails across your organization</li> </ul>"},{"location":"aws/security/multiple-accounts/#landing-zone","title":"Landing Zone","text":"<ul> <li>What you provision when you start using Control Tower. Your landing zone is a recommended customizable starting point.</li> </ul>"},{"location":"aws/security/multiple-accounts/#guardrails","title":"Guardrails","text":"<ul> <li>A high-level rule governed by service control policies or AWS Config rules</li> </ul>"},{"location":"aws/security/multiple-accounts/#baseline","title":"Baseline","text":"<ul> <li>The combination of blueprints (CloudFormation stacks) and guardrails applied to a member account</li> </ul>"},{"location":"aws/security/multiple-accounts/#outputs","title":"Outputs","text":"<ul> <li>Group your Workloads with AWS Organizations</li> <li>Only Allow Access when needed</li> <li>Automate Best Practives with Control Tower</li> </ul>"},{"location":"aws/security/understand-user-access/","title":"Understanding User Access to Your Organization","text":""},{"location":"aws/security/understand-user-access/#the-greatest-vulnerability","title":"The Greatest Vulnerability","text":"<ul> <li>Leaking Access Keys</li> <li>Compromised User Credentials</li> </ul>"},{"location":"aws/security/understand-user-access/#iam-users-vs-iam-identity-center","title":"IAM Users vs IAM Identity Center","text":"<ul> <li>IAM Users: Users created in the AWS Management Console</li> <li>IAM Identity Center: Maps users and groups from an identity provider</li> <li>IAM Users: Federated Identity not supported</li> <li>IAM Users: One permissions set per user</li> </ul>"},{"location":"aws/security/understand-user-access/#ad-connector","title":"AD Connector","text":"<ul> <li>Use your existing users and groups to grant permissions in AWS accounts</li> <li>Easily integrates with AWS Identity Center (formely AWS SSO)</li> <li>Allows for a single source for credential management</li> </ul>"},{"location":"aws/security/understand-user-access/#multi-factor-authentication","title":"Multi-Factor Authentication","text":"<ul> <li>Adds an additional layer of security to your account</li> <li>Requires a second form of verification in addition to your password</li> <li>Can be used to secure access to the AWS Management Console, AWS CLI, and AWS API</li> </ul>"},{"location":"aws/security/understand-user-access/#use-cloudtrail-to-monitor-user-activity","title":"Use CloudTrail to Monitor User Activity","text":"<ul> <li>CloudTrail logs all API calls made on your account</li> <li>Can be used to track user activity and detect unauthorized access</li> <li>Can be used to troubleshoot operational issues and ensure compliance with security policies</li> </ul>"},{"location":"aws/security/understand-user-access/#outputs","title":"Outputs","text":"<ul> <li>IAM Identity Center Uses Roles</li> <li>AD Connector Uses your Existing AD</li> <li>MFA Can be your last line of Defense</li> </ul>"},{"location":"dcv/main/","title":"Nice DCV","text":"<p>Here is the step by step guide to install Nice DCV Ecosystem</p>"},{"location":"dcv/main/#ecosystem","title":"Ecosystem","text":"<ul> <li>DCV Broker</li> <li>DCV Gateway</li> <li>DCV Server</li> </ul>"},{"location":"dcv/main/#dcv-server","title":"DCV Server","text":"<p>1 - Add the following lines to the /etc/dcv/dcv.conf file:</p> <pre><code>On Linux Amazon DCV servers, Session Manager uses a local service user named **dcvsmagent**. This user is automatically created when the Session Manager agent is installed. You must grant this service user administrator privileges for Amazon DCV so that it can perform actions on behalf of other users.\n\n```text\n[security]\nadministrators=[\"dcvsmagent\"]\n</code></pre>"},{"location":"dcv/main/#dcv-broker","title":"DCV Broker","text":""},{"location":"dcv/main/#broker-consist-in","title":"Broker consist in:","text":"<p>The Amazon DCV Session Manager Agent and Broker</p> <ul> <li>The DCV Broker is a service that manages the connections between clients and servers. It is responsible for routing client requests to the appropriate server and managing the session lifecycle.</li> <li>The DCV Broker is a lightweight service that can be deployed on any server in your environment. It does not require any special configuration or installation.</li> <li>The DCV Broker is designed to be highly available and scalable. It can handle thousands of concurrent connections and can be deployed in a clustered configuration for high availability.</li> </ul>"},{"location":"dcv/main/#how-to-install-dcv-session-manager-broker","title":"How to install DCV Session Manager (Broker)","text":"<p>1 - The packages are digitally signed with a secure GPG signature. To allow the package manager to verify the package signature, you must import the Amazon DCV GPG key.</p> <pre><code>wget https://d1uj6qtbmh3dt5.cloudfront.net/NICE-GPG-KEY\n</code></pre> <p>2 - Import the GPG key:</p> <pre><code>gpg --import NICE-GPG-KEY\n</code></pre> <p>3 - Download the installation package</p> <pre><code>wget https://d1uj6qtbmh3dt5.cloudfront.net/2024.0/SessionManagerBrokers/nice-dcv-session-manager-broker_2024.0.457-1_all.ubuntu2404.deb\n</code></pre> <p>4 - Install the package</p> <pre><code>sudo apt install -y ./nice-dcv-session-manager-broker_2024.0.457-1_all.ubuntu2404.deb\n</code></pre> <p>5 - Check java version</p> <pre><code>java --version\n</code></pre> <p>If you have another version besides 11, then you have to specify the version on broker configuration file</p> <p>6 - (Optional) Specify the Java version</p> <pre><code>sudo vi /etc/dcv-session-manager-broker/session-manager-broker.properties\n</code></pre> <p>Add the following line to the file:</p> <pre><code>broker-java-home = 11\n</code></pre> <p>7 - Restart the DCV Session Manager Broker service</p> <pre><code> sudo systemctl start dcv-session-manager-broker &amp;&amp; sudo systemctl enable dcv-session-manager-broker\n</code></pre> <p>8 - Place a copy of the broker's self-signed certificate in your user directory.</p> <pre><code>sudo cp /var/lib/dcvsmbroker/security/dcvsmbroker_ca.pem $HOME\n</code></pre>"},{"location":"dcv/main/#dcv-broker-folder-location-on-linux","title":"DCV Broker Folder location on Linux","text":"<pre><code>/etc/dcv-session-manager-broker\n</code></pre>"},{"location":"github/api/commits/","title":"Commits","text":""},{"location":"github/api/commits/#list-all-the-commits-from-a-repository","title":"List all the commits from a repository","text":"<pre><code>curl -L \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer &lt;github-token&gt;\" \\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/ORG/REPOSITORY/commits\n</code></pre>"},{"location":"github/api/releases/","title":"Releases","text":""},{"location":"github/api/releases/#list-all-the-releases-from-a-repository","title":"list all the releases from a repository","text":"<pre><code>curl -L \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer &lt;github-token&gt;\" \\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/ORG/REPOSITORY/releases\n</code></pre>"},{"location":"github/api/repository/","title":"Repository","text":""},{"location":"github/api/repository/#list-members-of-a-repository","title":"List members of a repository","text":"<pre><code>curl -L \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer &lt;github-token&gt;\" \\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/ORG/REPOSITORY/collaborators\n</code></pre>"},{"location":"github/api/repository/#delete-a-repository","title":"Delete a repository","text":"<pre><code>curl -L \\\n    -X DELETE \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer &lt;github-token&gt;\" \\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/ORG/REPOSITORY\n</code></pre>"},{"location":"github/api/runner-group/","title":"Runner group","text":""},{"location":"github/api/runner-group/#list-all-the-offline-runner-groups","title":"List all the offline runner groups","text":"<pre><code>curl -L \\\n-H \"Accept: application/vnd.github+json\" \\\n-H \"Authorization: Bearer &lt;github-token&gt;\" \\\n-H \"X-GitHub-Api-Version: 2022-11-28\" \\\nhttps://api.github.com/orgs/&lt;org-name&gt;/actions/runner-groups/&lt;runner-group-number&gt;/runners | jq '.runners | .[] | select(.status == \"offline\") | .id'\n</code></pre>"},{"location":"github/api/runner-group/#delete-a-runner-from-runner-group","title":"Delete a runner from runner group","text":"<pre><code>curl -L \\\n-X DELETE \\\n-H \"Accept: application/vnd.github+json\" \\\n-H \"Authorization: Bearer &lt;github-token&gt;\" \\\n-H \"X-GitHub-Api-Version: 2022-11-28\" \\\nhttps://api.github.com/orgs/&lt;org-name&gt;/actions/runner-groups/&lt;runner-group-number&gt;/runners/&lt;runner-id&gt;\n</code></pre>"},{"location":"github/api/teams/","title":"Teams","text":""},{"location":"github/api/teams/#list-the-repositories-of-a-specific-team","title":"List the repositories of a specific team","text":"<pre><code>curl -L \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer &lt;github-token&gt;\" \\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/ORG/REPOSITORY/teams\n</code></pre>"},{"location":"github/github-app/get-temp-token/","title":"Script to get a temporary token","text":""},{"location":"github/github-app/get-temp-token/#script","title":"Script","text":"<p>The script will generate a temporary token that can be used to authenticate with the GitHub API. You can grab a py script here Script </p> <pre><code>import jwt\nimport time\nimport requests\n\npem=\"/path/key.pem\" # Path to the private key of the GitHub App\n\nwith open(pem, 'r') as pem_file:\n    signing_key = pem_file.read()\n\npayload = {\n    'iat': int(time.time()),\n    'exp': int(time.time()) + 600,\n    'iss': '12345' # GitHub App ID\n}\n\nencoded_jwt = jwt.encode(payload, signing_key, algorithm='RS256')\n\nurl = 'https://api.github.com/app/installations'\nheaders = {'Authorization': 'Bearer ' + encoded_jwt, 'Accept': 'application/vnd.github+json'}\nr = requests.get(url, headers=headers)\n\ninstallation_id = None\n\nfor resp in r.json():\n  installation_id = resp['id']\n\nurl_post = 'https://api.github.com/app/installations/' + str(installation_id) + '/access_tokens'\nr_post = requests.post(url_post, headers=headers)\n\ntoken_response = r_post.json()\n\ntoken_ready = token_response['token']\n\nprint(token_ready)\n\nprint(encoded_jwt)\n</code></pre>"},{"location":"github/github-app/get-temp-token/#before-use","title":"Before use","text":"<p>You need to have the following:</p> <ul> <li>A GitHub App with a private key</li> <li>The GitHub App ID</li> </ul>"},{"location":"github/runners/bash/","title":"How to remove offline runners from a runner group","text":"<pre><code>#!/usr/bin/env bash\n\nresponse=$(curl -L \\\n  -H \"Accept: application/vnd.github+json\" \\\n  -H \"Authorization: Bearer &lt;your temp token&gt;\" \\\n  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n  https://api.github.com/orgs/&lt;org-name&gt;/actions/runner-groups/&lt;runner-group-id&gt;/runners | jq '.runners | .[] | select(.status == \"offline\") | .id')\n\n# Check if the response is not empty\nif [ -n \"$response\" ]; then\n    # Loop over each line of the response\n    while IFS= read -r line; do\n        # Process each line (replace this with your own logic)\n        echo \"Received: $line\"\n        curl -L \\\n          -X DELETE \\\n          -H \"Accept: application/vnd.github+json\" \\\n          -H \"Authorization: Bearer &lt;your temp token&gt;\" \\\n          -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n          https://api.github.com/orgs/&lt;org-name&gt;/actions/runner-groups/&lt;runner-group-id&gt;/runners/$line\n    done &lt;&lt;&lt; \"$response\"\nelse\n    echo \"Empty response received.\"\nfi\n</code></pre>"},{"location":"gitlab/13-to-14/","title":"How to upgrade from 13 to 14","text":"<p>First, you have to check what is your minor version on 13, because I strongly recommend you go to the latest minor version of 13 before jump to 14</p>"},{"location":"gitlab/13-to-14/#note","title":"Note","text":"<p>If you are running GitLab on Ubuntu 16, then jumping from 13 to 14 will not be possible since GitLab does not provide such repository.</p>"},{"location":"gitlab/13-to-14/#about-migrate-to-hashed-storage","title":"About migrate to hashed storage","text":"<p>In GitLab 13.0, hashed storage is enabled by default and the legacy storage is deprecated. GitLab 14.0 eliminates support for legacy storage.</p>"},{"location":"gitlab/13-to-14/#list-projects-using-hashed-storage","title":"List projects using hashed storage","text":"<pre><code># Projects\nsudo gitlab-rake gitlab:storage:hashed_projects\nsudo gitlab-rake gitlab:storage:list_hashed_projects\n\n# Attachments\nsudo gitlab-rake gitlab:storage:hashed_attachments\nsudo gitlab-rake gitlab:storage:list_hashed_attachments\n</code></pre>"},{"location":"gitlab/13-to-14/#migrate-to-hashed-storage","title":"Migrate to hashed storage","text":"<pre><code>sudo gitlab-rake gitlab:storage:migrate_to_hashed\n</code></pre>"},{"location":"gitlab/13-to-14/#if-you-encounter-some-issues-running-the-migrate-to-hashed-command-then-a-workaround-is","title":"If you encounter some issues running the migrate to hashed command, then a workaround is","text":"<pre><code>wget -O /tmp/fix-legacy-hashed-storage-migration.rb https://gitlab.com/snippets/2039252/raw 2\ngitlab-rails runner /tmp/fix-legacy-hashed-storage-migration.rb\n</code></pre>"},{"location":"gitlab/13-to-14/#db-migration-is-required","title":"DB migration is required","text":"<p>You have to do the DB migration yourself in order to finish the upgrade process. Check the db migration</p>"},{"location":"gitlab/14-to-15/","title":"How to upgrade from 14 to 15","text":""},{"location":"gitlab/14-to-15/#note","title":"Note","text":"<p>Find where your version sits in the upgrade path:</p>"},{"location":"gitlab/14-to-15/#path-to-follow","title":"Path to follow","text":"<p>GitLab 14: 14.0.12 &gt; 14.3.6 &gt; 14.9.5 &gt; 14.10.5 &gt; 15.0.5</p>"},{"location":"gitlab/14-to-15/#upgrade-gitlab-to-1436","title":"Upgrade GitLab to 14.3.6","text":"<pre><code>sudo apt install gitlab-ce=14.3.6-ce.0\n</code></pre>"},{"location":"gitlab/14-to-15/#upgrade-gitlab-to-1495","title":"Upgrade GitLab to 14.9.5","text":"<pre><code>sudo apt install gitlab-ce=14.9.5-ce.0\n</code></pre>"},{"location":"gitlab/14-to-15/#check-if-db-migration-succeeds","title":"Check if DB migration succeeds","text":"<p><pre><code>sudo gitlab-rake db:migrate:status\n</code></pre> If all is up, then you are ready to go.</p>"},{"location":"gitlab/14-to-15/#check-gitlab-services-status","title":"check gitlab services status","text":"<pre><code>sudo gitlab-ctl status\n</code></pre> <p>If they are all running like this, then you are ready to go live <pre><code>run: alertmanager: (pid 15045) 88s; run: log: (pid 2411) 1127s\nrun: gitaly: (pid 15076) 87s; run: log: (pid 2398) 1127s\nrun: gitlab-exporter: (pid 15036) 89s; run: log: (pid 2410) 1127s\nrun: gitlab-kas: (pid 14996) 91s; run: log: (pid 14830) 163s\nrun: gitlab-pages: (pid 15017) 90s; run: log: (pid 2403) 1127s\nrun: gitlab-workhorse: (pid 15007) 90s; run: log: (pid 2393) 1127s\nrun: grafana: (pid 15063) 88s; run: log: (pid 2412) 1127s\nrun: logrotate: (pid 15088) 87s; run: log: (pid 2377) 1128s\nrun: nginx: (pid 15094) 87s; run: log: (pid 2394) 1127s\nrun: node-exporter: (pid 15102) 86s; run: log: (pid 2397) 1127s\nrun: postgres-exporter: (pid 15038) 88s; run: log: (pid 2401) 1127s\nrun: postgresql: (pid 2400) 1127s; run: log: (pid 2392) 1127s\nrun: prometheus: (pid 15187) 86s; run: log: (pid 2399) 1127s\nrun: puma: (pid 15197) 85s; run: log: (pid 2415) 1127s\nrun: redis: (pid 14610) 170s; run: log: (pid 2407) 1127s\nrun: redis-exporter: (pid 15203) 85s; run: log: (pid 2396) 1127s\nrun: registry: (pid 15027) 89s; run: log: (pid 2402) 1127s\nrun: sidekiq: (pid 15212) 85s; run: log: (pid 2395) 1127s\n</code></pre></p>"},{"location":"gitlab/14-to-15/#upgrade-gitlab-to-14105","title":"Upgrade GitLab to 14.10.5","text":"<pre><code>sudo apt install gitlab-ce=14.10.5-ce.0\n</code></pre>"},{"location":"gitlab/14-to-15/#check-if-db-migration-succeeds-on-14105","title":"Check if DB migration succeeds on 14.10.5","text":"<p><pre><code>sudo gitlab-rake db:migrate:status\n</code></pre> If all is up, then you are ready to go.</p>"},{"location":"gitlab/14-to-15/#upgrade-gitlab-to-1505","title":"Upgrade GitLab to 15.0.5","text":"<pre><code>sudo apt install gitlab-ce=15.0.5-ce.0\n</code></pre>"},{"location":"gitlab/14-to-15/#check-if-db-migration-succeeds-on-1505","title":"Check if DB migration succeeds on 15.0.5","text":"<p><pre><code>sudo gitlab-rake db:migrate:status\n</code></pre> If all is up, then you are ready to go.</p>"},{"location":"gitlab/14-to-15/#check-gitlab-services-status-on-1505","title":"check gitlab services status on 15.0.5","text":"<pre><code>sudo gitlab-ctl status\n</code></pre> <p>If does not start or you get a 408, then follow this process:</p>"},{"location":"gitlab/14-to-15/#edit-etcgitlabgitlabrb-and-add-the-following-line","title":"Edit /etc/gitlab/gitlab.rb and add the following line","text":"<pre><code>nginx['ssl_ciphers'] = \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384\"\n</code></pre>"},{"location":"gitlab/14-to-15/#reconfigure-gitlab","title":"Reconfigure GitLab","text":"<pre><code>sudo gitlab-ctl reconfigure\n</code></pre>"},{"location":"gitlab/15-to-16/","title":"How to upgrade from 15 to 16","text":"<p>GitLab 15: 15.0.5 &gt; 15.1.0 &gt; 15.2.0 &gt; 15.2.5 &gt; 15.4.6 &gt; 15.11.13 &gt; 16.0.0</p>"},{"location":"gitlab/15-to-16/#upgrade-gitlab-to-1510","title":"Upgrade GitLab to 15.1.0","text":"<pre><code>sudo apt install gitlab-ce=15.1.0-ce.0\n</code></pre>"},{"location":"gitlab/15-to-16/#you-have-to-restart-services","title":"You have to restart services","text":"<pre><code>sudo gitlab-ctl restart\n</code></pre>"},{"location":"gitlab/15-to-16/#upgrade-gitlab-to-1520","title":"Upgrade GitLab to 15.2.0","text":"<pre><code>sudo apt install gitlab-ce=15.2.0-ce.0\n</code></pre>"},{"location":"gitlab/15-to-16/#upgrade-gitlab-to-1525","title":"Upgrade GitLab to 15.2.5","text":"<pre><code>sudo apt install gitlab-ce=15.2.5-ce.0\n</code></pre>"},{"location":"gitlab/15-to-16/#upgrade-gitlab-to-1546","title":"Upgrade GitLab to 15.4.6","text":"<pre><code>sudo apt install gitlab-ce=15.4.6-ce.0\n</code></pre>"},{"location":"gitlab/15-to-16/#upgrade-gitlab-to-151113","title":"Upgrade GitLab to 15.11.13","text":"<pre><code>sudo apt install gitlab-ce=15.11.13-ce.0\n</code></pre>"},{"location":"gitlab/15-to-16/#upgrade-gitlab-to-1600","title":"Upgrade GitLab to 16.0.0","text":"<pre><code>sudo apt install gitlab-ce=16.0.0-ce.0\n</code></pre> <p>After upgrade to 16.0.0 I had this issue</p> <p></p> <p>In order to remove this error, let's upgrade to 16.0.8</p>"},{"location":"gitlab/15-to-16/#upgrade-gitlab-to-1608","title":"Upgrade GitLab to 16.0.8","text":"<pre><code>sudo apt install gitlab-ce=16.0.8-ce.0\n</code></pre>"},{"location":"gitlab/16-to-17/","title":"How to upgrade from 15 to 16","text":"<p>GitLab 16: 16.0.8 (only instances with lots of users or large pipeline variables history) &gt; 16.1.6 (instances with NPM packages in their package registry) &gt; 16.2.9 (only instances with large pipeline variables history) &gt; 16.3.7 &gt; 16.7.z &gt; latest 16.Y.Z.</p>"},{"location":"gitlab/16-to-17/#upgrade-gitlab-to-1609","title":"Upgrade GitLab to 16.0.9","text":"<pre><code>sudo apt install gitlab-ce=16.0.9-ce.0\n</code></pre>"},{"location":"gitlab/16-to-17/#you-have-to-restart-services","title":"You have to restart services","text":"<pre><code>sudo gitlab-ctl restart\n</code></pre>"},{"location":"gitlab/16-to-17/#upgrade-gitlab-to-1639","title":"Upgrade GitLab to 16.3.9","text":"<pre><code>sudo apt-get install gitlab-ce=16.3.9-ce.0\n</code></pre>"},{"location":"gitlab/16-to-17/#you-have-to-restart-services_1","title":"You have to restart services","text":"<pre><code>sudo gitlab-ctl restart\n</code></pre>"},{"location":"gitlab/16-to-17/#upgrade-gitlab-to-16710","title":"Upgrade GitLab to 16.7.10","text":"<pre><code>sudo apt-get install gitlab-ce=16.7.10-ce.0\n</code></pre>"},{"location":"gitlab/16-to-17/#you-have-to-restart-services_2","title":"You have to restart services","text":"<pre><code>sudo gitlab-ctl restart\n</code></pre>"},{"location":"gitlab/16-to-17/#upgrade-gitlab-to-161110","title":"Upgrade GitLab to 16.11.10","text":"<pre><code>apt-get install gitlab-ce=16.11.10-ce.0\n</code></pre>"},{"location":"gitlab/db-migration/","title":"Run incomplete database migrations","text":"<p>Database migrations can be stuck in an incomplete state, with a down status in the output of the sudo gitlab-rake db:migrate:status command.</p>"},{"location":"gitlab/db-migration/#display-status-of-database-migrations","title":"Display status of database migrations","text":"<p>See the background migrations documentation for how to check that migrations are complete when upgrading GitLab. To check the status of specific migrations, you can use the following Rake task:</p> <pre><code>sudo gitlab-rake db:migrate:status\n</code></pre>"},{"location":"gitlab/db-migration/#to-complete-these-migrations-use-the-following-rake-task","title":"To complete these migrations, use the following Rake task","text":"<pre><code>sudo gitlab-rake db:migrate\n</code></pre> <p>After the migration is done, you should run the status again and be sure that they are all UP  If you encounter some issues while migrating, then I suggest you to read here and follow this process. Here is what I had to run in order to the db migrate finish properly.</p> <pre><code>sudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,ci_stages,id,'[[\"id\"]\\, [\"id_convert_to_bigint\"]]']\nsudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,ci_builds_metadata,id,'[[\"id\"]\\, [\"id_convert_to_bigint\"]]']\nsudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,ci_builds_metadata,id,'[[\"build_id\"]\\, [\"build_id_convert_to_bigint\"]]']\nsudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,push_event_payloads,event_id,'[[\"event_id\"]\\, [\"event_id_convert_to_bigint\"]]']\nsudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,deployments,id,'[[\"deployable_id\"]\\, [\"deployable_id_convert_to_bigint\"]]']\nsudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,taggings,id,'[[\"id\"\\, \"taggable_id\"]\\, [\"id_convert_to_bigint\"\\, \"taggable_id_convert_to_bigint\"]]']\n</code></pre>"},{"location":"gitlab/db-migration/#some-services-are-down","title":"Some services are down","text":"<p>If you see that some services are down, you can run</p> <pre><code>sudo gitlab-ctl restart\n</code></pre>"},{"location":"gitlab/upgrade-minor-version/","title":"How to do a minor upgrade","text":""},{"location":"gitlab/upgrade-minor-version/#how-to-add-gitlab-repository-into-ubuntu","title":"How to add Gitlab repository into ubuntu","text":"<pre><code>curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash\n</code></pre>"},{"location":"gitlab/upgrade-minor-version/#check-status-of-all-services-gitlab","title":"Check status of all services gitlab","text":"<pre><code>sudo gitlab-ctl status\n</code></pre>"},{"location":"gitlab/upgrade-minor-version/#run-a-update-before-list-packages","title":"Run a update before list packages","text":"<pre><code>apt update\n</code></pre>"},{"location":"gitlab/upgrade-minor-version/#check-all-the-available-versions","title":"Check all the available versions","text":"<pre><code>apt list -a | grep gitlab-ce\n</code></pre>"},{"location":"gitlab/upgrade-minor-version/#check-version-available-for-upgrade","title":"Check version available for upgrade","text":"<pre><code>apt list --upgradable | grep gitlab\n</code></pre>"},{"location":"gitlab/upgrade-minor-version/#upgrade-to-specific-version-my-case-131215","title":"Upgrade to specific version (my case 13.12.15)","text":"<pre><code>apt install gitlab-ce=13.12.15-ce.0\n</code></pre>"},{"location":"hacking/cyber-sec-search-engine/","title":"Cybersecurity Search Engine","text":""},{"location":"hacking/cyber-sec-search-engine/#introduction","title":"Introduction","text":"<p>The Cybersecurity Search Engine is a tool that allows users to search for cybersecurity-related information. The search engine is designed to help users find relevant information quickly and easily. It provides a simple and intuitive interface that allows users to enter search queries and receive relevant results.</p>"},{"location":"hacking/cyber-sec-search-engine/#1-dehashed","title":"1 - Dehashed","text":"<p>Dehashed is a data breach search engine that allows users to search for leaked data. It provides access to a vast database of breached data, including email addresses, passwords, and other sensitive information. Users can search for specific data breaches or browse through the database to find relevant information.</p> <p>Link: Dehashed</p>"},{"location":"hacking/cyber-sec-search-engine/#2-securitytrails","title":"2 - SecurityTrails","text":"<p>SecurityTrails is a cybersecurity search engine that provides information about domain names, IP addresses, and other cybersecurity-related data. It allows users to search for information about specific domains, IP addresses, and other online assets. SecurityTrails provides a range of tools and features to help users investigate cybersecurity threats and vulnerabilities.</p> <p>Link: SecurityTrails</p>"},{"location":"hacking/cyber-sec-search-engine/#3-google-dorking","title":"3 - Google Dorking","text":"<p>Google Dorking is a technique that involves using advanced search operators to find sensitive information on the internet. It allows users to search for specific types of data, such as passwords, usernames, and other sensitive information. Google Dorking can be used to find information that is not easily accessible through traditional search engines.</p> <p>Link: Google Dorking</p>"},{"location":"hacking/cyber-sec-search-engine/#4-exploit-database","title":"4 - Exploit Database","text":"<p>The Exploit Database is a cybersecurity search engine that provides information about security vulnerabilities and exploits. It allows users to search for information about specific vulnerabilities, exploits, and security issues. The Exploit Database provides a range of tools and features to help users identify and mitigate cybersecurity risks.</p> <p>Link: Exploit Database</p>"},{"location":"hacking/cyber-sec-search-engine/#5-zoomeye","title":"5 - ZoomEye","text":"<p>ZoomEye is a cybersecurity search engine that provides information about internet-connected devices. It allows users to search for information about specific devices, such as webcams, routers, and other internet-connected devices. ZoomEye provides a range of tools and features to help users identify and secure vulnerable devices.</p> <p>Link: ZoomEye</p>"},{"location":"hacking/cyber-sec-search-engine/#6-urlscan","title":"6 - URLScan","text":"<p>URLScan is a cybersecurity search engine that provides information about malicious URLs. It allows users to search for information about specific URLs, such as phishing sites, malware sites, and other malicious websites. URLScan provides a range of tools and features to help users identify and block malicious URLs.</p> <p>Link: URLScan</p>"},{"location":"hacking/cyber-sec-search-engine/#7-shodan","title":"7 - Shodan","text":"<p>Shodan is a cybersecurity search engine that provides information about internet-connected devices. It allows users to search for information about specific devices, such as webcams, routers, and other internet-connected devices. Shodan provides a range of tools and features to help users identify and secure vulnerable devices.</p> <p>Link: Shodan</p>"},{"location":"hacking/cyber-sec-search-engine/#8-censys","title":"8 - Censys","text":"<p>Censys is a cybersecurity search engine that provides information about internet-connected devices. It allows users to search for information about specific devices, such as webcams, routers, and other internet-connected devices. Censys provides a range of tools and features to help users identify and secure vulnerable devices.</p> <p>Link: Censys</p>"},{"location":"hacking/cyber-sec-search-engine/#9-grepapp","title":"9 - Grep.app","text":"<p>Grep.app is a cybersecurity search engine that provides information about code snippets and programming languages. It allows users to search for specific code snippets, programming languages, and other technical information. Grep.app provides a range of tools and features to help users find and analyze code snippets.</p> <p>Link: Grep.app</p>"},{"location":"hacking/cyber-sec-search-engine/#10-fullhunt","title":"10 - FullHunt","text":"<p>FullHunt is a cybersecurity search engine that provides information about security threats and vulnerabilities. It allows users to search for information about specific threats, vulnerabilities, and security issues. FullHunt provides a range of tools and features to help users identify and mitigate cybersecurity risks.</p> <p>Link: FullHunt</p>"},{"location":"hacking/cyber-sec-search-engine/#11-wayback-machine","title":"11 - Wayback Machine","text":"<p>The Wayback Machine is a cybersecurity search engine that provides information about historical versions of websites. It allows users to search for information about specific websites and view historical versions of web pages. The Wayback Machine provides a range of tools and features to help users explore the history of the internet.</p> <p>Link: Wayback Machine</p>"},{"location":"hacking/cyber-sec-search-engine/#12-wigle","title":"12 - WiGLE","text":"<p>WiGLE is a cybersecurity search engine that provides information about wireless networks. It allows users to search for information about specific wireless networks, such as Wi-Fi networks and other wireless devices. WiGLE provides a range of tools and features to help users identify and secure wireless networks.</p> <p>Link: WiGLE</p>"},{"location":"hacking/cyber-sec-search-engine/#13-gray-hat-warfare","title":"13 - Gray Hat Warfare","text":"<p>Gray Hat Warfare is a cybersecurity search engine that provides information about hacking tools and techniques. It allows users to search for information about specific hacking tools, techniques, and other cybersecurity-related topics. Gray Hat Warfare provides a range of tools and features to help users explore the world of hacking.</p> <p>Link: Gray Hat Warfare</p>"},{"location":"hacking/dns/","title":"DNS DATA","text":"<p>security trail</p>"},{"location":"hacking/forensic/","title":"Forensic tools","text":""},{"location":"hacking/forensic/#1-pyew","title":"1 - Pyew","text":"<p>Pyew is a Python tool to analyse malware. It is capable of extracting information from the malware and can be used to analyse the malware in a controlled environment.</p> <p>Link: Pyew</p>"},{"location":"hacking/forensic/#2-wireshark","title":"2 - Wireshark","text":"<p>Wireshark is a network protocol analyser that can be used to capture and analyse network traffic. It can be used to identify malicious network activity and to monitor network traffic.</p> <p>Link: Wireshark</p>"},{"location":"hacking/forensic/#3-encase","title":"3 - EnCase","text":"<p>EnCase is a digital forensic tool that can be used to analyse digital evidence. It can be used to recover deleted files, analyse file systems, and to identify and recover digital evidence.</p> <p>Link: EnCase</p>"},{"location":"k8s/delete-stuck-pod/","title":"Force a pod termination","text":""},{"location":"k8s/delete-stuck-pod/#delete-a-pod-when-is-stuck-in-a-terminating-state","title":"Delete a pod when is stuck in a terminating state","text":"<pre><code>#!/bin/bash\n\n# Get all pods in Terminating state and delete them\nterminating_pods=$(kubectl get pods | grep Terminating | awk '{print $1}')\n\nif [ -n \"$terminating_pods\" ]; then\n    echo \"Deleting Terminating pods: $terminating_pods\"\n    kubectl patch pod $terminating_pods -n actions-runner-system -p '{\"metadata\":{\"finalizers\":null}}'\nelse\n    echo \"No pods in Terminating state found.\"\nfi\n</code></pre>"},{"location":"k8s/pods/","title":"Pods commands and scripts","text":""},{"location":"k8s/pods/#restart-all-pods-from-a-specific-namespace","title":"Restart all pods from a specific namespace","text":"<pre><code>#!/bin/bash\n\n# Check if namespace argument is provided\nif [ $# -ne 1 ]; then\necho \"Usage: $0 &lt;namespace&gt;\"\nexit 1\nfi\n\nNAMESPACE=$1\n\n# Get all pod names within the specified namespace\nPOD_NAMES=$(kubectl get pods -n $NAMESPACE -o=jsonpath='{.items[*].metadata.name}')\n\n# Restart each pod\nfor POD_NAME in $POD_NAMES; do\necho \"Restarting pod: $POD_NAME\"\nkubectl delete pod $POD_NAME -n $NAMESPACE --grace-period=0 --force\ndone\n\necho \"All pods in namespace $NAMESPACE have been restarted.\"\n</code></pre>"},{"location":"k8s/pods/#download-the-script","title":"Download the script","text":"<p>restart-pods.sh</p>"},{"location":"k8s/kcna/application-delivery/","title":"Application Delivery","text":"<p>Cloud native techniques and technilogies support rapid innovation and reliability when delivering applications.</p>"},{"location":"k8s/kcna/application-delivery/#gitops","title":"GitOps","text":"<ul> <li>Tools like Flux and ArgoCD enable GitOps practices by automating application deployment and management using Git as the source of truth.</li> <li>Flux is built on top of the GitOps Toolkit standard library.</li> <li>Flux and ArgoCD are both written in Go.</li> </ul>"},{"location":"k8s/kcna/application-delivery/#cicd","title":"CI/CD","text":"<p>Continuous Integration and Continuous Deployment (CI/CD) are practices that require developers to integrate code into a shared repository several times a day.</p>"},{"location":"k8s/kcna/basic-info/","title":"Resource","text":"<p>1 - An object of a certain type in the kubernetes API.  2 - You can list all available resources with <code>kubectl api-resources</code>. 3 - You can create your own custom resources using Custom Resource Definitions (CRDs). 4 - Use an init container to run a task before a Pod starts.</p>"},{"location":"k8s/kcna/basic-info/#pods-management-resources","title":"Pods management resources","text":"<p>1 - ReplicaSet - Ensures a specified number of pod replicas are running at any given time. 2 - Deployment - Manages ReplicaSets and provides declarative updates for Pods. 3 - Imperative command to create a Deployment: kubectl create deployment nginx --image=nginx --replicas=3 4 - StatefulSet - Manages stateful applications, providing stable network IDs and persistent storage.</p>"},{"location":"k8s/kcna/basic-info/#daemonset-dynamically-runs-a-copy-of-a-pod-on-all-or-selected-nodes-in-the-cluster","title":"DaemonSet - Dynamically runs a copy of a Pod on all or selected nodes in the cluster.","text":""},{"location":"k8s/kcna/basic-info/#job-runs-a-task-once","title":"Job - Runs a task once.","text":""},{"location":"k8s/kcna/basic-info/#cronjob-runs-a-task-periodically","title":"CronJob - Runs a task periodically.","text":""},{"location":"k8s/kcna/basic-info/#kubernetes-architecture","title":"Kubernetes Architecture","text":"<p>1 - Worker Nodes - Responsible for running containers. 2 - Control Plane - Manages the cluster, including the API server, scheduler, and controller manager. 3 - API Server (Control Plane) - Provides a unified interface for interacting with the cluster. 4 - Scheduler (Control Plane) - Assigns Pods to Nodes. 5 - Controller Manager (Control Plane) - Runs controllers that manage the cluster state. 6 - etcd (Control Plane) - Stores cluster state. 7 - Cloud Controller Manager (Control Plane) - Manages cloud provider features. 8 - kube-proxy (Worker Nodes) - Provides network routing and load balancing for Pods. 9 - Container Runtime (Worker Nodes) - Runs containers. 10 - kubelet (Worker Nodes) - Agent that runs Pods and provides runtime features. 11 - Kubernetes CRI - Interface between container runtime and kubelet. 12 - kubelet no longer uses dockershim to support Docker, since Docker does not implement the CRI.</p>"},{"location":"k8s/kcna/basic-info/#kubernetes-api","title":"Kubernetes API","text":"<p>1 - REST API - A set of endpoints that can be used to interact with the Kubernetes API. 2 - Kubernetes API Server - The server that exposes the Kubernetes API. 3 - Formats: JSON, YAML - The formats used to define Kubernetes resources.</p>"},{"location":"k8s/kcna/basic-info/#containers","title":"Containers","text":"<p>1 - Linux control groups (cgroups) - Used to provide container isolation. 2 - You can have more then one container in a Pod. 3 - A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.</p>"},{"location":"k8s/kcna/basic-info/#scheduling","title":"Scheduling","text":"<p>1 - Scheduling is the process of assigning Pods to Nodes. 2 - Scheduling occurs when a new Pod is created or when a Node is added to the cluster. 3 - Scheduler takes into account things like resources requirements, Pod affinity, and tolerations when selecting a Node for a Pod.</p>"},{"location":"k8s/kcna/cloud-native-architecture/","title":"Cloud Native Architecture","text":"<p>1 \u2013 Cloud native technology is important because it removes roadblocks to innovation.</p>"},{"location":"k8s/kcna/cloud-native-architecture/#autoscaling","title":"AutoScaling","text":"<p>1 - Vertical scaling: Adjusting the resources (CPU, memory) of existing instances. 2 - Horizontal scaling: Adding more instances, apps and replicas. 3 \u2013 Horizontal Pod Autoscaler (HPA): Automatically adjust the number of pods in a deployment based on CPU utilization. 4 \u2013 Cluster Autoscaler: Automatically adjust the number of nodes in a cluster based on CPU utilization.</p>"},{"location":"k8s/kcna/cloud-native-architecture/#serveles","title":"Serveles","text":"<p>1 - Serverless computing: Running code without having to provision or manage servers.</p>"},{"location":"k8s/kcna/cloud-native-architecture/#community-and-governance","title":"Community and governance","text":"<p>1 - CNCF: Cloud Native Computing Foundation. 2 - CNCF makes decisions through public discussion and voting.</p>"},{"location":"k8s/kcna/cloud-native-architecture/#organizational-personas","title":"Organizational Personas","text":"<p>1 - Organizational personas are roles that describe the work of managing cloud native applications. 2 - Site Reliability Engineer (SRE): Responsible for creating and maintaining SLOs and SLIs.</p>"},{"location":"k8s/kcna/cloud-native-architecture/#open-standards","title":"Open Standards","text":"<p>1 - Open Container Initiative (OCI): A community-driven open standard for container images. 2 - Runc (runtime-spec): A specification for container runtimes.</p>"},{"location":"k8s/kcna/container-orchestration/","title":"Container Orchestration","text":"<p>1 - Orchestration means using automation to manage the lifecycle of containers.</p>"},{"location":"k8s/kcna/container-orchestration/#container-runtime","title":"Container Runtime","text":"<p>1 - Container runtime is the software that manages the containers. 2 - CRI (Container Runtime Interface) is the interface between the container runtime and the kubelet. 3 - CRI-O and Containerd are two popular container runtimes.</p>"},{"location":"k8s/kcna/container-orchestration/#kubernetes-security","title":"Kubernetes security","text":"<p>1 - 4Cs of Cloud Native Security: Cloud, Cluster, Container and code 2 - Client Certificates - API Server uses a signed X509 client certificate to authenticate a user.  3 - OpenID Connect - Uses a JSON Web Token (JWT) to authenticate users. 4 - OPA Gatekeeper - A Kubernetes admission controller that enforces policies on pods.</p>"},{"location":"k8s/kcna/container-orchestration/#kubernetes-networking","title":"Kubernetes networking","text":"<p>1 - Kubernetes uses a virtual cluster network to connect pods. 2 - The cluster Domain Name Server (DNS) allows containers to discover Services by hostname 3 - Network policies allow you to control the traffic flow between pods. 4 - Pods are non-isolated by default. 5 - If a Network Policy selects a pod, the Pod is isolated from all other pods in the cluster. Only traffic allowed by the Network Policy is allowed. 6 - Isolation is treated separately for incoming (ingress) traffic and outgoing (egress) traffic.</p>"},{"location":"k8s/kcna/container-orchestration/#services","title":"Services","text":"<p>1 - Services expose an application running on a set of Pods as a network service. 2 - Services Types: \u2014 ClusterIP: Expose internally within the cluster network     \u2013 NodePort: Expose externally on a static port on each node     \u2013 LoadBalancer: Expose using a cloud provider load balancer     \u2013 ExternalName: Provide a DNS name for an external service 3 - Headless Services: A service with no cluster IP address. 4 - A service without a selector requires any endpoints to be manually created. 5 - There are two main service discovery mechanisms: DNS and environment variables. 6 - An Ingress exposes and application externally and routes traffic to a Service. It can also provide additional functionality like SSL termination,</p>"},{"location":"k8s/kcna/container-orchestration/#service-mesh","title":"Service Mesh","text":"<p>1 - Service Mesh is a software system that provides a way to manage communication between applications components, often adding additional functionality like logging, tracing or encryption. 2 - Two main parts of service mesh are the control plane and service proxy/data plane. 3 - Sidecar - An additional container running in a Pod alongside the main container.  4 - Service Mesh Interface (SMI) \u2013 A specification for a set of APIs that define a common interface for service mesh implementations.</p>"},{"location":"k8s/kcna/container-orchestration/#storage","title":"Storage","text":"<p>1 - Volume - Provide external storage to your kubernetes containers to store application data. 2 - PersistentVolume - Define a dynamically consumable storage resource. 3 - PersistentVolumeClaim - Request a specific size and access mode for a PersistentVolume. 4 - Rook - Open source storage platform for Kubernetes.</p>"},{"location":"k8s/kcna/deployment/","title":"What is deployment","text":"<p>A Deployment in Kubernetes is a resource object that provides declarative updates to applications. It manages the creation and scaling of a set of Pods, ensuring that the desired number of replicas are running at any given time. Deployments allow you to define the desired state of your application, and Kubernetes will automatically manage the underlying Pods to match that state.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n\n  selector:\n    matchLabels:\n      app: web-app\n\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n        - name: web\n          image: nginx:1.25-alpine\n          ports:\n            - containerPort: 80\n\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 5\n            periodSeconds: 10\n\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 15\n            periodSeconds: 20\n</code></pre>"},{"location":"k8s/kcna/replica-set/","title":"ReplicaSet","text":"<p>A ReplicaSet is a Kubernetes controller whose job is very simple:</p> <p>Ensure that a specified number of identical Pods are always running.</p> <p>If a Pod dies, the ReplicaSet creates a new one. If there are too many Pods, it deletes the extras.</p> <p>What a ReplicaSet does (core responsibility)</p> <p>In Kubernetes, a ReplicaSet continuously watches the cluster and enforces this rule:</p> <p>desired replicas == actual running Pods</p> <p>So why would anyone create a ReplicaSet directly? 1) You want fixed Pods, no rollout behavior at all</p> <p>A Deployment always assumes:</p> <p>versions change</p> <p>rolling updates</p> <p>history &amp; rollbacks</p> <p>If you want:</p> <p>exactly N identical Pods</p> <p>no updates</p> <p>no rollout strategy</p> <p>no revision history</p> <p>then a ReplicaSet is simpler and more explicit.</p> <p>Example use case</p> <p>Batch workers that are redeployed by deleting/recreating</p> <p>Short-lived systems where updates are destructive anyway</p> <p>2) You are writing a custom controller/operator</p> <p>If you\u2019re building:</p> <p>a Kubernetes Operator</p> <p>a custom reconciliation loop</p> <p>you may deliberately create ReplicaSets directly instead of Deployments.</p> <p>Why?</p> <p>Full control over Pod lifecycle</p> <p>No implicit Deployment behavior</p> <p>You decide when a new ReplicaSet is created</p> <p>This is common in advanced platform engineering (you\u2019ll see this in operators).</p> <p>3) You want to opt out of Deployment abstractions</p> <p>Deployments add:</p> <p>rolling update logic</p> <p>surge/unavailable limits</p> <p>revision tracking</p> <p>If those abstractions get in your way, a ReplicaSet gives:</p> <p>one responsibility only: Pod count enforcement</p> <p>zero opinionated behavior</p> <p>This can matter in:</p> <p>research clusters</p> <p>custom schedulers</p> <p>constrained environments</p>"},{"location":"k8s/kcna/useful-kubectl-command/","title":"Usefull kubectl conmmands","text":"<pre><code>kubectl get nodes\n</code></pre> <pre><code>kubectl get all\n</code></pre> <pre><code>kubectl get pods --all-namespaces\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#available-commands","title":"Available Commands:","text":"Command Description clusterrole Create a cluster role clusterrolebinding Create a cluster role binding for a particular cluster role configmap Create a config map from a local file, directory or literal value cronjob Create a cron job with the specified name deployment Create a deployment with the specified name ingress Create an ingress with the specified name job Create a job with the specified name namespace Create a namespace with the specified name poddisruptionbudget Create a pod disruption budget with the specified name priorityclass Create a priority class with the specified name quota Create a quota with the specified name role Create a role with single rule rolebinding Create a role binding for a particular role or cluster role secret Create a secret using a specified subcommand service Create a service using a specified subcommand serviceaccount Create a service account with the specified name token Request a service account token"},{"location":"k8s/kcna/useful-kubectl-command/#create-a-deployment-using-imperative-command","title":"Create a deployment using imperative command","text":"<pre><code>kubectl create deployment myapp --image=nginx:1.27\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#scale-the-deployment","title":"Scale the deployment","text":"<pre><code>kubectl scale deployment myapp --replicas=3\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#expose-the-deployment","title":"Expose the deployment","text":"<pre><code>kubectl expose deployment myapp \\\n--type=ClusterIP \\\n--port=80 \\\n--target-port=80\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#update-the-image-rolling-update","title":"Update the image (rolling update)","text":"<pre><code>kubectl set image deployment/myapp myapp=nginx:1.28\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#add-environment-variables","title":"Add environment variables","text":"<pre><code>kubectl set env deployment/myapp ENV=production LOG_LEVEL=info\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#list-pods-with-labels","title":"List pods with labels","text":"<pre><code>kubectl get pod --show-labels\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#add-labels-to-pods","title":"Add labels to pods","text":"<pre><code>kubectl label pod myapp-85cf5b8c8d-fs764 app=DEBUG --overwrite\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#list-nodes","title":"List nodes","text":"<pre><code>kubectl get nodes --watch\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#list-all-the-resources-available","title":"List all the resources available","text":"<pre><code>kubectl api-resources\n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#explain-a-resource-type","title":"Explain a resource type","text":"<pre><code>kubectl explain \n</code></pre>"},{"location":"k8s/kcna/useful-kubectl-command/#what-is-a-resource-on-kubernetes","title":"What is a resource on Kubernetes","text":"<p>In Kubernetes, a resource is an object that represents a component of your cluster, such as pods, services, deployments, nodes, or custom resources. Resources are defined using YAML or JSON manifests and managed via the Kubernetes API. Each resource has a kind, metadata, and a spec that describes its desired state.</p>"},{"location":"k8s/kcna/useful-kubectl-command/#init-containers-on-pods","title":"Init Containers on Pods","text":"<p>An init container is a specialized Kubernetes container that runs and completes before a pod\u2019s main app containers start, often used for setup tasks like waiting for dependencies, prepping config, or warming caches. Each init container must finish successfully, ensuring predictable pod startup.</p> <p>Example <code>yaml</code> manifest showing an init container copying config before the main app runs:</p> <p>```yaml apiVersion: v1 kind: Pod metadata:    name: init-container-example spec:    volumes:       - name: shared-data         emptyDir: {}</p> <p>initContainers:       - name: init-setup         image: busybox:1.36         command:            - sh            - -c            - |               echo \"Hello from init container\" &gt; /data/message.txt         volumeMounts:            - name: shared-data              mountPath: /data</p> <p>containers:       - name: app         image: busybox:1.36         command:            - sh            - -c            - |               echo \"Main container started\"               cat /data/message.txt               sleep 3600         volumeMounts:            - name: shared-data              mountPath: /data</p> <p>restartPolicy: Never</p> <p>```</p>"},{"location":"k8s/kcna/useful-kubectl-command/#what-is-a-statefulset","title":"What is a StatefulSet","text":"<ul> <li>A StatefulSet is a Kubernetes workload object designed to run stateful applications\u2014apps that require:</li> <li>Stable network identity </li> <li>Stable, persistent storage </li> <li>Ordered startup, scaling, and shutdown </li> <li>Unlike a Deployment, a StatefulSet does not treat Pods as interchangeable.</li> </ul>"},{"location":"k8s/kcna/useful-kubectl-command/#what-is-daemonset","title":"What is DaemonSet","text":"<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod.</p>"},{"location":"k8s/kcna/useful-kubectl-command/#what-is-job","title":"What is Job","text":"<p>A Job creates one or more Pods and ensures that a specified number of them successfully terminate.</p>"},{"location":"k8s/kcna/useful-kubectl-command/#what-is-cronjob","title":"What is CronJob","text":"<ul> <li>A CronJob creates Jobs on a schedule. </li> <li>Cronjob creates a Job and a job creates a pod</li> </ul>"},{"location":"linux/fstab/","title":"fstab","text":""},{"location":"linux/fstab/#mount-a-efs-when-machine-starts-using-fstab","title":"Mount a EFS when machine starts using fstab","text":"<pre><code>LABEL=cloudimg-rootfs   /    ext4   defaults,discard    0 1\nfs-123456.efs.eu-west-1.amazonaws.com:/ /mnt/efs nfs4 defaults,_netdev 0 0\n</code></pre>"},{"location":"mac/","title":"MAC tips","text":""},{"location":"mac/#copy-and-paste-issue-across-devices","title":"Copy and Paste issue across devices","text":"<p>If you are having trouble copying and pasting text between your Mac and other devices, make sure that you have the Handoff feature enabled on your Mac. To do this, go to System Preferences &gt; General and check the box next to \"Allow Handoff between this Mac and your iCloud devices.\" This will allow you to copy and paste text between your Mac and other devices that are signed in to the same iCloud account.</p>"},{"location":"mac/#last-trick","title":"Last trick","text":"<p>If above does not work and you are still having issue, please do:</p> <pre><code> defaults delete ~/Library/Preferences/com.apple.coreservices.useractivityd.plist\n killall useractivityd\n</code></pre>"},{"location":"python/send-message-teams/","title":"How to send message to teams using Python","text":"<pre><code>from urllib.parse import urlparse\nimport json\nimport http.client\nimport os\n\n\ndef send_message(instance_id, message_type):\n\n    message_body = json.dumps({\"body\": \"Broker \" + instance_id + \" is getting \" + message_type})\n\n    headers = {\"Content-Type\": \"application/json\"}\n\n    url = urlparse(\"&lt;teams-slack-channel-url&gt;\")\n\n    conn = http.client.HTTPSConnection(url.netloc)\n\n    conn.request(\"POST\", url.path, message_body, headers)\n\n    response = conn.getresponse()\n\n    # Check for successful response (status code 201)\n    if response.status == 201:\n        print(f\"Message sent successfully to Teams channel!\")\n    else:\n        print(f\"An error occurred while sending the message: {response.status} - {response.reason}\")\n</code></pre>"},{"location":"redhat/subscription/","title":"Red Hat subscriptions","text":""},{"location":"redhat/subscription/#about","title":"About","text":"<p>subscription-manager is the command-line based client for the Red Hat Subscription Manager tool.</p> <p>The Subscription Manager performs several key operations:</p> <ul> <li>It registers machines to the Red Hat entitlement service and adds the machine to the systems inventory. Once a machine is registered, it can receive updates based on its subscriptions to any kind of software products.</li> <li> <p>It lists both available and used (consumed) subscriptions.</p> </li> <li> <p>It allows administrators to both subscribe and unsubscribe a system from specific subscriptions.</p> </li> </ul>"},{"location":"redhat/subscription/#check-subscription","title":"Check subscription","text":"<pre><code>subscription-manager list\n</code></pre>"},{"location":"zsh/alias/","title":"Alias","text":""},{"location":"zsh/alias/#kubectl","title":"kubectl","text":""},{"location":"zsh/alias/#get-all-contexts","title":"Get all contexts","text":"<pre><code>alias get-context=\"kubectl config get-contexts\"\n</code></pre>"},{"location":"zsh/alias/#use-a-specific-context","title":"Use a specific context","text":"<pre><code>alias context-tt=\"kubectl config use-context arn:aws:eks:&lt;region&gt;:&lt;account&gt;:cluster/&lt;name cluster&gt;\"\n</code></pre>"},{"location":"zsh/alias/#open-idea-folder-on-mac","title":"Open idea folder on MAC","text":"<pre><code>alias idea=\u2019open -a \u201c`ls -dt /Applications/IntelliJ\\ IDEA*|head -1`\u201c\u2019\n</code></pre>"}]}